{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install transformers\n# wget https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv\n# wget https://github.com/brianbt/AI6127_NLP_project/raw/master/data/labelled_data_spell.csv\n# Put me under data/","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:02.204367Z","iopub.execute_input":"2022-04-21T09:27:02.204922Z","iopub.status.idle":"2022-04-21T09:27:02.226237Z","shell.execute_reply.started":"2022-04-21T09:27:02.204836Z","shell.execute_reply":"2022-04-21T09:27:02.2256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification,AutoModel\nfrom transformers import AutoTokenizer, AutoConfig\nfrom sklearn.utils import shuffle\nimport sklearn\nimport random\nimport warnings\nimport re\n# from math import comb\n\nseed = 888\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using {device}\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:02.22957Z","iopub.execute_input":"2022-04-21T09:27:02.230013Z","iopub.status.idle":"2022-04-21T09:27:09.155224Z","shell.execute_reply.started":"2022-04-21T09:27:02.229978Z","shell.execute_reply":"2022-04-21T09:27:09.154423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def preprocess(string):\n    temp = string.lower()\n    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n    temp = re.sub(\"[!:]+\",\"\", temp)\n    temp = re.sub(r\"&amp;\", \"\", temp)\n    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n    temp = re.sub(r\"http\\S+\", \"\", temp)\n    temp = re.sub(r\"www.\\S+\", \"\", temp)\n    temp = re.sub('[()!?]', ' ', temp)\n    temp = re.sub('\\[.*?\\]',' ', temp)\n    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n    temp = re.sub(r\"rt\", \"\", temp)\n    return temp\n\ndef misspellings(string):\n    d = enchant.request_dict(\"en_US\")\n    chkr = SpellChecker(\"en_US\", string)\n    for err in chkr:\n        suggest = d.suggest(err.word)\n        if len(suggest) != 0:\n            err.replace(suggest[0])\n    \n    return chkr.get_text()#print(chkr.get_text())\n\ndef dup_data(df, on_class, repeat=1000):\n    \"\"\" data augmentation\n    \n    This will random pick two row and mix them together\n    So two sentence will be concat tgt, generate a longer sentence\n    For safety, it only do augmentation on the same class\n      It will NOT generate new data nest-ed-ly. \n      Means augmented data will not be used to generate new data\n    \n    Args:\n        df: DataFrame\n        on_class (int): Which class to augment, either [0,1,2]\n        repeat (int): how many new data to generate\n    \n    Return:\n        pd.DataFrame: The augmented data and org df will be concat tgt\n        \n    Examples:\n        >>> df = dup_data(df, 0)\n    \"\"\"\n    cl = {'hate_speech': 0, 'offensive_language': 1, 'neither': 2}\n    out = []\n    tmp = df[df['class'] == on_class]\n#     print(f\"class={on_class} have {tmp.shape[0]} data, each time pick 2. nCr = {comb(tmp.shape[0], 2)}\")\n    for k in range(repeat):\n        i,j = random.randint(0, tmp.shape[0]-1), random.randint(0, tmp.shape[0]-1)\n        out.append(tmp.iloc[i]+tmp.iloc[j])\n    out = pd.concat(out, axis=1).T\n    out[['count', 'hate_speech', 'offensive_language', 'neither', 'class']] = out[['count', 'hate_speech', 'offensive_language', 'neither', 'class']].astype('int')\n    # handle edge case\n    out['class'] = out[['hate_speech', 'offensive_language', 'neither']].idxmax(1).map({'hate_speech': 0, 'offensive_language': 1, 'neither': 2})\n    return pd.concat([df,out]).reset_index(drop=True)\n\ndef number_params(model, exclude_freeze=False):\n    \"\"\"calculate the number of parameters in a model\n\n    Args:\n        model (nn.Module): PyTorch model\n        exclude_freeze (bool, optional): Whether to count the frozen layer. Defaults to False.\n    \"\"\"\n    pp = 0\n    for p in list(model.parameters()):\n        if exclude_freeze and p.requires_grad is False:\n            continue\n        nn = 1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef ensemble(models, do_argmax=False):\n    \"\"\" ensemble models prediction\n    \n    Args:\n        models (list(tensor)): list of prediction, each prediction should have same shape(N,C).\n    \n    Examples:\n        tweet = iter(dataloader_train).next()[-1]\n        out = model(list(tweet))\n        ensemble([out[0], out[0]])\n    \"\"\"\n    out = torch.softmax(models[0], 1)\n    for i in range(1, len(models)):\n        out += torch.softmax(models[i], 1)\n    if do_argmax:\n        return out.argmax(1)\n    else:\n        return out\n\ndef finetune(\n        model: nn.Module,\n        base_lr: float,\n        groups,\n        ignore_the_rest: bool = False,\n        raw_query: bool = False,\n        regex=False):\n    \"\"\" This is something call per-parameter options\n\n    Separate out the finetune parameters with a learning rate for each layers of parameters\n    This function only support setting a different learning rate for each layer's arameter.\n    Depending on the optimizer, you can set extra parameter for that layer for the optmizer -> See Notes \n    If you freeze layer using this function and want to unfreeze it later:\n    See https://discuss.pytorch.org/t/correct-way-to-freeze-layers/26714/2\n\n    Args:\n        model (nn.Module): Pytorch Model\n        base_lr (float): learning rate of all layers\n        groups (Dict[str, float]): key is `name` of layers, value is the `extra_lr` (or False).\n          all layers that contains that `name` will have `lr` of base_lr*extra_lr.\n          it uses fnmatch|regex to check whether a layer contains that `name`.\n          fnmatch is matching structure like `layer1*`, `layer?.conv?.`, `*conv2*`, etc...\n          regex is the comman regex matching.\n          Hence, `name` here is either fnmatch or regex expression if using raw_query.\n          If `float` is False: those layers with `name` will be freeze. \n          In particular, they will not be included in the return output and require_grad will be set to False\n        ignore_the_rest (bool, optional): Include the remaining layer that are not stated in `grouprs` or not. Defaults to False.\n        raw_query (bool, optional): Modify the keys of `groups` as f'*{key}*' if False. Only useful when `regex=False`\n          Do not do any modification to the keys of `groups` if True. Defaults to False.\n        regex (bool, optional): Use regex instead of fnmatch on keys of groups. Defaults to False.\n          This will overrride raw_query to True. \n          Notice: `regex=False` is depracted\n\n    Returns:\n        List[Dict[str, Union[float, Iterable]]]: list of dict that has two or more key-value pair.\n          The first one is feature generation layers. [those layers must start with `features` name] <usually is backbone>\n            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'query':query, 'lr':base_lr*groups[groups.keys()]]\n          The remaining are all others layer. [all others params for last one, if ignore_the_rest = False]\n            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'lr':base_lr]\n\n    Examples:\n        >>> model = models.resnet50()\n        >>> # all layers that has name start with `layer1 and layer2` will have learning rate `0.001*0.01`\n        >>> # all layers that has name start with `layer3` will be froozen`\n        >>> # all layers that has name start with `layer4` will have learning rate `0.001*0.001`\n        >>> # for all other layers will have the base_lr `0.001`\n        >>> model_params = finetune(model, base_lr=0.001, groups={'^layer[1-2].*': 0.01, '^layer3.*': False, '^layer4.*': 0.001}, regex=True)\n        >>> # setting extra parameter (other than learning rate) for that optimizer\n        >>> # the second param_group `layer4` will have weight_decay 1e-2\n        >>> model_params[1]['weight_decay'] = 1e-2\n        >>> # init optimizer with the above setting\n        >>> # the argument under `torch.optim.SGD` will be overrided by finetune() if they exist.\n        >>> # For example, all model_params will have weight_decay=5e-3 except model_params[1]\n        >>> optimizer = torch.optim.SGD(model_params, momentum=0.9, lr=0.1, weight_decay=5e-3)\n    \"\"\"\n    if regex:\n        raw_query = True\n    else:\n        warnings.warn(\"regex=False is deprecated; use regex=True\", DeprecationWarning)\n    # Deal with Freeze Later\n    freeze_group = dict()\n    freeze = False\n    for k,v in groups.items():\n        if v is False:\n            freeze_group[k] = 1\n            freeze=True\n    for k in freeze_group.keys():\n        del groups[k]\n    freeze_group = \"(\" + \")|(\".join(freeze_group) + \")\"\n\n    parameters = [\n        dict(params=[],\n             names=[],\n             query=query if raw_query else '*' + query + '*',\n             lr = lr * base_lr,\n             initial_lr = lr * base_lr) for query, lr in groups.items()\n    ]\n    rest_parameters = dict(params=[], names=[], lr=base_lr, initial_lr=base_lr)\n    for k, v in model.named_parameters():\n        rest = 0\n        if freeze and regex and re.match(freeze_group, k):\n            v.requires_grad = False\n            continue\n        for group in parameters:\n            if not regex and fnmatch(k, group['query']):\n                group['params'].append(v)\n                group['names'].append(k)\n                rest = 1\n                break\n            elif regex and re.compile(group['query']).search(k):\n                group['params'].append(v)\n                group['names'].append(k)\n                rest = 1\n                break\n        if rest == 0:\n            rest_parameters['params'].append(v)\n            rest_parameters['names'].append(k)\n\n    if not ignore_the_rest:\n        parameters.append(rest_parameters)\n    for group in parameters:\n        group['params'] = iter(group['params'])\n    return parameters","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.15712Z","iopub.execute_input":"2022-04-21T09:27:09.157354Z","iopub.status.idle":"2022-04-21T09:27:09.188074Z","shell.execute_reply.started":"2022-04-21T09:27:09.157321Z","shell.execute_reply":"2022-04-21T09:27:09.187394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# path of data and the name of pretrained weights\n# path = '../input/ai6127/labeled_data.csv'\npath = '../input/ai6127/labelled_data_spell.csv'\n# path = './data/labeled_data.csv'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.189242Z","iopub.execute_input":"2022-04-21T09:27:09.189487Z","iopub.status.idle":"2022-04-21T09:27:09.203016Z","shell.execute_reply.started":"2022-04-21T09:27:09.189454Z","shell.execute_reply":"2022-04-21T09:27:09.202269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path, index_col = 0).dropna().reset_index()\ndf = shuffle(df)\ndf.describe()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.205112Z","iopub.execute_input":"2022-04-21T09:27:09.205885Z","iopub.status.idle":"2022-04-21T09:27:09.317795Z","shell.execute_reply.started":"2022-04-21T09:27:09.205791Z","shell.execute_reply":"2022-04-21T09:27:09.317098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 15920 & 4856, Just to make sure the experiment is reproducible\ndf.iloc[0:2] ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.319079Z","iopub.execute_input":"2022-04-21T09:27:09.319319Z","iopub.status.idle":"2022-04-21T09:27:09.328204Z","shell.execute_reply.started":"2022-04-21T09:27:09.319286Z","shell.execute_reply":"2022-04-21T09:27:09.327536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df['class'].value_counts())\ndisplay(df['class'].value_counts()/df['class'].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.329593Z","iopub.execute_input":"2022-04-21T09:27:09.33022Z","iopub.status.idle":"2022-04-21T09:27:09.3441Z","shell.execute_reply.started":"2022-04-21T09:27:09.330181Z","shell.execute_reply":"2022-04-21T09:27:09.343489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data into train and test\ntrain_data = df.sample(frac = 0.8)\ntest_data = df.drop(train_data.index)\nvalid_data = test_data.sample(frac = 0.5)\ntest_data = test_data.drop(valid_data.index)\n\ndisplay(train_data.head())\nprint(\"===================================\")\ndisplay(valid_data.head())\nprint(\"===================================\")\ndisplay(test_data.head())\n\nprint(train_data.shape)\nprint(valid_data.shape)\nprint(test_data.shape)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.345143Z","iopub.execute_input":"2022-04-21T09:27:09.345312Z","iopub.status.idle":"2022-04-21T09:27:09.38404Z","shell.execute_reply.started":"2022-04-21T09:27:09.345291Z","shell.execute_reply":"2022-04-21T09:27:09.383361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.385075Z","iopub.execute_input":"2022-04-21T09:27:09.385725Z","iopub.status.idle":"2022-04-21T09:27:09.392923Z","shell.execute_reply.started":"2022-04-21T09:27:09.385688Z","shell.execute_reply":"2022-04-21T09:27:09.392082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation (experiments)","metadata":{}},{"cell_type":"code","source":"# train_data = dup_data(train_data, 0, 15361-1129)\n# train_data = dup_data(train_data, 2, 15361-3336)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.394105Z","iopub.execute_input":"2022-04-21T09:27:09.394502Z","iopub.status.idle":"2022-04-21T09:27:09.400388Z","shell.execute_reply.started":"2022-04-21T09:27:09.394466Z","shell.execute_reply":"2022-04-21T09:27:09.399659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.403311Z","iopub.execute_input":"2022-04-21T09:27:09.404014Z","iopub.status.idle":"2022-04-21T09:27:09.40859Z","shell.execute_reply.started":"2022-04-21T09:27:09.403977Z","shell.execute_reply":"2022-04-21T09:27:09.407878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Dataset\n\nThe data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data df contains 5 columns:\n\ncount = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n\nhate_speech = number of CF users who judged the tweet to be hate speech.\n\noffensive_language = number of CF users who judged the tweet to be offensive.\n\nneither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n\nclass = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_name=None,  train = True, device='cuda'):\n        super(Dataset, self).__init__()\n        self.df = df\n        self.device = device\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        count = torch.LongTensor([self.df.iloc[idx]['count']])\n        hate_speech = torch.LongTensor([self.df.iloc[idx]['hate_speech']])\n        offensive_language = torch.LongTensor([self.df.iloc[idx]['offensive_language']])\n        neither = torch.LongTensor([self.df.iloc[idx]['neither']])\n        target = torch.LongTensor([self.df.iloc[idx]['class']])\n        tweet = self.df.iloc[idx]['tweet']\n        return (count, hate_speech, offensive_language, neither, target, tweet)\n        \n        ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.409592Z","iopub.execute_input":"2022-04-21T09:27:09.410318Z","iopub.status.idle":"2022-04-21T09:27:09.420216Z","shell.execute_reply.started":"2022-04-21T09:27:09.410282Z","shell.execute_reply":"2022-04-21T09:27:09.419539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This one is just for DEBUG, not the real dataset to be used\ndataset = Dataset(df)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size= 6, shuffle=True)\noutput = next(iter(dataloader))\nprint(output)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.42147Z","iopub.execute_input":"2022-04-21T09:27:09.421942Z","iopub.status.idle":"2022-04-21T09:27:09.481595Z","shell.execute_reply.started":"2022-04-21T09:27:09.421907Z","shell.execute_reply":"2022-04-21T09:27:09.480795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    \n    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False):\n        super(LanguageModel, self).__init__()\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if freeze_pretrained:\n            print(\"You are freezing the BERT\")\n            for name, p in self.model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        print(f\"Total number of params: {number_params(self.model)}\")\n        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n\n    def forward(self, src, has_mask=False):\n        # print(src)\n        output = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        output = torch.LongTensor(output['input_ids']).to(device)\n        if has_mask == True:\n            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n            output = self.model(output, attention_mask=attention_mask)\n        else:\n            output = self.model(output)\n        return output","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.482718Z","iopub.execute_input":"2022-04-21T09:27:09.483134Z","iopub.status.idle":"2022-04-21T09:27:09.493293Z","shell.execute_reply.started":"2022-04-21T09:27:09.483096Z","shell.execute_reply":"2022-04-21T09:27:09.492488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LastAttnModel(nn.Module):\n    \"\"\"\n    Use the [CLS] as query and all other output as key and values.\n    Pass it to a Multi-Head Attention, then a Linear classifier\n    \n    Args:\n      auxiliary_head(list(int)): Only used when training\n        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n        - See BertConfig['num_hidden_layers'] for total number of layers\n        - EG: `auxiliary_head=[10,11,12]`.\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n      all_CLS_attn(bool): use last CLS as query, all previous CLS as key and values -> Multi-Head Attention\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n      out = modelA(list(tweet))\n      print(out[0].shape, attn_weight[1].shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_model, tokenizer, \n                 last_attn_num_head = 8,\n                 classifier_hidden_dim = 512, \n                 classifier_dropout = 0,\n                 num_labels = 3, \n                 freeze_pretrained=False,\n                 auxiliary_head=None,\n                 last_hidden_layer=-1,\n                 all_CLS_attn=False,\n                 **kwargs):\n        super(LastAttnModel, self).__init__()\n        self.pretrain_model = pretrain_model\n        self.tokenizer = tokenizer\n        self.auxiliary_head = auxiliary_head\n        self.num_layers = len(pretrain_model.encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        self.all_CLS_attn = all_CLS_attn\n        \n        if freeze_pretrained:\n            if self.auxiliary_head is not None:\n              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n            print(\"You are freezing the BERT pertrain\")\n            for name, p in self.pretrain_model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        \n        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n        self.final_classifier = nn.Sequential(\n            nn.Linear(embed_size, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        # deal with aux \n        if self.auxiliary_head is not None:\n          self.aux_classifiers = nn.ModuleList()\n          for i in self.auxiliary_head:\n            self.aux_classifiers.append(nn.Sequential(\n              nn.Linear(embed_size, classifier_hidden_dim),\n              nn.ReLU(),\n              nn.Dropout(classifier_dropout),\n              nn.Linear(classifier_hidden_dim, num_labels)\n            ))\n            \n        # use all CLS attention\n        if self.all_CLS_attn:\n            self.all_CLS = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n        else:\n            pre_train_output = self.pretrain_model(inputs)\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n        if self.all_CLS_attn:\n            o = [hidden[:,0,:] for hidden in pre_train_output[\"hidden_states\"][:self.last_hidden_layer-1]]\n            rest = torch.stack(o).permute(1,0,2)  # (N,self.last_hidden_layer-1,E)\n            last_hidden_state_cls,_=self.all_CLS(last_hidden_state_cls, rest, rest)\n        last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n        atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n        last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n                                                    key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n        last_attn_out = last_attn_out.squeeze(1) #(N,E)\n        output = self.final_classifier(last_attn_out)\n        out += [output, last_attn_w]\n        ## auxiliary_head\n        if self.auxiliary_head is not None:\n          if \"hidden_states\" not in pre_train_output:\n            raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n          for idx in range(len(self.aux_classifiers)):\n            hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n            out.append(self.aux_classifiers[idx](hidden_cls))\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out, attn_weight = modelA(list(tweet))\n# print(out.shape, attn_weight.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.496871Z","iopub.execute_input":"2022-04-21T09:27:09.497409Z","iopub.status.idle":"2022-04-21T09:27:09.520461Z","shell.execute_reply.started":"2022-04-21T09:27:09.49737Z","shell.execute_reply":"2022-04-21T09:27:09.519658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiLSTMModel(nn.Module):\n    \"\"\"\n    Fit the last layer BERT output to bi-lstm.\n    Concat the forward and backward final hidden state, then a Linear classifier\n    \n    Args:\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = BiLSTMModel(pretrain_model, tokenizer).to(device)\n      out  = modelA(list(tweet))\n      print(out[0].shape) #torch.Size([32, 3])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_model, tokenizer, \n                 lstm_hidden = 1024,\n                 lstm_num_layer = 2,\n                 classifier_hidden_dim = 512, \n                 classifier_dropout = 0,\n                 num_labels = 3, \n                 freeze_pretrained=False,\n                 last_hidden_layer=-1,\n                 **kwargs):\n        super(BiLSTMModel, self).__init__()\n        self.pretrain_model = pretrain_model\n        self.tokenizer = tokenizer\n        self.lstm_hidden = lstm_hidden\n        self.lstm_num_layer = lstm_num_layer\n        self.num_layers = len(pretrain_model.encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        \n        if freeze_pretrained:\n            print(\"You are freezing the BERT pertrain\")\n            for name, p in self.pretrain_model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        \n        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n        self.lstm = nn.LSTM(embed_size, lstm_hidden, lstm_num_layer, bidirectional=True, batch_first=True)\n        \n        self.final_classifier = nn.Sequential(\n            nn.Linear(2*lstm_hidden, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n        else:\n            pre_train_output = self.pretrain_model(inputs)\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        batch_size, seq_len, embed_size = last_hidden.shape\n        output, (h_n, c_n) = self.lstm(last_hidden)\n        output = output.view(batch_size, seq_len, 2, self.lstm_hidden) #batch, seq_len, num_directions, hidden_size\n        h_n = h_n.view(self.lstm_num_layer, 2, batch_size, self.lstm_hidden) # num_layers, num_directions, batch, hidden_size\n        c_n = c_n.view(self.lstm_num_layer, 2, batch_size, self.lstm_hidden) # num_layers, num_directions, batch, hidden_size\n        forward_last = h_n[-1, 0, :, :]  #(N, H)\n        backward_last = h_n[-1, 1, :, :] #(N, H)\n        output = torch.hstack([forward_last, backward_last]) #(N,2H)\n        output = self.final_classifier(output)\n        out += [output]\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out = modelA(list(tweet))\n# print(out[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:09.522817Z","iopub.execute_input":"2022-04-21T09:27:09.523956Z","iopub.status.idle":"2022-04-21T09:27:09.542992Z","shell.execute_reply.started":"2022-04-21T09:27:09.523914Z","shell.execute_reply":"2022-04-21T09:27:09.542151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization","metadata":{}},{"cell_type":"code","source":"dataset = Dataset(train_data)\ndataloader_train = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\ndataset_valid = Dataset(valid_data)\ndataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=32, shuffle=False)\ndataset_test = Dataset(test_data)\ndataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=False)\n\nlossfn = nn.CrossEntropyLoss().to(device)\n# lossfn = nn.CrossEntropyLoss(weight=torch.tensor([4.6761, 1.8533, 4.0554])).to(device)\n\n\n# model_name = 'bert-base-uncased'\n# # model_name = 'cardiffnlp/twitter-roberta-base-emotion'\nmodel_name = 'GroNLP/hateBERT'\n# config = {}\n# Dconfig = {'Dmodel_name':'hateBERT',\n#            'freeze_pretrained':False,\n#            'epochs': 20}\n# model = LanguageModel(model_name, freeze_pretrained=False).to(device)\n\nconfig = AutoConfig.from_pretrained(\n    model_name, \n    output_hidden_states = True,\n    output_attention = False,\n    hidden_dropout_prob = 0.2,\n) \nprint(config)\npretrain_model = AutoModel.from_pretrained(\n    model_name,\n    config = config\n).to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nDconfig = {'Dmodel_name':'LastAttnModel',\n           'freeze_pretrained':False,\n           'classifier_dropout':0.1,\n           'auxiliary_head':None,\n           'last_hidden_layer':5,\n           'all_CLS_attn':True,\n           'epochs': 20}\nmodel = LastAttnModel(pretrain_model, tokenizer, **Dconfig).to(device)\n# Dconfig = {'Dmodel_name':'BiLSTMModel',\n#            'freeze_pretrained':True,\n#            'lstm_num_layer':2,\n#            'classifier_dropout':0.1,\n#            'last_hidden_layer':5,\n#            'epochs': 20}\n# model = BiLSTMModel(pretrain_model, tokenizer, **Dconfig).to(device)\nprint(Dconfig)\n\n# model_params = finetune(model, base_lr=1e-4, groups={'^pretrain_model.*':0.01}, regex=True)\n# model_params = finetune(model, base_lr=1e-4, \n#                         groups={'^pretrain_model.encoder.layer.([0-2])\\..*': 1, \n#                                 '^pretrain_model.encoder.layer.([3-4])\\..*': 1, \n#                                 '^pretrain_model.encoder.layer.([5]|1[012]).*': False, \n#                                 '^pretrain_model.pooler.*': False,\n#                                 '^pretrain_model.embeddings.*':False},\n#                         regex=True)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\nepochs = Dconfig['epochs']\n\nsave_path=\"./lastATNN.pt\"","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T09:27:09.546051Z","iopub.execute_input":"2022-04-21T09:27:09.546574Z","iopub.status.idle":"2022-04-21T09:27:31.851134Z","shell.execute_reply.started":"2022-04-21T09:27:09.546469Z","shell.execute_reply":"2022-04-21T09:27:31.8504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for datas in dataloader_train:\n    count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:31.852411Z","iopub.execute_input":"2022-04-21T09:27:31.852831Z","iopub.status.idle":"2022-04-21T09:27:31.890103Z","shell.execute_reply.started":"2022-04-21T09:27:31.852794Z","shell.execute_reply":"2022-04-21T09:27:31.889441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(list(tweet))[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:31.891339Z","iopub.execute_input":"2022-04-21T09:27:31.891587Z","iopub.status.idle":"2022-04-21T09:27:32.708374Z","shell.execute_reply.started":"2022-04-21T09:27:31.891554Z","shell.execute_reply":"2022-04-21T09:27:32.707607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DEBUG","metadata":{}},{"cell_type":"code","source":"# model_name = 'GroNLP/hateBERT'\n# config = AutoConfig.from_pretrained(\n#     model_name, \n#     output_hidden_states = True,\n#     output_attention = False\n# ) \n# print(config)\n# model = AutoModel.from_pretrained(\n#     model_name,\n#     config = config\n# ).to(device)\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:27:35.244055Z","iopub.execute_input":"2022-04-21T09:27:35.244711Z","iopub.status.idle":"2022-04-21T09:27:40.842908Z","shell.execute_reply.started":"2022-04-21T09:27:35.244674Z","shell.execute_reply":"2022-04-21T09:27:40.842164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataloader_train = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:29:16.579065Z","iopub.execute_input":"2022-04-21T09:29:16.579479Z","iopub.status.idle":"2022-04-21T09:29:16.583533Z","shell.execute_reply.started":"2022-04-21T09:29:16.579441Z","shell.execute_reply":"2022-04-21T09:29:16.582874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweet = iter(dataloader_train).next()[-1]\n# tokens = tokenizer(list(tweet), padding=True, truncation=True, max_length=50)\n# print(tokens.keys())\n# inputs = torch.LongTensor(tokens['input_ids']).to(device)\n# atten_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n# print(inputs.shape)\n# hiddens = model(inputs)\n# print(hiddens.keys())\n# print(hiddens['last_hidden_state'].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T09:29:17.727244Z","iopub.execute_input":"2022-04-21T09:29:17.728044Z","iopub.status.idle":"2022-04-21T09:29:17.772101Z","shell.execute_reply.started":"2022-04-21T09:29:17.727994Z","shell.execute_reply":"2022-04-21T09:29:17.77141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lstm = nn.LSTM(model.embeddings.word_embeddings.embedding_dim, 1024, 2, bidirectional=True, batch_first=True).cuda()\n# batch_size, seq_len, embed_size = hiddens['last_hidden_state'].shape\n# output, (h_n, c_n) = lstm(hiddens['last_hidden_state'])\n# print(output.shape, h_n.shape, c_n.shape)\n# output = output.view(batch_size, seq_len, 2, 1024) #batch, seq_len, num_directions, hidden_size\n# h_n = h_n.view(2, 2, batch_size, 1024) # num_layers, num_directions, batch, hidden_size\n# c_n = c_n.view(2, 2, batch_size, 1024) # num_layers, num_directions, batch, hidden_size\n# print(output.shape, h_n.shape, c_n.shape)\n# forward_last = h_n[-1, 0, :, :]\n# backward_last = h_n[-1, 1, :, :]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:42:57.814176Z","iopub.execute_input":"2022-04-18T03:42:57.814429Z","iopub.status.idle":"2022-04-18T03:42:57.821597Z","shell.execute_reply.started":"2022-04-18T03:42:57.814395Z","shell.execute_reply":"2022-04-18T03:42:57.82094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hiddens['pooler_output'].shape\n# hiddens['last_hidden_state'][:,0,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:42:57.822926Z","iopub.execute_input":"2022-04-18T03:42:57.823246Z","iopub.status.idle":"2022-04-18T03:42:57.832554Z","shell.execute_reply.started":"2022-04-18T03:42:57.823211Z","shell.execute_reply":"2022-04-18T03:42:57.831703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.pooler","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:42:57.834133Z","iopub.execute_input":"2022-04-18T03:42:57.834449Z","iopub.status.idle":"2022-04-18T03:42:57.84154Z","shell.execute_reply.started":"2022-04-18T03:42:57.834412Z","shell.execute_reply":"2022-04-18T03:42:57.840824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.pooler(hiddens['last_hidden_state']) == hiddens['pooler_output']","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:42:57.843073Z","iopub.execute_input":"2022-04-18T03:42:57.843323Z","iopub.status.idle":"2022-04-18T03:42:57.850435Z","shell.execute_reply.started":"2022-04-18T03:42:57.843291Z","shell.execute_reply":"2022-04-18T03:42:57.849688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# atten_mask_pad = (inputs == 0)\n# print(atten_mask_pad.shape)\n# print(atten_mask_pad[:,1:].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:42:57.851853Z","iopub.execute_input":"2022-04-18T03:42:57.852264Z","iopub.status.idle":"2022-04-18T03:42:57.858865Z","shell.execute_reply.started":"2022-04-18T03:42:57.852229Z","shell.execute_reply":"2022-04-18T03:42:57.858112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble([out[0], torch.randn((32,3)).cuda(), out[0]-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:42:57.860303Z","iopub.execute_input":"2022-04-18T03:42:57.860613Z","iopub.status.idle":"2022-04-18T03:42:57.867168Z","shell.execute_reply.started":"2022-04-18T03:42:57.860581Z","shell.execute_reply":"2022-04-18T03:42:57.86633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train(dataloader_train, dataloader_valid = None, model = None, \n          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n    \n    trainloss = []\n    validloss = []\n    trainscore = []\n    validscore = []\n    bestt_score = 0\n    for i in range(epochs):\n        model.train()\n        averageloss = 0\n        averagef1 = 0\n        averagePrecision = 0\n        averageRecall = 0\n        for datas in tqdm(dataloader_train):\n            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n            target = target.to('cuda')\n            optimizer.zero_grad()\n            pred = model(list(tweet), has_mask)\n            loss = lossfn(pred[0], target.squeeze(1))\n            # aux head\n            for j in range(2, len(pred)):\n                loss+=0.3*lossfn(pred[j], target.squeeze(1))\n            loss.backward()\n            optimizer.step()\n            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n            precision=sklearn.metrics.precision_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n            recall=sklearn.metrics.recall_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n            averageloss += loss.item()/len(dataloader_train)\n            averagef1 += f1score/len(dataloader_train)\n            averagePrecision += precision/len(dataloader_train)\n            averageRecall += recall/len(dataloader_train)\n        trainloss.append(averageloss)\n        trainscore.append((averagef1, averagePrecision, averageRecall))\n        if dataloader_valid is not None:\n            model.eval()\n            averageloss = 0\n            averagef1 = 0\n            averagePrecision = 0\n            averageRecall = 0\n            for datas in tqdm(dataloader_valid):\n                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n                target = target.to('cuda')\n                pred = model(list(tweet), has_mask)\n                loss = lossfn(pred[0], target.squeeze(1))\n                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n                precision=sklearn.metrics.precision_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n                recall=sklearn.metrics.recall_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n                averageloss += loss.item()/len(dataloader_valid)\n                averagef1 += f1score/len(dataloader_valid)\n                averagePrecision += precision/len(dataloader_valid)\n                averageRecall += recall/len(dataloader_valid)\n            validloss.append(averageloss)\n            validscore.append((averagef1, averagePrecision, averageRecall))\n            print(f\"epoch: {i}, train loss: {trainloss[-1]}, validation loss: {validloss[-1]}\\n train f1score: {trainscore[-1]}\\nvalidation f1score: {validscore[-1]}\")\n            if averagef1 > bestt_score:\n                print(\"Found Best Model\")\n                to_save = {'model': model.state_dict(),\n                           'config': config,\n                           'Dconfig': Dconfig,\n                           'optimizer': optimizer,\n                           'lr_s':None}\n                torch.save(to_save, save_path.replace('.pt', '_best.pt'))\n                bestt_score = averagef1\n        else:\n            print(f\"epoch: {i}, train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n\n\n    return trainloss, validloss, trainscore, validscore\n\n\ndef test(dataloader, model = None, lossfn = None, epochs = 1, has_mask = True):\n    \"\"\"\n    Args:\n        model: if `pytorch Model` -> normal test. if `list(pytorch Model)` -> ensemble\n    \"\"\"\n#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n    testloss = []\n    testscore = []\n    if isinstance(model, list):\n        for mmm in model:\n            mmm.eval()\n    else:\n        model.eval()\n    averageloss = 0\n    averagef1 = 0\n    averagePrecision = 0\n    averageRecall = 0\n    overallPred = []\n    overallTar = []\n    for datas in tqdm(dataloader):\n        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n        target = target.to('cuda')\n        if isinstance(model, list):\n            ensem = []\n            for mmm in model:\n                p = mmm(list(tweet), has_mask)\n                ensem.append(p[0])\n            pred = [ensemble(ensem)]\n        else:\n            pred = model(list(tweet), has_mask)\n        loss = lossfn(pred[0], target.squeeze(1))\n        averageloss += loss.item()/len(dataloader)\n        overallPred.append(pred[0].argmax(-1).cpu().numpy())\n        overallTar.append(target.squeeze(1).cpu().numpy())\n#     print(overallPred[:2])\n#     print(overallTar[:2])\n    predicts = np.concatenate(overallPred)\n    targets = np.concatenate(overallTar)\n#     print(predicts.shape)\n#     print(targets.shape)\n\n    f1score = sklearn.metrics.f1_score(targets, predicts, average = 'weighted')\n    precision=sklearn.metrics.precision_score(targets, predicts, average='weighted')\n    recall=sklearn.metrics.recall_score(targets, predicts, average='weighted')\n\n    print(f\"test loss: {averageloss}\")\n    print(f\"test score: {(f1score, precision, recall)}\")\n    return averageloss, (f1score, precision, recall)\n            \ndef get_csv(dataloader, model = None, has_mask = True):\n    \"\"\"get the prediction csv\n    \n    Examples:\n        o = get_csv(dataset_test, model1)\n        o.to_csv(save_path.replace('.pt', '.csv'))\n    \"\"\"\n#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n    OverallPred = []\n    for datas in tqdm(dataloader):\n        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n        target = target.to('cuda')\n        if isinstance(model, list):\n            ensem = []\n            for mmm in model:\n                mmm.eval()\n                p = mmm(list(tweet), has_mask)\n                ensem.append(p[0])\n            pred = [ensemble(ensem)]\n        else:\n            model.eval()\n            pred = model(list(tweet), has_mask)\n        OverallPred.append(pred[0].cpu().detach().numpy())\n    output = np.concatenate(OverallPred)\n    return pd.DataFrame(output)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-18T04:03:38.910689Z","iopub.execute_input":"2022-04-18T04:03:38.911007Z","iopub.status.idle":"2022-04-18T04:03:38.944599Z","shell.execute_reply.started":"2022-04-18T04:03:38.910975Z","shell.execute_reply":"2022-04-18T04:03:38.943598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloss, validloss, trainscore, validscore = train(dataloader_train, dataloader_valid, model = model, optimizer = optimizer, \n      lossfn = lossfn, epochs = epochs, has_mask = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:46:04.856557Z","iopub.execute_input":"2022-04-18T03:46:04.857368Z","iopub.status.idle":"2022-04-18T03:47:28.715789Z","shell.execute_reply.started":"2022-04-18T03:46:04.85733Z","shell.execute_reply":"2022-04-18T03:47:28.715002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use best val set model\nstate = torch.load(save_path.replace('.pt', '_best.pt'))\nmodel.load_state_dict(state['model'])\n# Do test\ntest(dataloader_test, model = model, lossfn = lossfn, has_mask = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:39:28.15798Z","iopub.execute_input":"2022-04-18T05:39:28.158747Z","iopub.status.idle":"2022-04-18T05:39:41.470117Z","shell.execute_reply.started":"2022-04-18T05:39:28.158699Z","shell.execute_reply":"2022-04-18T05:39:41.469203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Below is not used anymore, the best_valid model is auto saved with train()\n# to_save = {'model': model.state_dict(),\n#            'config': config,\n#            'Dconfig': Dconfig,\n#            'optimizer': optimizer,\n#            'lr_s':None}\n# torch.save(to_save, save_path)\n\no = get_csv(dataloader_test, model)\no.to_csv(save_path.replace('.pt', '.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:56:52.498652Z","iopub.execute_input":"2022-04-18T03:56:52.498988Z","iopub.status.idle":"2022-04-18T03:57:03.664175Z","shell.execute_reply.started":"2022-04-18T03:56:52.49895Z","shell.execute_reply":"2022-04-18T03:57:03.663462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# state = torch.load('../input/lastattn-best/lastATNN_best.pt')\n# model.load_state_dict(state['model'])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:48:13.173068Z","iopub.execute_input":"2022-04-18T06:48:13.173318Z","iopub.status.idle":"2022-04-18T06:48:20.283384Z","shell.execute_reply.started":"2022-04-18T06:48:13.173291Z","shell.execute_reply":"2022-04-18T06:48:20.282653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(dataloader_test, model = model, lossfn = lossfn, has_mask = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:48:24.295629Z","iopub.execute_input":"2022-04-18T06:48:24.296328Z","iopub.status.idle":"2022-04-18T06:48:29.418356Z","shell.execute_reply.started":"2022-04-18T06:48:24.296291Z","shell.execute_reply":"2022-04-18T06:48:29.417545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"# Dconfig = {'Dmodel_name':'BiLSTMModel',\n#            'freeze_pretrained':True,\n#            'lstm_num_layer':1, \n#            'classifier_dropout':0.1,\n#            'last_hidden_layer':5,\n#            'epochs': 20}\n# model1 = BiLSTMModel(pretrain_model, tokenizer, **Dconfig).to(device)\n# state1 = torch.load('../input/brian/biLSTM_layer1.pt')\n# assert Dconfig == state1['Dconfig']\n# model1.load_state_dict(state1['model'])\n# print(test(dataloader_test, model = model1, lossfn = lossfn, has_mask = True))\n\n# Dconfig = {'Dmodel_name':'LastAttnModel',\n#            'freeze_pretrained':True,\n#            'classifier_dropout':0.1,\n#            'auxiliary_head':None,\n#            'last_hidden_layer':5,\n#            'all_CLS_attn':True,\n#            'epochs': 20}\n# model2 = LastAttnModel(pretrain_model, tokenizer, **Dconfig).to(device)\n# state2 = torch.load('./lastATNN.pt')\n# assert Dconfig == state2['Dconfig']\n# model2.load_state_dict(state2['model'])\n# print(test(dataloader_test, model = model2, lossfn = lossfn, has_mask = True))\n\n# print(test(dataloader_test, model = [model1, model2], lossfn = lossfn, has_mask = True))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T03:56:43.230423Z","iopub.status.idle":"2022-04-18T03:56:43.231217Z","shell.execute_reply.started":"2022-04-18T03:56:43.230928Z","shell.execute_reply":"2022-04-18T03:56:43.230957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In case You want know what is `pred`  \n# Check the attension weight to swear word, only useful for LastAttnModel()","metadata":{}},{"cell_type":"code","source":"cnt = 0\nfor datas in dataloader_valid:\n    cnt += 1\n    if cnt < 30:\n        continue\n    model.eval()\n    count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n    pred = model(list(tweet), True)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:16:36.422628Z","iopub.execute_input":"2022-04-18T10:16:36.422933Z","iopub.status.idle":"2022-04-18T10:16:37.49529Z","shell.execute_reply.started":"2022-04-18T10:16:36.422899Z","shell.execute_reply":"2022-04-18T10:16:37.494554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred, w = pred[0], pred[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:16:37.496909Z","iopub.execute_input":"2022-04-18T10:16:37.497336Z","iopub.status.idle":"2022-04-18T10:16:37.501148Z","shell.execute_reply.started":"2022-04-18T10:16:37.497296Z","shell.execute_reply":"2022-04-18T10:16:37.500554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 30\nfor idx in range(0, 32):\n    if target[idx] != 0:\n        continue\n    print(f\"Prediction: {pred[idx].argmax().item()}. Ground-Truth: {target[idx].item()}\")\n    print(tweet[idx])\n    w_pure = w[idx][w[idx] !=0 ][1:].cpu().detach().numpy()\n    tokens_pure = tokenizer.tokenize(tweet[idx])\n    tokens = [f\"{i}_{tokens_pure[i]}\" for i in range(len(tokens_pure))]\n    full = list(zip(tokens, w_pure))\n    display(full)\n    break\n# here `w_pure.sum() != 1` because the <cls> token score is not included, so will be small then one.","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:16:37.57703Z","iopub.execute_input":"2022-04-18T10:16:37.577276Z","iopub.status.idle":"2022-04-18T10:16:37.588887Z","shell.execute_reply.started":"2022-04-18T10:16:37.57725Z","shell.execute_reply":"2022-04-18T10:16:37.587792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine tokens to word by handling '##'\nout = []\nout.append(list(full[0]))\nfor i in range(1, len(full)):\n    if full[i][0][:2] == '##':\n        out[-1][0] += full[i][0][2:]\n        out[-1][1] += full[i][1]\n    else:\n        out.append(list(full[i]))\nout","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:16:38.022569Z","iopub.execute_input":"2022-04-18T10:16:38.022958Z","iopub.status.idle":"2022-04-18T10:16:38.031269Z","shell.execute_reply.started":"2022-04-18T10:16:38.022925Z","shell.execute_reply":"2022-04-18T10:16:38.030536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score by attention score\nprint(f\"Prediction: {pred[idx].argmax().item()}. Ground-Truth: {target[idx].item()}\")\nprint(tweet[idx])\nsorted(out, key=lambda out: out[1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:16:38.474628Z","iopub.execute_input":"2022-04-18T10:16:38.475182Z","iopub.status.idle":"2022-04-18T10:16:38.485087Z","shell.execute_reply.started":"2022-04-18T10:16:38.475148Z","shell.execute_reply":"2022-04-18T10:16:38.484335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ndf = pd.DataFrame(w_pure, index=tokens_pure, columns=['attn_weight'])\nplt.figure(figsize=(10,1.3))\nfig = sns.heatmap(df.T, fmt=\"g\", cmap='viridis',xticklabels=tokens_pure)\nfig.get_figure().savefig(\"out.png\",bbox_inches='tight', dpi=300) ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:16:38.81108Z","iopub.execute_input":"2022-04-18T10:16:38.811644Z","iopub.status.idle":"2022-04-18T10:16:39.225411Z","shell.execute_reply.started":"2022-04-18T10:16:38.811608Z","shell.execute_reply":"2022-04-18T10:16:39.224549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
