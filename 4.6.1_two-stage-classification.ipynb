{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !wget https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv\n",
    "# Put me under data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import btorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification,AutoModel\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "seed = 888\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path of data and the name of pretrained weights\n",
    "# path = '../input/nlpdata/labeled_data.csv'\n",
    "path = './data/labeled_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path, index_col = 0)\n",
    "df = shuffle(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"@likemicah: Y'all dumb ass niggas be so press...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11176</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I snorted shit with more girls in my presence ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "364        3            1                   2        0      1   \n",
       "11176      3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \n",
       "364    \"@likemicah: Y'all dumb ass niggas be so press...  \n",
       "11176  I snorted shit with more girls in my presence ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 364 & 11176, Just to make sure the experiment is reproducible\n",
    "df.iloc[0:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btorch.utils.get_class_weight(df['class'].to_numpy(), 'sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>@WebAntOnYT btw Jim crow laws were supported b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Before I let a pussy cut my hair I just will l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16962</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @NASAWatch: .@tweetsoutloud we found your b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20491</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @sorryimalex: idkmadz tbh I've seen you in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@PaulConroy @habeshasuperman oh, it's coming u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "5278       3            0                   0        3      2   \n",
       "8208       3            0                   3        0      1   \n",
       "16962      3            0                   0        3      2   \n",
       "20491      3            0                   2        1      1   \n",
       "4523       3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \n",
       "5278   @WebAntOnYT btw Jim crow laws were supported b...  \n",
       "8208   Before I let a pussy cut my hair I just will l...  \n",
       "16962  RT @NASAWatch: .@tweetsoutloud we found your b...  \n",
       "20491  RT @sorryimalex: idkmadz tbh I've seen you in ...  \n",
       "4523   @PaulConroy @habeshasuperman oh, it's coming u...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@StarbucksSanae Are you sure about that, slant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@Coley_Cee bitch dats yo sahara desert ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11082</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I never seen a Asian or Chinese bitch pregnant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22484</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>This bitch wanna smoke all of my weed,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>&amp;#8220;@GianneNichole: Lakers really trash.&amp;#8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "4883       3            1                   0        2      2   \n",
       "2946       3            0                   3        0      1   \n",
       "11082      3            1                   2        0      1   \n",
       "22484      3            0                   3        0      1   \n",
       "1305       3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "4883   @StarbucksSanae Are you sure about that, slant...  \n",
       "2946          @Coley_Cee bitch dats yo sahara desert ass  \n",
       "11082  I never seen a Asian or Chinese bitch pregnant...  \n",
       "22484             This bitch wanna smoke all of my weed,  \n",
       "1305   &#8220;@GianneNichole: Lakers really trash.&#8...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 6)\n",
      "(2478, 6)\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test\n",
    "train_data = df.sample(frac = 0.8)\n",
    "valid_data = df.drop(train_data.index)\n",
    "test_data = valid_data.sample(frac = 0.5)\n",
    "valid_data = valid_data.drop(test_data.index)\n",
    "\n",
    "display(train_data.head())\n",
    "print(\"===================================\")\n",
    "display(test_data.head())\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset\n",
    "\n",
    "The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data df contains 5 columns:\n",
    "\n",
    "count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, model_name=None,  train = True, device='cuda'):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        device = self.device\n",
    "        count = torch.LongTensor([self.df.iloc[idx]['count']])\n",
    "        hate_speech = torch.LongTensor([self.df.iloc[idx]['hate_speech']])\n",
    "        offensive_language = torch.LongTensor([self.df.iloc[idx]['offensive_language']])\n",
    "        neither = torch.LongTensor([self.df.iloc[idx]['neither']])\n",
    "        target = torch.LongTensor([self.df.iloc[idx]['class']])\n",
    "        tweet = self.df.iloc[idx]['tweet']\n",
    "        return (count.to(device), hate_speech.to(device), offensive_language.to(device)\n",
    "                , neither.to(device), target.to(device), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]], device='cuda:0'), tensor([[0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), tensor([[3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0]], device='cuda:0'), tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [3]], device='cuda:0'), tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), ('RT @BCniggas: I know plenty of hoes in love with they niggas but it be the same bitch fuckin niggas', 'fuck ray rice ,someone crack that bitch', 'Yo bitch she boof she got that throwbac pussy !', \"RT @UFlorida: @GatorZoneFB It's a bird...It's a plane...It's Jake McGee! http://t.co/WVdLScoDvw\", 'RT @CourtneyyKay: Cruising down the street in my 64, jockin the bitches, slappin the hoes', 'Early bird gets the worm! &#128027;&#128036;')]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(df, device=device)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size= 6, shuffle=True)\n",
    "output = next(iter(dataloader))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_params(model, exclude_freeze=False):\n",
    "    \"\"\"calculate the number of parameters in a model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model\n",
    "        exclude_freeze (bool, optional): Whether to count the frozen layer. Defaults to False.\n",
    "    \"\"\"\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        if exclude_freeze and p.requires_grad is False:\n",
    "            continue\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "def finetune(\n",
    "        model: nn.Module,\n",
    "        base_lr: float,\n",
    "        groups,\n",
    "        ignore_the_rest: bool = False,\n",
    "        raw_query: bool = False,\n",
    "        regex=False):\n",
    "    \"\"\" This is something call per-parameter options\n",
    "\n",
    "    Separate out the finetune parameters with a learning rate for each layers of parameters\n",
    "    This function only support setting a different learning rate for each layer's arameter.\n",
    "    Depending on the optimizer, you can set extra parameter for that layer for the optmizer -> See Notes \n",
    "    If you freeze layer using this function and want to unfreeze it later:\n",
    "    See https://discuss.pytorch.org/t/correct-way-to-freeze-layers/26714/2\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Pytorch Model\n",
    "        base_lr (float): learning rate of all layers\n",
    "        groups (Dict[str, float]): key is `name` of layers, value is the `extra_lr` (or False).\n",
    "          all layers that contains that `name` will have `lr` of base_lr*extra_lr.\n",
    "          it uses fnmatch|regex to check whether a layer contains that `name`.\n",
    "          fnmatch is matching structure like `layer1*`, `layer?.conv?.`, `*conv2*`, etc...\n",
    "          regex is the comman regex matching.\n",
    "          Hence, `name` here is either fnmatch or regex expression if using raw_query.\n",
    "          If `float` is False: those layers with `name` will be freeze. \n",
    "          In particular, they will not be included in the return output and require_grad will be set to False\n",
    "        ignore_the_rest (bool, optional): Include the remaining layer that are not stated in `grouprs` or not. Defaults to False.\n",
    "        raw_query (bool, optional): Modify the keys of `groups` as f'*{key}*' if False. Only useful when `regex=False`\n",
    "          Do not do any modification to the keys of `groups` if True. Defaults to False.\n",
    "        regex (bool, optional): Use regex instead of fnmatch on keys of groups. Defaults to False.\n",
    "          This will overrride raw_query to True. \n",
    "          Notice: `regex=False` is depracted\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Union[float, Iterable]]]: list of dict that has two or more key-value pair.\n",
    "          The first one is feature generation layers. [those layers must start with `features` name] <usually is backbone>\n",
    "            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'query':query, 'lr':base_lr*groups[groups.keys()]]\n",
    "          The remaining are all others layer. [all others params for last one, if ignore_the_rest = False]\n",
    "            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'lr':base_lr]\n",
    "\n",
    "    Examples:\n",
    "        >>> model = models.resnet50()\n",
    "        >>> # all layers that has name start with `layer1 and layer2` will have learning rate `0.001*0.01`\n",
    "        >>> # all layers that has name start with `layer3` will be froozen`\n",
    "        >>> # all layers that has name start with `layer4` will have learning rate `0.001*0.001`\n",
    "        >>> # for all other layers will have the base_lr `0.001`\n",
    "        >>> model_params = finetune(model, base_lr=0.001, groups={'^layer[1-2].*': 0.01, '^layer3.*': False, '^layer4.*': 0.001}, regex=True)\n",
    "        >>> # setting extra parameter (other than learning rate) for that optimizer\n",
    "        >>> # the second param_group `layer4` will have weight_decay 1e-2\n",
    "        >>> model_params[1]['weight_decay'] = 1e-2\n",
    "        >>> # init optimizer with the above setting\n",
    "        >>> # the argument under `torch.optim.SGD` will be overrided by finetune() if they exist.\n",
    "        >>> # For example, all model_params will have weight_decay=5e-3 except model_params[1]\n",
    "        >>> optimizer = torch.optim.SGD(model_params, momentum=0.9, lr=0.1, weight_decay=5e-3)\n",
    "    \"\"\"\n",
    "    if regex:\n",
    "        raw_query = True\n",
    "    else:\n",
    "        warnings.warn(\"regex=False is deprecated; use regex=True\", DeprecationWarning)\n",
    "    # Deal with Freeze Later\n",
    "    freeze_group = dict()\n",
    "    freeze = False\n",
    "    for k,v in groups.items():\n",
    "        if v is False:\n",
    "            freeze_group[k] = 1\n",
    "            freeze=True\n",
    "    for k in freeze_group.keys():\n",
    "        del groups[k]\n",
    "    freeze_group = \"(\" + \")|(\".join(freeze_group) + \")\"\n",
    "\n",
    "    parameters = [\n",
    "        dict(params=[],\n",
    "             names=[],\n",
    "             query=query if raw_query else '*' + query + '*',\n",
    "             lr = lr * base_lr,\n",
    "             initial_lr = lr * base_lr) for query, lr in groups.items()\n",
    "    ]\n",
    "    rest_parameters = dict(params=[], names=[], lr=base_lr, initial_lr=base_lr)\n",
    "    for k, v in model.named_parameters():\n",
    "        rest = 0\n",
    "        if freeze and regex and re.match(freeze_group, k):\n",
    "            v.requires_grad = False\n",
    "            continue\n",
    "        for group in parameters:\n",
    "            if not regex and fnmatch(k, group['query']):\n",
    "                group['params'].append(v)\n",
    "                group['names'].append(k)\n",
    "                rest = 1\n",
    "                break\n",
    "            elif regex and re.compile(group['query']).search(k):\n",
    "                group['params'].append(v)\n",
    "                group['names'].append(k)\n",
    "                rest = 1\n",
    "                break\n",
    "        if rest == 0:\n",
    "            rest_parameters['params'].append(v)\n",
    "            rest_parameters['names'].append(k)\n",
    "\n",
    "    if not ignore_the_rest:\n",
    "        parameters.append(rest_parameters)\n",
    "    for group in parameters:\n",
    "        group['params'] = iter(group['params'])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if freeze_pretrained:\n",
    "            print(\"You are freezing the BERT\")\n",
    "            for name, p in self.model.named_parameters():\n",
    "                if 'classifier' not in name:\n",
    "                    p.requires_grad = False\n",
    "        print(f\"Total number of params: {number_params(self.model)}\")\n",
    "        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n",
    "\n",
    "    def forward(self, src, has_mask=False):\n",
    "        # print(src)\n",
    "        output = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n",
    "        output = torch.LongTensor(output['input_ids']).to(device)\n",
    "        if has_mask == True:\n",
    "            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n",
    "            output = self.model(output, attention_mask=attention_mask)\n",
    "        else:\n",
    "            output = self.model(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastAttnModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Use the [CLS] as query and all other output as key and values.\n",
    "    Pass it to a Multi-Head Attention, then a Linear classifier\n",
    "    \n",
    "    Args:\n",
    "      auxiliary_head(list(int)): Only used when training\n",
    "        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n",
    "        - See BertConfig['num_hidden_layers'] for total number of layers\n",
    "        - EG: `auxiliary_head=[10,11,12]`.\n",
    "      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n",
    "\n",
    "    Examples:\n",
    "      tweet = iter(dataloader_train).next()[-1]\n",
    "      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n",
    "      out, attn_weight = modelA(list(tweet))\n",
    "      print(out.shape, attn_weight.shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n",
    "      \n",
    "    Returns:\n",
    "      list(tensor): the first tensor is the prediction, the second is the attention weight\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrain_model, tokenizer, \n",
    "                 last_attn_num_head = 8,\n",
    "                 classifier_hidden_dim = 512, \n",
    "                 classifier_dropout = 0,\n",
    "                 num_labels = 3, \n",
    "                 freeze_pretrained=False,\n",
    "                 auxiliary_head=None,\n",
    "                 last_hidden_layer=-1):\n",
    "        super(LastAttnModel, self).__init__()\n",
    "        self.pretrain_model = pretrain_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.auxiliary_head = auxiliary_head\n",
    "        self.num_layers = len(pretrain_model.encoder.layer)\n",
    "        self.last_hidden_layer = last_hidden_layer\n",
    "        \n",
    "        if freeze_pretrained:\n",
    "            if self.auxiliary_head is not None:\n",
    "              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n",
    "            print(\"You are freezing the BERT pertrain\")\n",
    "            for name, p in self.pretrain_model.named_parameters():\n",
    "                if 'classifier' not in name:\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n",
    "        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(embed_size, classifier_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(classifier_hidden_dim, num_labels)\n",
    "        )\n",
    "\n",
    "        # deal with aux \n",
    "        if self.auxiliary_head is not None:\n",
    "          self.aux_classifiers = nn.ModuleList()\n",
    "          for i in self.auxiliary_head:\n",
    "            self.aux_classifiers.append(nn.Sequential(\n",
    "              nn.Linear(embed_size, classifier_hidden_dim),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(classifier_dropout),\n",
    "              nn.Linear(classifier_hidden_dim, num_labels)\n",
    "            ))\n",
    "\n",
    "\n",
    "        print(f\"Total number of params: {number_params(self)}\")\n",
    "        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n",
    "    def forward(self, src, has_mask=False):\n",
    "        # print(src)\n",
    "        out = []\n",
    "        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n",
    "        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n",
    "        if has_mask == True:\n",
    "            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n",
    "            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n",
    "            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n",
    "        else:\n",
    "            pre_train_output = self.pretrain_model(inputs)\n",
    "        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n",
    "        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n",
    "        last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n",
    "        last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n",
    "        atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n",
    "        last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n",
    "                                                    key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n",
    "        last_attn_out = last_attn_out.squeeze(1) #(N,E)\n",
    "        output = self.final_classifier(last_attn_out)\n",
    "        out += [output, last_attn_w]\n",
    "        ## auxiliary_head\n",
    "        if self.auxiliary_head is not None:\n",
    "          if \"hidden_states\" not in pre_train_output:\n",
    "            raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n",
    "          for idx in range(len(self.aux_classifiers)):\n",
    "            hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n",
    "            out.append(self.aux_classifiers[idx](hidden_cls))\n",
    "        return out\n",
    "    \n",
    "# ## Usage\n",
    "# tweet = iter(dataloader_train).next()[-1]\n",
    "# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n",
    "# out, attn_weight = modelA(list(tweet))\n",
    "# print(out.shape, attn_weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are freezing the BERT pertrain\n",
      "Total number of params: 112239362\n",
      "Total number of trainable params: 2757122\n"
     ]
    }
   ],
   "source": [
    "traindataset1 = Dataset(train_data)\n",
    "dataloader_train1 = torch.utils.data.DataLoader(traindataset1, batch_size=32, shuffle=True)\n",
    "\n",
    "validdataset1 = Dataset(valid_data)\n",
    "dataloader_valid1 = torch.utils.data.DataLoader(validdataset1, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset1 = Dataset(test_data)\n",
    "dataloader_test1 = torch.utils.data.DataLoader(test_dataset1, batch_size=32, shuffle=False)\n",
    "\n",
    "lossfn1 = nn.CrossEntropyLoss().to(device)\n",
    "# lossfn = nn.CrossEntropyLoss(weight=torch.tensor([5.7769, 0.4305, 1.9844])).to(device)\n",
    "# model = LanguageModel(model_name, freeze_pretrained=False).to(device)\n",
    "model_name1 = 'bert-base-uncased'\n",
    "config1 = AutoConfig.from_pretrained(\n",
    "    model_name1, \n",
    "    num_labels = 2,\n",
    "    output_hidden_states = True,\n",
    "    output_attention = False,\n",
    "    hidden_dropout_prob = 0.2,\n",
    ") \n",
    "print(config1)\n",
    "pretrain_model1 = AutoModel.from_pretrained(\n",
    "    model_name1,\n",
    "    config = config1\n",
    ").to(device)\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "model1 = LastAttnModel(pretrain_model1, tokenizer1, freeze_pretrained=True, num_labels = 2,\n",
    "                      classifier_dropout=0.2,\n",
    "                      auxiliary_head=None,\n",
    "                      last_hidden_layer=5).to(device)\n",
    "# model_params = finetune(model, base_lr=1e-4, \n",
    "#                         groups={'^pretrain_model.encoder.layer.([0-7])\\..*': False, \n",
    "#                                 '^pretrain_model.encoder.layer.([8-9]|1[012]).*': 0.01, \n",
    "#                                 '^pretrain_model.pooler.*': False,\n",
    "#                                 '^pretrain_model.embeddings.*':False},\n",
    "#                         regex=True)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'bert-base-uncased'\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     model_name, \n",
    "#     output_hidden_states = True,\n",
    "#     output_attention = False\n",
    "# ) \n",
    "# print(config)\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     model_name,\n",
    "#     config = config\n",
    "# ).to(device)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet = iter(dataloader_train).next()[-1]\n",
    "# tokens = tokenizer(list(tweet), padding=True, truncation=True, max_length=50)\n",
    "# print(tokens.keys())\n",
    "# inputs = torch.LongTensor(tokens['input_ids']).to(device)\n",
    "# atten_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n",
    "# print(inputs.shape)\n",
    "# hiddens = model(inputs)\n",
    "# print(hiddens.keys())\n",
    "# print(hiddens['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (inputs[0]!=0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atten_mask_pad = (inputs == 0)\n",
    "# print(atten_mask_pad.shape)\n",
    "# print(atten_mask_pad[:,1:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train1(dataloader_train, dataloader_valid = None, dataloader_test = None, model1 = None, \n",
    "          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n",
    "    \n",
    "    trainloss = []\n",
    "    validloss = []\n",
    "    testloss = []\n",
    "    trainscore = []\n",
    "    validscore = []\n",
    "    testscore = []\n",
    "    \n",
    "    traincm = []\n",
    "    validcm = []\n",
    "    testcm = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model1.train()\n",
    "        averageloss = 0\n",
    "        averagescore = 0\n",
    "        for datas in tqdm(dataloader_train):\n",
    "            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "            \n",
    "            # 0 = neither hate speech nor offensive language\n",
    "            # 1 = either hate speech or offensive language\n",
    "            target_new = torch.LongTensor([[0] if t == 2 else [1] for t in target]).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model1(list(tweet), has_mask)\n",
    "            loss = lossfn(pred[0], target_new.squeeze(1))\n",
    "            # aux head\n",
    "            for j in range(2, len(pred)):\n",
    "              loss+=0.3*lossfn(pred[j], target_new.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            f1score = sklearn.metrics.f1_score(target_new.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n",
    "            averageloss += loss.item()/len(dataloader_train)\n",
    "            averagescore += f1score/len(dataloader_train)\n",
    "        trainloss.append(averageloss)\n",
    "        trainscore.append(averagescore)\n",
    "        if dataloader_valid is not None:\n",
    "            model1.eval()\n",
    "            averageloss = 0\n",
    "            averagescore = 0\n",
    "            for datas in tqdm(dataloader_valid):\n",
    "                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "                \n",
    "                # 0 = neither hate speech nor offensive language\n",
    "                # 1 = either hate speech or offensive language\n",
    "                target_new = torch.LongTensor([[0] if t == 2 else [1] for t in target]).to(device)\n",
    "                \n",
    "                pred = model1(list(tweet), has_mask)\n",
    "                loss = lossfn(pred[0], target_new.squeeze(1))\n",
    "                f1score = sklearn.metrics.f1_score(target_new.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n",
    "                averageloss += loss.item()/len(dataloader_valid)\n",
    "                averagescore += f1score/len(dataloader_valid)\n",
    "            validloss.append(averageloss)\n",
    "            validscore.append(averagescore)\n",
    "        if dataloader_test is not None:\n",
    "            model1.eval()\n",
    "            averageloss = 0\n",
    "            averagescore = 0\n",
    "            for datas in tqdm(dataloader_test):\n",
    "                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "                \n",
    "                # 0 = neither hate speech nor offensive language\n",
    "                # 1 = either hate speech or offensive language\n",
    "                target_new = torch.LongTensor([[0] if t == 2 else [1] for t in target]).to(device)\n",
    "                \n",
    "                pred = model1(list(tweet), has_mask)\n",
    "                loss = lossfn(pred[0], target_new.squeeze(1))\n",
    "                f1score = sklearn.metrics.f1_score(target_new.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n",
    "                averageloss += loss.item()/len(dataloader_test)\n",
    "                averagescore += f1score/len(dataloader_test)\n",
    "            testloss.append(averageloss)\n",
    "            testscore.append(averagescore)\n",
    "            \n",
    "            \n",
    "        print(f\"epoch: {i+1}\")\n",
    "        print(f\"train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n",
    "        if dataloader_valid is not None:\n",
    "            print(f\"validation loss: {validloss[-1]}, validation f1score: {validscore[-1]}\")\n",
    "        if dataloader_test is not None:\n",
    "            print(f\"test loss: {testloss[-1]}, test f1score: {testscore[-1]}\")\n",
    "\n",
    "    return trainloss, validloss, testloss, trainscore, validscore, testscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 0.14979061067044266, train f1score: 0.9336889490668375\n",
      "validation loss: 0.10726041886477898, validation f1score: 0.957026564288009\n",
      "test loss: 0.10960878220458445, test f1score: 0.9573215041028394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "train loss: 0.10018462354257215, train f1score: 0.9611723125932705\n",
      "validation loss: 0.1083862581409705, validation f1score: 0.9555316146770725\n",
      "test loss: 0.10841767424836947, test f1score: 0.9573881531685822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "train loss: 0.09202000658316985, train f1score: 0.9655884545280914\n",
      "validation loss: 0.10432043170126584, validation f1score: 0.9565913849133489\n",
      "test loss: 0.10520326635704783, test f1score: 0.9598588159235296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "train loss: 0.08283704971451494, train f1score: 0.969954804119201\n",
      "validation loss: 0.11366790375457361, validation f1score: 0.9589319907229515\n",
      "test loss: 0.11912514939784814, test f1score: 0.960048669271405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "train loss: 0.07628191690676062, train f1score: 0.9723029400280312\n",
      "validation loss: 0.12630009364623287, validation f1score: 0.9582615769498036\n",
      "test loss: 0.12757198941649103, test f1score: 0.9559714655692211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "train loss: 0.07051364952818519, train f1score: 0.9732122773575447\n",
      "validation loss: 0.13864749460928863, validation f1score: 0.9509963425005151\n",
      "test loss: 0.1346715210716073, test f1score: 0.9568538243543364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "train loss: 0.06307000820956854, train f1score: 0.975583652066253\n",
      "validation loss: 0.12323950453756902, validation f1score: 0.9587199840781723\n",
      "test loss: 0.12359811118147215, test f1score: 0.9579547986490029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "train loss: 0.058878085353682136, train f1score: 0.9771028175012676\n",
      "validation loss: 0.12243100257369521, validation f1score: 0.9524811100611483\n",
      "test loss: 0.12434970503314759, test f1score: 0.9555048647110438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "train loss: 0.0555835037802031, train f1score: 0.978594263091581\n",
      "validation loss: 0.11609552684837043, validation f1score: 0.9487079246544438\n",
      "test loss: 0.12420538414078648, test f1score: 0.9468179213782166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "train loss: 0.05248453440107103, train f1score: 0.979581327858186\n",
      "validation loss: 0.13224543657344887, validation f1score: 0.9556436968579249\n",
      "test loss: 0.12795496846322352, test f1score: 0.9571286016884881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\n",
      "train loss: 0.04738443526063834, train f1score: 0.9806456914912757\n",
      "validation loss: 0.13620633535454263, validation f1score: 0.9458791894607448\n",
      "test loss: 0.1353236912191511, test f1score: 0.9483087303230727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12\n",
      "train loss: 0.04443426501351386, train f1score: 0.9838535020248055\n",
      "validation loss: 0.13015654073574415, validation f1score: 0.9482584540670904\n",
      "test loss: 0.12349964137702515, test f1score: 0.9527276841012294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13\n",
      "train loss: 0.04228087439846754, train f1score: 0.9834539501155998\n",
      "validation loss: 0.15135527977373647, validation f1score: 0.9539405152503652\n",
      "test loss: 0.15188027260592207, test f1score: 0.9552055294910252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:18<00:00,  7.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\n",
      "train loss: 0.041041821803880725, train f1score: 0.9829826685948024\n",
      "validation loss: 0.14380276210319543, validation f1score: 0.9545875243896446\n",
      "test loss: 0.1426421739399815, test f1score: 0.9564896701502049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  7.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\n",
      "train loss: 0.03736743032131926, train f1score: 0.984272990536984\n",
      "validation loss: 0.17006568043516615, validation f1score: 0.9468090656167948\n",
      "test loss: 0.17096445903156662, test f1score: 0.9517850452886367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:19<00:00,  7.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  7.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16\n",
      "train loss: 0.03560971393200171, train f1score: 0.9847898937970024\n",
      "validation loss: 0.15671424704562056, validation f1score: 0.9517230579709405\n",
      "test loss: 0.1464689866824745, test f1score: 0.9548502042948914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:26<00:00,  7.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:10<00:00,  7.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:10<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17\n",
      "train loss: 0.03882596704446062, train f1score: 0.9841594687399614\n",
      "validation loss: 0.15558650512367686, validation f1score: 0.9500426217837971\n",
      "test loss: 0.14273544768534174, test f1score: 0.9547701741213658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:30<00:00,  6.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:10<00:00,  7.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:10<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18\n",
      "train loss: 0.03432367644297387, train f1score: 0.985065913774087\n",
      "validation loss: 0.17536417549686467, validation f1score: 0.9520648255844363\n",
      "test loss: 0.1686706265695064, test f1score: 0.9531854987072717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:26<00:00,  7.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:10<00:00,  7.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:10<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19\n",
      "train loss: 0.03518129933267292, train f1score: 0.9846050386289664\n",
      "validation loss: 0.14812211007273823, validation f1score: 0.957023702062327\n",
      "test loss: 0.15462304345251487, test f1score: 0.9554984432326289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 620/620 [01:21<00:00,  7.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:09<00:00,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20\n",
      "train loss: 0.031038437609171403, train f1score: 0.9861789266019946\n",
      "validation loss: 0.15884340264258354, validation f1score: 0.9508958745970569\n",
      "test loss: 0.15633568211566082, test f1score: 0.9549776779973497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs1 = 20\n",
    "trainloss1, validloss1, testloss1, trainscore1, validscore1, testscore1 = train1(\n",
    "    dataloader_train1, dataloader_valid1, dataloader_test1,\n",
    "    model1 = model1,\n",
    "    optimizer = optimizer1, lossfn = lossfn1, epochs = epochs1, has_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14979061067044266,\n",
       " 0.10018462354257215,\n",
       " 0.09202000658316985,\n",
       " 0.08283704971451494,\n",
       " 0.07628191690676062,\n",
       " 0.07051364952818519,\n",
       " 0.06307000820956854,\n",
       " 0.058878085353682136,\n",
       " 0.0555835037802031,\n",
       " 0.05248453440107103,\n",
       " 0.04738443526063834,\n",
       " 0.04443426501351386,\n",
       " 0.04228087439846754,\n",
       " 0.041041821803880725,\n",
       " 0.03736743032131926,\n",
       " 0.03560971393200171,\n",
       " 0.03882596704446062,\n",
       " 0.03432367644297387,\n",
       " 0.03518129933267292,\n",
       " 0.031038437609171403]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10726041886477898,\n",
       " 0.1083862581409705,\n",
       " 0.10432043170126584,\n",
       " 0.11366790375457361,\n",
       " 0.12630009364623287,\n",
       " 0.13864749460928863,\n",
       " 0.12323950453756902,\n",
       " 0.12243100257369521,\n",
       " 0.11609552684837043,\n",
       " 0.13224543657344887,\n",
       " 0.13620633535454263,\n",
       " 0.13015654073574415,\n",
       " 0.15135527977373647,\n",
       " 0.14380276210319543,\n",
       " 0.17006568043516615,\n",
       " 0.15671424704562056,\n",
       " 0.15558650512367686,\n",
       " 0.17536417549686467,\n",
       " 0.14812211007273823,\n",
       " 0.15884340264258354]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validloss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10960878220458445,\n",
       " 0.10841767424836947,\n",
       " 0.10520326635704783,\n",
       " 0.11912514939784814,\n",
       " 0.12757198941649103,\n",
       " 0.1346715210716073,\n",
       " 0.12359811118147215,\n",
       " 0.12434970503314759,\n",
       " 0.12420538414078648,\n",
       " 0.12795496846322352,\n",
       " 0.1353236912191511,\n",
       " 0.12349964137702515,\n",
       " 0.15188027260592207,\n",
       " 0.1426421739399815,\n",
       " 0.17096445903156662,\n",
       " 0.1464689866824745,\n",
       " 0.14273544768534174,\n",
       " 0.1686706265695064,\n",
       " 0.15462304345251487,\n",
       " 0.15633568211566082]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testloss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9336889490668375,\n",
       " 0.9611723125932705,\n",
       " 0.9655884545280914,\n",
       " 0.969954804119201,\n",
       " 0.9723029400280312,\n",
       " 0.9732122773575447,\n",
       " 0.975583652066253,\n",
       " 0.9771028175012676,\n",
       " 0.978594263091581,\n",
       " 0.979581327858186,\n",
       " 0.9806456914912757,\n",
       " 0.9838535020248055,\n",
       " 0.9834539501155998,\n",
       " 0.9829826685948024,\n",
       " 0.984272990536984,\n",
       " 0.9847898937970024,\n",
       " 0.9841594687399614,\n",
       " 0.985065913774087,\n",
       " 0.9846050386289664,\n",
       " 0.9861789266019946]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainscore1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.957026564288009,\n",
       " 0.9555316146770725,\n",
       " 0.9565913849133489,\n",
       " 0.9589319907229515,\n",
       " 0.9582615769498036,\n",
       " 0.9509963425005151,\n",
       " 0.9587199840781723,\n",
       " 0.9524811100611483,\n",
       " 0.9487079246544438,\n",
       " 0.9556436968579249,\n",
       " 0.9458791894607448,\n",
       " 0.9482584540670904,\n",
       " 0.9539405152503652,\n",
       " 0.9545875243896446,\n",
       " 0.9468090656167948,\n",
       " 0.9517230579709405,\n",
       " 0.9500426217837971,\n",
       " 0.9520648255844363,\n",
       " 0.957023702062327,\n",
       " 0.9508958745970569]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validscore1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9573215041028394,\n",
       " 0.9573881531685822,\n",
       " 0.9598588159235296,\n",
       " 0.960048669271405,\n",
       " 0.9559714655692211,\n",
       " 0.9568538243543364,\n",
       " 0.9579547986490029,\n",
       " 0.9555048647110438,\n",
       " 0.9468179213782166,\n",
       " 0.9571286016884881,\n",
       " 0.9483087303230727,\n",
       " 0.9527276841012294,\n",
       " 0.9552055294910252,\n",
       " 0.9564896701502049,\n",
       " 0.9517850452886367,\n",
       " 0.9548502042948914,\n",
       " 0.9547701741213658,\n",
       " 0.9531854987072717,\n",
       " 0.9554984432326289,\n",
       " 0.9549776779973497]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testscore1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In case You want know what is `pred`  \n",
    "# Check the attension weight to swear word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datas in dataloader_valid1:\n",
    "    model1.eval()\n",
    "    count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "    pred, w = model1(list(tweet), True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.6735,   3.6505],\n",
       "        [-10.4447,  10.4085],\n",
       "        [ -5.5244,   5.4978],\n",
       "        [  1.2984,  -1.3067],\n",
       "        [  2.9445,  -2.9613],\n",
       "        [  7.1286,  -7.1529],\n",
       "        [ -3.9050,   3.8821],\n",
       "        [ -4.5350,   4.5106],\n",
       "        [  0.8427,  -0.8518],\n",
       "        [ -3.7783,   3.7554],\n",
       "        [ -5.4188,   5.3933],\n",
       "        [ -5.3347,   5.3089],\n",
       "        [ -3.0732,   3.0517],\n",
       "        [ -1.5118,   1.4934],\n",
       "        [ -6.8288,   6.7996],\n",
       "        [ -7.7434,   7.7133],\n",
       "        [ -6.4596,   6.4312],\n",
       "        [ -4.7019,   4.6772],\n",
       "        [ -8.7932,   8.7598],\n",
       "        [ -8.0605,   8.0288],\n",
       "        [ -3.3683,   3.3459],\n",
       "        [  3.1295,  -3.1500],\n",
       "        [ -4.3012,   4.2774],\n",
       "        [ -7.9652,   7.9343],\n",
       "        [ -7.0695,   7.0403],\n",
       "        [ -8.1264,   8.0946],\n",
       "        [  0.8340,  -0.8383],\n",
       "        [ -8.0964,   8.0651],\n",
       "        [ -7.2449,   7.2146],\n",
       "        [ -3.0113,   2.9892],\n",
       "        [ -5.8623,   5.8355],\n",
       "        [ -6.0737,   6.0459]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1. Ground-Truth: 1\n",
      "RT @YoungCoke__: \"@1stBlocJeremiah: @1stBlock_Rody yu ah hoe\"lmfao,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rt', 0.0074906293),\n",
       " ('@', 0.012355257),\n",
       " ('young', 0.008500561),\n",
       " ('##co', 0.049675375),\n",
       " ('##ke', 0.008071665),\n",
       " ('_', 0.007797317),\n",
       " ('_', 0.020686295),\n",
       " (':', 0.01042659),\n",
       " ('\"', 0.0077177677),\n",
       " ('@', 0.024243329),\n",
       " ('1st', 0.028999342),\n",
       " ('##bl', 0.0089940075),\n",
       " ('##oc', 0.029942203),\n",
       " ('##jer', 0.032848917),\n",
       " ('##emia', 0.017617648),\n",
       " ('##h', 0.04548478),\n",
       " (':', 0.00777625),\n",
       " ('@', 0.03684309),\n",
       " ('1st', 0.02670883),\n",
       " ('##block', 0.008195631),\n",
       " ('_', 0.058038127),\n",
       " ('rod', 0.0454424),\n",
       " ('##y', 0.052891042),\n",
       " ('yu', 0.08151081),\n",
       " ('ah', 0.2260769),\n",
       " ('ho', 0.017454928),\n",
       " ('##e', 0.008713197),\n",
       " ('\"', 0.0316812),\n",
       " ('l', 0.0074782753),\n",
       " ('##m', 0.019092595),\n",
       " ('##fa', 0.014751794),\n",
       " ('##o', 0.00981262),\n",
       " (',', 0.01100533)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 6\n",
    "print(f\"Prediction: {pred[idx].argmax().item()}. Ground-Truth: {target[idx].item()}\")\n",
    "print(tweet[idx])\n",
    "w_pure = w[idx][w[idx] !=0 ][1:].cpu().detach().numpy()\n",
    "tokens = tokenizer1.tokenize(tweet[idx])\n",
    "full = list(zip(tokens, w_pure))\n",
    "display(full)\n",
    "# here `w_pure.sum() != 1` because the <cls> token score is not included, so will be small then one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rt', 0.0074906293],\n",
       " ['@', 0.012355257],\n",
       " ['youngcoke', 0.0662476],\n",
       " ['_', 0.007797317],\n",
       " ['_', 0.020686295],\n",
       " [':', 0.01042659],\n",
       " ['\"', 0.0077177677],\n",
       " ['@', 0.024243329],\n",
       " ['1stblocjeremiah', 0.1638869],\n",
       " [':', 0.00777625],\n",
       " ['@', 0.03684309],\n",
       " ['1stblock', 0.03490446],\n",
       " ['_', 0.058038127],\n",
       " ['rody', 0.09833344],\n",
       " ['yu', 0.08151081],\n",
       " ['ah', 0.2260769],\n",
       " ['hoe', 0.026168125],\n",
       " ['\"', 0.0316812],\n",
       " ['lmfao', 0.051135283],\n",
       " [',', 0.01100533]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine tokens to word by handling '##'\n",
    "out = []\n",
    "out.append(list(full[0]))\n",
    "for i in range(1, len(full)):\n",
    "    if full[i][0][:2] == '##':\n",
    "        out[-1][0] += full[i][0][2:]\n",
    "        out[-1][1] += full[i][1]\n",
    "    else:\n",
    "        out.append(list(full[i]))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ah', 0.2260769],\n",
       " ['1stblocjeremiah', 0.1638869],\n",
       " ['rody', 0.09833344],\n",
       " ['yu', 0.08151081],\n",
       " ['youngcoke', 0.0662476],\n",
       " ['_', 0.058038127],\n",
       " ['lmfao', 0.051135283],\n",
       " ['@', 0.03684309],\n",
       " ['1stblock', 0.03490446],\n",
       " ['\"', 0.0316812],\n",
       " ['hoe', 0.026168125],\n",
       " ['@', 0.024243329],\n",
       " ['_', 0.020686295],\n",
       " ['@', 0.012355257],\n",
       " [',', 0.01100533],\n",
       " [':', 0.01042659],\n",
       " ['_', 0.007797317],\n",
       " [':', 0.00777625],\n",
       " ['\"', 0.0077177677],\n",
       " ['rt', 0.0074906293]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score by attention score\n",
    "sorted(out, key=lambda out: out[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are freezing the BERT pertrain\n",
      "Total number of params: 112239362\n",
      "Total number of trainable params: 2757122\n"
     ]
    }
   ],
   "source": [
    "train_data2 = train_data.copy(deep=True)\n",
    "train_data2 = train_data2.drop(train_data2.index[train_data2['class'] == 2].tolist())\n",
    "train_data2 = train_data2.reset_index(drop=True)\n",
    "\n",
    "valid_data2 = valid_data.copy(deep=True)\n",
    "valid_data2 = valid_data2.drop(valid_data2.index[valid_data2['class'] == 2].tolist())\n",
    "valid_data2 = valid_data2.reset_index(drop=True)\n",
    "\n",
    "test_data2 = test_data.copy(deep=True)\n",
    "test_data2 = test_data2.drop(test_data2.index[test_data2['class'] == 2].tolist())\n",
    "test_data2 = test_data2.reset_index(drop=True)\n",
    "\n",
    "traindataset2 = Dataset(train_data2)\n",
    "validdataset2 = Dataset(valid_data2)\n",
    "testdataset2 = Dataset(test_data2)\n",
    "\n",
    "dataloader_train2 = torch.utils.data.DataLoader(traindataset2, batch_size=32, shuffle=True)\n",
    "dataloader_valid2 = torch.utils.data.DataLoader(validdataset2, batch_size=32, shuffle=False)\n",
    "dataloader_test2 = torch.utils.data.DataLoader(testdataset2, batch_size=32, shuffle=False)\n",
    "\n",
    "test_data3 = test_data.copy(deep=True)\n",
    "testdataset3 = Dataset(test_data3)\n",
    "dataloader_test3 = torch.utils.data.DataLoader(testdataset3, batch_size=32, shuffle=False)\n",
    "\n",
    "lossfn2 = nn.CrossEntropyLoss().to(device)\n",
    "# lossfn = nn.CrossEntropyLoss(weight=torch.tensor([5.7769, 0.4305, 1.9844])).to(device)\n",
    "# model = LanguageModel(model_name, freeze_pretrained=False).to(device)\n",
    "model_name2 = 'bert-base-uncased'\n",
    "config2 = AutoConfig.from_pretrained(\n",
    "    model_name2, \n",
    "    num_labels = 2,\n",
    "    output_hidden_states = True,\n",
    "    output_attention = False,\n",
    "    hidden_dropout_prob = 0.2,\n",
    ") \n",
    "print(config2)\n",
    "pretrain_model2 = AutoModel.from_pretrained(\n",
    "    model_name2,\n",
    "    config = config2\n",
    ").to(device)\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "model2 = LastAttnModel(pretrain_model2, tokenizer2, freeze_pretrained=True, num_labels = 2,\n",
    "                      classifier_dropout=0.2,\n",
    "                      auxiliary_head=None,\n",
    "                      last_hidden_layer=5).to(device)\n",
    "# model_params = finetune(model, base_lr=1e-4, \n",
    "#                         groups={'^pretrain_model.encoder.layer.([0-7])\\..*': False, \n",
    "#                                 '^pretrain_model.encoder.layer.([8-9]|1[012]).*': 0.01, \n",
    "#                                 '^pretrain_model.pooler.*': False,\n",
    "#                                 '^pretrain_model.embeddings.*':False},\n",
    "#                         regex=True)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(dataloader_train2, dataloader_valid2 = None, dataloader_test2 = None, dataloader_test3 = None, model1 = None, model2 = None,\n",
    "          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n",
    "    \n",
    "    trainloss = []\n",
    "    validloss = []\n",
    "    testloss = []\n",
    "    trainscore = []\n",
    "    validscore = []\n",
    "    testscore = []\n",
    "    \n",
    "    testcounter_all = []\n",
    "    testcounter_either = []\n",
    "    testscores = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model2.train()\n",
    "        averageloss = 0\n",
    "        averagescore = 0\n",
    "        for datas in tqdm(dataloader_train2):\n",
    "            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model2(list(tweet), has_mask)\n",
    "            loss = lossfn(pred[0], target.squeeze(1))\n",
    "            # aux head\n",
    "            for j in range(2, len(pred)):\n",
    "              loss+=0.3*lossfn(pred[j], target.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n",
    "            averageloss += loss.item()/len(dataloader_train2)\n",
    "            averagescore += f1score/len(dataloader_train2)\n",
    "        trainloss.append(averageloss)\n",
    "        trainscore.append(averagescore)\n",
    "        \n",
    "        if dataloader_valid2 is not None:\n",
    "            model2.eval()\n",
    "            averageloss = 0\n",
    "            averagescore = 0\n",
    "            for datas in tqdm(dataloader_valid2):\n",
    "                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "                \n",
    "                pred = model2(list(tweet), has_mask)\n",
    "                loss = lossfn(pred[0], target.squeeze(1))\n",
    "                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n",
    "                averageloss += loss.item()/len(dataloader_valid2)\n",
    "                averagescore += f1score/len(dataloader_valid2)\n",
    "            validloss.append(averageloss)\n",
    "            validscore.append(averagescore)\n",
    "            \n",
    "        if dataloader_test2 is not None:\n",
    "            model2.eval()\n",
    "            averagescore = 0\n",
    "            counter = 0\n",
    "            for datas in tqdm(dataloader_test2):\n",
    "                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "                \n",
    "                pred = model2(list(tweet), has_mask)\n",
    "                loss = lossfn(pred[0], target.squeeze(1))\n",
    "                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n",
    "                averageloss += loss.item()/len(dataloader_test2)\n",
    "                averagescore += f1score/len(dataloader_test2)\n",
    "            testloss.append(averageloss)\n",
    "            testscore.append(averagescore)\n",
    "            \n",
    "        if dataloader_test3 is not None:\n",
    "            model2.eval()\n",
    "            averagescore = 0\n",
    "            counter_all = 0\n",
    "            counter_either = 0\n",
    "            for datas in tqdm(dataloader_test3):\n",
    "                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n",
    "                \n",
    "                model1.eval()\n",
    "                pred1 = model1(list(tweet), True) # predict with stage 1 model\n",
    "                pred1 = pred1[0].argmax(-1)\n",
    "                pred1 = list(map(bool, pred1))\n",
    "                \n",
    "                target_new = target[pred1].to(device)\n",
    "                \n",
    "                tweet_new = [t if pred1[i] else None for i, t in enumerate(tweet)]\n",
    "                tweet_new = list(filter(None, tweet_new))\n",
    "                tweet_new = tuple(tweet_new)\n",
    "                \n",
    "                counter_all += len(tweet)\n",
    "                \n",
    "                pred2 = model2(list(tweet_new), has_mask)\n",
    "                \n",
    "                for idx3 in range(len(pred2[0])):\n",
    "                    \n",
    "                    f1score = sklearn.metrics.f1_score(target_new[idx3].cpu().numpy(), pred2[0].argmax(-1)[idx3].unsqueeze(0).cpu().numpy(), average = 'weighted')\n",
    "                    counter_either += 1\n",
    "                    averagescore += f1score\n",
    "                    \n",
    "            testcounter_all.append(counter_all)\n",
    "            testcounter_either.append(counter_either)\n",
    "            testscores.append(averagescore/testcounter_either[-1])\n",
    "        \n",
    "        \n",
    "        print(f\"epoch: {i+1}\")\n",
    "        print(f\"train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n",
    "        if dataloader_valid2 is not None:\n",
    "            print(f\"validation loss: {validloss[-1]}, validation f1score: {validscore[-1]}\")\n",
    "        if dataloader_test2 is not None:\n",
    "            print(f\"test loss: {testloss[-1]}, test f1score: {testscore[-1]}\")\n",
    "        if dataloader_test3 is not None:\n",
    "            print(f\"final test f1score: {testscores[-1]}, {testcounter_either[-1]}/{testcounter_all[-1]}\")\n",
    "        \n",
    "            \n",
    "    return trainloss, validloss, testloss, trainscore, validscore, testscore, testscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:07<00:00,  7.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 0.19345033957890065, train f1score: 0.911747642585105\n",
      "validation loss: 0.17875599872786552, validation f1score: 0.9258048982993812\n",
      "test loss: 0.36609898795811197, test f1score: 0.9182416451269046\n",
      "final test f1score: 0.8928909952606635, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:06<00:00,  7.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  7.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:17<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "train loss: 0.16588398793445125, train f1score: 0.9232315490981621\n",
      "validation loss: 0.17153550492366776, validation f1score: 0.924988857184504\n",
      "test loss: 0.3521895806400359, test f1score: 0.9171291045829011\n",
      "final test f1score: 0.8962085308056872, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:06<00:00,  7.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:08<00:00,  7.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:17<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "train loss: 0.15866907913772055, train f1score: 0.927821598184319\n",
      "validation loss: 0.1761324374238029, validation f1score: 0.9131137817501886\n",
      "test loss: 0.356980803045162, test f1score: 0.9112565541410812\n",
      "final test f1score: 0.8985781990521327, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  7.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "train loss: 0.15096000777178467, train f1score: 0.930691606092174\n",
      "validation loss: 0.1888341586382012, validation f1score: 0.9257431992586246\n",
      "test loss: 0.4082402364493289, test f1score: 0.914857804138425\n",
      "final test f1score: 0.8971563981042654, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:03<00:00,  8.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "train loss: 0.14363724109536205, train f1score: 0.9370222802090351\n",
      "validation loss: 0.1858036730773165, validation f1score: 0.9217056017573693\n",
      "test loss: 0.38470922337353713, test f1score: 0.9151535032233576\n",
      "final test f1score: 0.9009478672985782, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  8.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "train loss: 0.13350474474547325, train f1score: 0.9428420624492115\n",
      "validation loss: 0.17708812539058272, validation f1score: 0.9173224769362599\n",
      "test loss: 0.3720006041134406, test f1score: 0.911520786184359\n",
      "final test f1score: 0.8943127962085308, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:05<00:00,  7.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:08<00:00,  7.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "train loss: 0.1261653406590641, train f1score: 0.9449729659537174\n",
      "validation loss: 0.17982847438543104, validation f1score: 0.9230527940194653\n",
      "test loss: 0.37230786911448266, test f1score: 0.9188543946779608\n",
      "final test f1score: 0.8995260663507109, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:08<00:00,  7.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:08<00:00,  7.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  7.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:17<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "train loss: 0.11763438142207987, train f1score: 0.9485392020185618\n",
      "validation loss: 0.19222994867595844, validation f1score: 0.9219023347783489\n",
      "test loss: 0.4025802674736813, test f1score: 0.9149206124896895\n",
      "final test f1score: 0.8919431279620853, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:08<00:00,  7.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:18<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "train loss: 0.11113020356712046, train f1score: 0.9531204697281761\n",
      "validation loss: 0.2057357031153515, validation f1score: 0.9203744766243311\n",
      "test loss: 0.43645750366638175, test f1score: 0.9183045688430638\n",
      "final test f1score: 0.8966824644549763, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:05<00:00,  7.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "train loss: 0.10242915013398926, train f1score: 0.9546318922385606\n",
      "validation loss: 0.2133626615977846, validation f1score: 0.9185998847801519\n",
      "test loss: 0.4461527899221631, test f1score: 0.9185395873589997\n",
      "final test f1score: 0.8981042654028436, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:03<00:00,  8.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\n",
      "train loss: 0.09988029898900565, train f1score: 0.9537856793176093\n",
      "validation loss: 0.22403574269264936, validation f1score: 0.9176228589501041\n",
      "test loss: 0.4708566720664472, test f1score: 0.912762937799887\n",
      "final test f1score: 0.8914691943127963, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:05<00:00,  7.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12\n",
      "train loss: 0.0935307705360681, train f1score: 0.9587591053914164\n",
      "validation loss: 0.21046449727145955, validation f1score: 0.9171036753085134\n",
      "test loss: 0.43141483580235, test f1score: 0.9121236644476787\n",
      "final test f1score: 0.895260663507109, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  7.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13\n",
      "train loss: 0.09143234416278431, train f1score: 0.9602276097362495\n",
      "validation loss: 0.2259934559406247, validation f1score: 0.921322223612996\n",
      "test loss: 0.45952831694739865, test f1score: 0.9178943855100608\n",
      "final test f1score: 0.895260663507109, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:05<00:00,  7.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\n",
      "train loss: 0.08763914010460415, train f1score: 0.9593330750559601\n",
      "validation loss: 0.25221860217061476, validation f1score: 0.9185121321571742\n",
      "test loss: 0.5288807820304073, test f1score: 0.9121429838540471\n",
      "final test f1score: 0.8867298578199052, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  7.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\n",
      "train loss: 0.08041670344374685, train f1score: 0.9642280743332838\n",
      "validation loss: 0.2519320841311128, validation f1score: 0.9132210977918974\n",
      "test loss: 0.5261261239411946, test f1score: 0.9154827063463996\n",
      "final test f1score: 0.8957345971563981, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  8.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16\n",
      "train loss: 0.08025556306269876, train f1score: 0.9637460174766899\n",
      "validation loss: 0.26245560075403773, validation f1score: 0.9166561144780981\n",
      "test loss: 0.5480923053552089, test f1score: 0.9133101900379214\n",
      "final test f1score: 0.8933649289099526, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  8.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17\n",
      "train loss: 0.07966211287380562, train f1score: 0.9656252391530057\n",
      "validation loss: 0.26826016208906367, validation f1score: 0.9151437115575806\n",
      "test loss: 0.5547624434022899, test f1score: 0.9123212679762145\n",
      "final test f1score: 0.8933649289099526, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:04<00:00,  8.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18\n",
      "train loss: 0.07409733127623926, train f1score: 0.9669843402341298\n",
      "validation loss: 0.2527930217256653, validation f1score: 0.9169673065074467\n",
      "test loss: 0.5257001550209789, test f1score: 0.9108533303583476\n",
      "final test f1score: 0.8876777251184834, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:06<00:00,  7.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19\n",
      "train loss: 0.07362104833483431, train f1score: 0.9681194320048382\n",
      "validation loss: 0.28585269609175157, validation f1score: 0.9212713900229901\n",
      "test loss: 0.60671774088824, test f1score: 0.9071792230871841\n",
      "final test f1score: 0.8829383886255924, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 516/516 [01:03<00:00,  8.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:07<00:00,  8.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:16<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20\n",
      "train loss: 0.07113749089347589, train f1score: 0.9676954237321835\n",
      "validation loss: 0.2524229860646301, validation f1score: 0.916556035838939\n",
      "test loss: 0.5403365415186273, test f1score: 0.9063163437772063\n",
      "final test f1score: 0.8881516587677725, 2110/2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs2 = 20\n",
    "trainloss2, validloss2, testloss2, trainscore2, validscore2, testscore2, testscores = train2(\n",
    "    dataloader_train2, dataloader_valid2, dataloader_test2, dataloader_test3,\n",
    "    model1 = model1, model2 = model2,\n",
    "    optimizer = optimizer2, lossfn = lossfn2, epochs = epochs2, has_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19345033957890065,\n",
       " 0.16588398793445125,\n",
       " 0.15866907913772055,\n",
       " 0.15096000777178467,\n",
       " 0.14363724109536205,\n",
       " 0.13350474474547325,\n",
       " 0.1261653406590641,\n",
       " 0.11763438142207987,\n",
       " 0.11113020356712046,\n",
       " 0.10242915013398926,\n",
       " 0.09988029898900565,\n",
       " 0.0935307705360681,\n",
       " 0.09143234416278431,\n",
       " 0.08763914010460415,\n",
       " 0.08041670344374685,\n",
       " 0.08025556306269876,\n",
       " 0.07966211287380562,\n",
       " 0.07409733127623926,\n",
       " 0.07362104833483431,\n",
       " 0.07113749089347589]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17875599872786552,\n",
       " 0.17153550492366776,\n",
       " 0.1761324374238029,\n",
       " 0.1888341586382012,\n",
       " 0.1858036730773165,\n",
       " 0.17708812539058272,\n",
       " 0.17982847438543104,\n",
       " 0.19222994867595844,\n",
       " 0.2057357031153515,\n",
       " 0.2133626615977846,\n",
       " 0.22403574269264936,\n",
       " 0.21046449727145955,\n",
       " 0.2259934559406247,\n",
       " 0.25221860217061476,\n",
       " 0.2519320841311128,\n",
       " 0.26245560075403773,\n",
       " 0.26826016208906367,\n",
       " 0.2527930217256653,\n",
       " 0.28585269609175157,\n",
       " 0.2524229860646301]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36609898795811197,\n",
       " 0.3521895806400359,\n",
       " 0.356980803045162,\n",
       " 0.4082402364493289,\n",
       " 0.38470922337353713,\n",
       " 0.3720006041134406,\n",
       " 0.37230786911448266,\n",
       " 0.4025802674736813,\n",
       " 0.43645750366638175,\n",
       " 0.4461527899221631,\n",
       " 0.4708566720664472,\n",
       " 0.43141483580235,\n",
       " 0.45952831694739865,\n",
       " 0.5288807820304073,\n",
       " 0.5261261239411946,\n",
       " 0.5480923053552089,\n",
       " 0.5547624434022899,\n",
       " 0.5257001550209789,\n",
       " 0.60671774088824,\n",
       " 0.5403365415186273]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testloss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.911747642585105,\n",
       " 0.9232315490981621,\n",
       " 0.927821598184319,\n",
       " 0.930691606092174,\n",
       " 0.9370222802090351,\n",
       " 0.9428420624492115,\n",
       " 0.9449729659537174,\n",
       " 0.9485392020185618,\n",
       " 0.9531204697281761,\n",
       " 0.9546318922385606,\n",
       " 0.9537856793176093,\n",
       " 0.9587591053914164,\n",
       " 0.9602276097362495,\n",
       " 0.9593330750559601,\n",
       " 0.9642280743332838,\n",
       " 0.9637460174766899,\n",
       " 0.9656252391530057,\n",
       " 0.9669843402341298,\n",
       " 0.9681194320048382,\n",
       " 0.9676954237321835]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainscore2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9258048982993812,\n",
       " 0.924988857184504,\n",
       " 0.9131137817501886,\n",
       " 0.9257431992586246,\n",
       " 0.9217056017573693,\n",
       " 0.9173224769362599,\n",
       " 0.9230527940194653,\n",
       " 0.9219023347783489,\n",
       " 0.9203744766243311,\n",
       " 0.9185998847801519,\n",
       " 0.9176228589501041,\n",
       " 0.9171036753085134,\n",
       " 0.921322223612996,\n",
       " 0.9185121321571742,\n",
       " 0.9132210977918974,\n",
       " 0.9166561144780981,\n",
       " 0.9151437115575806,\n",
       " 0.9169673065074467,\n",
       " 0.9212713900229901,\n",
       " 0.916556035838939]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validscore2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9182416451269046,\n",
       " 0.9171291045829011,\n",
       " 0.9112565541410812,\n",
       " 0.914857804138425,\n",
       " 0.9151535032233576,\n",
       " 0.911520786184359,\n",
       " 0.9188543946779608,\n",
       " 0.9149206124896895,\n",
       " 0.9183045688430638,\n",
       " 0.9185395873589997,\n",
       " 0.912762937799887,\n",
       " 0.9121236644476787,\n",
       " 0.9178943855100608,\n",
       " 0.9121429838540471,\n",
       " 0.9154827063463996,\n",
       " 0.9133101900379214,\n",
       " 0.9123212679762145,\n",
       " 0.9108533303583476,\n",
       " 0.9071792230871841,\n",
       " 0.9063163437772063]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testscore2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8928909952606635,\n",
       " 0.8962085308056872,\n",
       " 0.8985781990521327,\n",
       " 0.8971563981042654,\n",
       " 0.9009478672985782,\n",
       " 0.8943127962085308,\n",
       " 0.8995260663507109,\n",
       " 0.8919431279620853,\n",
       " 0.8966824644549763,\n",
       " 0.8981042654028436,\n",
       " 0.8914691943127963,\n",
       " 0.895260663507109,\n",
       " 0.895260663507109,\n",
       " 0.8867298578199052,\n",
       " 0.8957345971563981,\n",
       " 0.8933649289099526,\n",
       " 0.8933649289099526,\n",
       " 0.8876777251184834,\n",
       " 0.8829383886255924,\n",
       " 0.8881516587677725]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
