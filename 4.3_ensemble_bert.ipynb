{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install transformers\n# wget https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv\n# wget https://github.com/brianbt/AI6127_NLP_project/raw/master/data/labeled_data_spell.csv\n# Put me under data/","metadata":{"execution":{"iopub.status.busy":"2022-04-17T09:51:50.360749Z","iopub.execute_input":"2022-04-17T09:51:50.360955Z","iopub.status.idle":"2022-04-17T09:51:50.364493Z","shell.execute_reply.started":"2022-04-17T09:51:50.36093Z","shell.execute_reply":"2022-04-17T09:51:50.363732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\n!pip install pyenchant\n!wget http://archive.ubuntu.com/ubuntu/pool/main/libr/libreoffice-dictionaries/hunspell-id_6.4.3-1_all.deb\n!dpkg -i hunspell-id_6.4.3-1_all.deb\n!apt update && apt install -y enchant libenchant1c2a hunspell hunspell-en-us libhunspell-1.6-0\nnltk.download('wordnet')\n!apt-get install libenchant1c2a -y\n!pip install contractions\n\nimport contractions\nimport enchant\nfrom enchant.checker import SpellChecker","metadata":{"execution":{"iopub.status.busy":"2022-04-18T17:17:44.466808Z","iopub.execute_input":"2022-04-18T17:17:44.467358Z","iopub.status.idle":"2022-04-18T17:18:19.359383Z","shell.execute_reply.started":"2022-04-18T17:17:44.467265Z","shell.execute_reply":"2022-04-18T17:18:19.358455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification,AutoModel\nfrom transformers import AutoTokenizer, AutoConfig\nfrom sklearn.utils import shuffle\nimport sklearn\nimport random\nimport warnings\nimport re\n# from math import comb\n\nseed = 888\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using {device}\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:56:46.976786Z","iopub.execute_input":"2022-04-21T11:56:46.977213Z","iopub.status.idle":"2022-04-21T11:56:50.127347Z","shell.execute_reply.started":"2022-04-21T11:56:46.977126Z","shell.execute_reply":"2022-04-21T11:56:50.126571Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def preprocess(string):\n    temp = string.lower()\n    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n    temp = re.sub(\"[!:]+\",\"\", temp)\n    temp = re.sub(r\"&amp;\", \"\", temp)\n    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n    temp = re.sub(r\"http\\S+\", \"\", temp)\n    temp = re.sub(r\"www.\\S+\", \"\", temp)\n    temp = re.sub('[()!?]', ' ', temp)\n    temp = re.sub('\\[.*?\\]',' ', temp)\n    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n    temp = re.sub(r\"rt\", \"\", temp)\n    return temp\n\ndef misspellings(string):\n    d = enchant.request_dict(\"en_US\")\n    chkr = SpellChecker(\"en_US\", string)\n    for err in chkr:\n        suggest = d.suggest(err.word)\n        if len(suggest) != 0:\n            err.replace(suggest[0])\n    \n    return chkr.get_text()#print(chkr.get_text())\n\ndef dup_data(df, on_class, repeat=1000):\n    \"\"\" data augmentation\n    \n    This will random pick two row and mix them together\n    So two sentence will be concat tgt, generate a longer sentence\n    For safety, it only do augmentation on the same class\n      It will NOT generate new data nest-ed-ly. \n      Means augmented data will not be used to generate new data\n    \n    Args:\n        df: DataFrame\n        on_class (int): Which class to augment, either [0,1,2]\n        repeat (int): how many new data to generate\n    \n    Return:\n        pd.DataFrame: The augmented data and org df will be concat tgt\n        \n    Examples:\n        >>> df = dup_data(df, 0)\n    \"\"\"\n    cl = {'hate_speech': 0, 'offensive_language': 1, 'neither': 2}\n    out = []\n    tmp = df[df['class'] == on_class]\n#     print(f\"class={on_class} have {tmp.shape[0]} data, each time pick 2. nCr = {comb(tmp.shape[0], 2)}\")\n    for k in range(repeat):\n        i,j = random.randint(0, tmp.shape[0]-1), random.randint(0, tmp.shape[0]-1)\n        out.append(tmp.iloc[i]+tmp.iloc[j])\n    out = pd.concat(out, axis=1).T\n    out[['count', 'hate_speech', 'offensive_language', 'neither', 'class']] = out[['count', 'hate_speech', 'offensive_language', 'neither', 'class']].astype('int')\n    # handle edge case\n    out['class'] = out[['hate_speech', 'offensive_language', 'neither']].idxmax(1).map({'hate_speech': 0, 'offensive_language': 1, 'neither': 2})\n    return pd.concat([df,out]).reset_index(drop=True)\n\ndef number_params(model, exclude_freeze=False):\n    \"\"\"calculate the number of parameters in a model\n\n    Args:\n        model (nn.Module): PyTorch model\n        exclude_freeze (bool, optional): Whether to count the frozen layer. Defaults to False.\n    \"\"\"\n    pp = 0\n    for p in list(model.parameters()):\n        if exclude_freeze and p.requires_grad is False:\n            continue\n        nn = 1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef ensemble(models, do_argmax=False):\n    \"\"\" ensemble models prediction\n    \n    Args:\n        models (list(tensor)): list of prediction, each prediction should have same shape(N,C).\n    \n    Examples:\n        tweet = iter(dataloader_train).next()[-1]\n        out = model(list(tweet))\n        ensemble([out[0], out[0]])\n    \"\"\"\n    out = torch.softmax(models[0], 1)\n    for i in range(1, len(models)):\n        out += torch.softmax(models[i], 1)\n    if do_argmax:\n        return out.argmax(1)\n    else:\n        return out\n\ndef finetune(\n        model: nn.Module,\n        base_lr: float,\n        groups,\n        ignore_the_rest: bool = False,\n        raw_query: bool = False,\n        regex=False):\n    \"\"\" This is something call per-parameter options\n\n    Separate out the finetune parameters with a learning rate for each layers of parameters\n    This function only support setting a different learning rate for each layer's arameter.\n    Depending on the optimizer, you can set extra parameter for that layer for the optmizer -> See Notes \n    If you freeze layer using this function and want to unfreeze it later:\n    See https://discuss.pytorch.org/t/correct-way-to-freeze-layers/26714/2\n\n    Args:\n        model (nn.Module): Pytorch Model\n        base_lr (float): learning rate of all layers\n        groups (Dict[str, float]): key is `name` of layers, value is the `extra_lr` (or False).\n          all layers that contains that `name` will have `lr` of base_lr*extra_lr.\n          it uses fnmatch|regex to check whether a layer contains that `name`.\n          fnmatch is matching structure like `layer1*`, `layer?.conv?.`, `*conv2*`, etc...\n          regex is the comman regex matching.\n          Hence, `name` here is either fnmatch or regex expression if using raw_query.\n          If `float` is False: those layers with `name` will be freeze. \n          In particular, they will not be included in the return output and require_grad will be set to False\n        ignore_the_rest (bool, optional): Include the remaining layer that are not stated in `grouprs` or not. Defaults to False.\n        raw_query (bool, optional): Modify the keys of `groups` as f'*{key}*' if False. Only useful when `regex=False`\n          Do not do any modification to the keys of `groups` if True. Defaults to False.\n        regex (bool, optional): Use regex instead of fnmatch on keys of groups. Defaults to False.\n          This will overrride raw_query to True. \n          Notice: `regex=False` is depracted\n\n    Returns:\n        List[Dict[str, Union[float, Iterable]]]: list of dict that has two or more key-value pair.\n          The first one is feature generation layers. [those layers must start with `features` name] <usually is backbone>\n            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'query':query, 'lr':base_lr*groups[groups.keys()]]\n          The remaining are all others layer. [all others params for last one, if ignore_the_rest = False]\n            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'lr':base_lr]\n\n    Examples:\n        >>> model = models.resnet50()\n        >>> # all layers that has name start with `layer1 and layer2` will have learning rate `0.001*0.01`\n        >>> # all layers that has name start with `layer3` will be froozen`\n        >>> # all layers that has name start with `layer4` will have learning rate `0.001*0.001`\n        >>> # for all other layers will have the base_lr `0.001`\n        >>> model_params = finetune(model, base_lr=0.001, groups={'^layer[1-2].*': 0.01, '^layer3.*': False, '^layer4.*': 0.001}, regex=True)\n        >>> # setting extra parameter (other than learning rate) for that optimizer\n        >>> # the second param_group `layer4` will have weight_decay 1e-2\n        >>> model_params[1]['weight_decay'] = 1e-2\n        >>> # init optimizer with the above setting\n        >>> # the argument under `torch.optim.SGD` will be overrided by finetune() if they exist.\n        >>> # For example, all model_params will have weight_decay=5e-3 except model_params[1]\n        >>> optimizer = torch.optim.SGD(model_params, momentum=0.9, lr=0.1, weight_decay=5e-3)\n    \"\"\"\n    if regex:\n        raw_query = True\n    else:\n        warnings.warn(\"regex=False is deprecated; use regex=True\", DeprecationWarning)\n    # Deal with Freeze Later\n    freeze_group = dict()\n    freeze = False\n    for k,v in groups.items():\n        if v is False:\n            freeze_group[k] = 1\n            freeze=True\n    for k in freeze_group.keys():\n        del groups[k]\n    freeze_group = \"(\" + \")|(\".join(freeze_group) + \")\"\n\n    parameters = [\n        dict(params=[],\n             names=[],\n             query=query if raw_query else '*' + query + '*',\n             lr = lr * base_lr,\n             initial_lr = lr * base_lr) for query, lr in groups.items()\n    ]\n    rest_parameters = dict(params=[], names=[], lr=base_lr, initial_lr=base_lr)\n    for k, v in model.named_parameters():\n        rest = 0\n        if freeze and regex and re.match(freeze_group, k):\n            v.requires_grad = False\n            continue\n        for group in parameters:\n            if not regex and fnmatch(k, group['query']):\n                group['params'].append(v)\n                group['names'].append(k)\n                rest = 1\n                break\n            elif regex and re.compile(group['query']).search(k):\n                group['params'].append(v)\n                group['names'].append(k)\n                rest = 1\n                break\n        if rest == 0:\n            rest_parameters['params'].append(v)\n            rest_parameters['names'].append(k)\n\n    if not ignore_the_rest:\n        parameters.append(rest_parameters)\n    for group in parameters:\n        group['params'] = iter(group['params'])\n    return parameters","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:56:50.130103Z","iopub.execute_input":"2022-04-21T11:56:50.130320Z","iopub.status.idle":"2022-04-21T11:56:50.162836Z","shell.execute_reply.started":"2022-04-21T11:56:50.130291Z","shell.execute_reply":"2022-04-21T11:56:50.162001Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# path of data and the name of pretrained weights\n# path = '../input/AI6127/labeled_data.csv'\n# path = '../input/labeled-data-spell/labeled_data_spell.csv'\n# path = './data/labeled_data.csv'\npath = '../input/hate-speech-and-offensive-language-dataset/labeled_data.csv'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:01.169722Z","iopub.execute_input":"2022-04-21T11:57:01.170252Z","iopub.status.idle":"2022-04-21T11:57:01.173869Z","shell.execute_reply.started":"2022-04-21T11:57:01.170214Z","shell.execute_reply":"2022-04-21T11:57:01.173172Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path, index_col = 0).dropna()\n# df[\"tweet\"] = df[\"tweet\"].apply(fix_contractions)\n# df[\"tweet\"] = df[\"tweet\"].apply(preprocess)\n# df[\"tweet\"] = df[\"tweet\"].apply(misspellings)\ndf = shuffle(df)\ndf.describe()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:15.361944Z","iopub.execute_input":"2022-04-21T11:57:15.362193Z","iopub.status.idle":"2022-04-21T11:57:15.494263Z","shell.execute_reply.started":"2022-04-21T11:57:15.362164Z","shell.execute_reply":"2022-04-21T11:57:15.493625Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 16307 & 5009, Just to make sure the experiment is reproducible\ndf.iloc[0:2] ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:16.588340Z","iopub.execute_input":"2022-04-21T11:57:16.588683Z","iopub.status.idle":"2022-04-21T11:57:16.606347Z","shell.execute_reply.started":"2022-04-21T11:57:16.588644Z","shell.execute_reply":"2022-04-21T11:57:16.605432Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:17.427518Z","iopub.execute_input":"2022-04-21T11:57:17.428044Z","iopub.status.idle":"2022-04-21T11:57:17.435348Z","shell.execute_reply.started":"2022-04-21T11:57:17.428008Z","shell.execute_reply":"2022-04-21T11:57:17.434457Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# split data into train and test\ntrain_data = df.sample(frac = 0.8)\ntest_data = df.drop(train_data.index)\nvalid_data = test_data.sample(frac = 0.5)\ntest_data = test_data.drop(valid_data.index)\n\ndisplay(train_data.head())\nprint(\"===================================\")\ndisplay(valid_data.head())\nprint(\"===================================\")\ndisplay(test_data.head())\n\nprint(train_data.shape)\nprint(valid_data.shape)\nprint(test_data.shape)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:19.543659Z","iopub.execute_input":"2022-04-21T11:57:19.543922Z","iopub.status.idle":"2022-04-21T11:57:19.584717Z","shell.execute_reply.started":"2022-04-21T11:57:19.543894Z","shell.execute_reply":"2022-04-21T11:57:19.583906Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:21.001360Z","iopub.execute_input":"2022-04-21T11:57:21.001759Z","iopub.status.idle":"2022-04-21T11:57:21.008910Z","shell.execute_reply.started":"2022-04-21T11:57:21.001725Z","shell.execute_reply":"2022-04-21T11:57:21.008242Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation (experiments)","metadata":{}},{"cell_type":"code","source":"# train_data = dup_data(train_data, 0, 15361-1129)\n# train_data = dup_data(train_data, 2, 15361-3336)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T09:52:26.280519Z","iopub.execute_input":"2022-04-17T09:52:26.280709Z","iopub.status.idle":"2022-04-17T09:52:26.284118Z","shell.execute_reply.started":"2022-04-17T09:52:26.280683Z","shell.execute_reply":"2022-04-17T09:52:26.283145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T09:52:26.285911Z","iopub.execute_input":"2022-04-17T09:52:26.286393Z","iopub.status.idle":"2022-04-17T09:52:26.293959Z","shell.execute_reply.started":"2022-04-17T09:52:26.286345Z","shell.execute_reply":"2022-04-17T09:52:26.293154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Dataset\n\nThe data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data df contains 5 columns:\n\ncount = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n\nhate_speech = number of CF users who judged the tweet to be hate speech.\n\noffensive_language = number of CF users who judged the tweet to be offensive.\n\nneither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n\nclass = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_name=None,  train = True, device='cuda'):\n        super(Dataset, self).__init__()\n        self.df = df\n        self.device = device\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        device = self.device\n        count = torch.LongTensor([self.df.iloc[idx]['count']])\n        hate_speech = torch.LongTensor([self.df.iloc[idx]['hate_speech']])\n        offensive_language = torch.LongTensor([self.df.iloc[idx]['offensive_language']])\n        neither = torch.LongTensor([self.df.iloc[idx]['neither']])\n        target = torch.LongTensor([self.df.iloc[idx]['class']])\n        tweet = self.df.iloc[idx]['tweet']\n        return (count.to(device), hate_speech.to(device), offensive_language.to(device)\n                , neither.to(device), target.to(device), tweet)\n        \n        ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:25.087258Z","iopub.execute_input":"2022-04-21T11:57:25.087751Z","iopub.status.idle":"2022-04-21T11:57:25.096347Z","shell.execute_reply.started":"2022-04-21T11:57:25.087713Z","shell.execute_reply":"2022-04-21T11:57:25.095681Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# This one is just for DEBUG, not the real dataset to be used\ndataset = Dataset(df, device=device)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size= 6, shuffle=True)\noutput = next(iter(dataloader))\nprint(output)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:26.385497Z","iopub.execute_input":"2022-04-21T11:57:26.386046Z","iopub.status.idle":"2022-04-21T11:57:29.114312Z","shell.execute_reply.started":"2022-04-21T11:57:26.386007Z","shell.execute_reply":"2022-04-21T11:57:29.113601Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    \n    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False):\n        super(LanguageModel, self).__init__()\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if freeze_pretrained:\n            print(\"You are freezing the BERT\")\n            for name, p in self.model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        print(f\"Total number of params: {number_params(self.model)}\")\n        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n\n    def forward(self, src, has_mask=False):\n        # print(src)\n        output = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        output = torch.LongTensor(output['input_ids']).to(device)\n        if has_mask == True:\n            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n            output = self.model(output, attention_mask=attention_mask)\n        else:\n            output = self.model(output)\n        return output","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:57:29.176558Z","iopub.execute_input":"2022-04-21T11:57:29.176857Z","iopub.status.idle":"2022-04-21T11:57:29.186982Z","shell.execute_reply.started":"2022-04-21T11:57:29.176826Z","shell.execute_reply":"2022-04-21T11:57:29.186216Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class LastAttnModel(nn.Module):\n    \"\"\"\n    Use the [CLS] as query and all other output as key and values.\n    Pass it to a Multi-Head Attention, then a Linear classifier\n    \n    Args:\n      auxiliary_head(list(int)): Only used when training\n        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n        - See BertConfig['num_hidden_layers'] for total number of layers\n        - EG: `auxiliary_head=[10,11,12]`.\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n      all_CLS_attn(bool): use last CLS as query, all previous CLS as key and values -> Multi-Head Attention\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n      out = modelA(list(tweet))\n      print(out[0].shape, attn_weight[1].shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_model, tokenizer, \n                 last_attn_num_head = 8,\n                 classifier_hidden_dim = 512, \n                 classifier_dropout = 0,\n                 num_labels = 3, \n                 freeze_pretrained=False,\n                 auxiliary_head=None,\n                 last_hidden_layer=-1,\n                 all_CLS_attn=False,\n                 **kwargs):\n        super(LastAttnModel, self).__init__()\n        self.pretrain_model = pretrain_model\n        self.tokenizer = tokenizer\n        self.auxiliary_head = auxiliary_head\n        self.num_layers = len(pretrain_model.encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        self.all_CLS_attn = all_CLS_attn\n        \n        if freeze_pretrained:\n            if self.auxiliary_head is not None:\n              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n            print(\"You are freezing the BERT pertrain\")\n            for name, p in self.pretrain_model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        \n        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n        self.final_classifier = nn.Sequential(\n            nn.Linear(embed_size, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        # deal with aux \n        if self.auxiliary_head is not None:\n          self.aux_classifiers = nn.ModuleList()\n          for i in self.auxiliary_head:\n            self.aux_classifiers.append(nn.Sequential(\n              nn.Linear(embed_size, classifier_hidden_dim),\n              nn.ReLU(),\n              nn.Dropout(classifier_dropout),\n              nn.Linear(classifier_hidden_dim, num_labels)\n            ))\n            \n        # use all CLS attention\n        if self.all_CLS_attn:\n            self.all_CLS = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n        else:\n            pre_train_output = self.pretrain_model(inputs)\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n        if self.all_CLS_attn:\n            o = [hidden[:,0,:] for hidden in pre_train_output[\"hidden_states\"][:self.last_hidden_layer-1]]\n            rest = torch.stack(o).permute(1,0,2)  # (N,self.last_hidden_layer-1,E)\n            last_hidden_state_cls,_=self.all_CLS(last_hidden_state_cls, rest, rest)\n        last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n        atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n        last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n                                                    key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n        last_attn_out = last_attn_out.squeeze(1) #(N,E)\n        output = self.final_classifier(last_attn_out)\n        out += [output, last_attn_w]\n        ## auxiliary_head\n        if self.auxiliary_head is not None:\n          if \"hidden_states\" not in pre_train_output:\n            raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n          for idx in range(len(self.aux_classifiers)):\n            hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n            out.append(self.aux_classifiers[idx](hidden_cls))\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out, attn_weight = modelA(list(tweet))\n# print(out.shape, attn_weight.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:30.066503Z","iopub.execute_input":"2022-04-21T11:57:30.067030Z","iopub.status.idle":"2022-04-21T11:57:30.088993Z","shell.execute_reply.started":"2022-04-21T11:57:30.066992Z","shell.execute_reply":"2022-04-21T11:57:30.088211Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class BiLSTMModel(nn.Module):\n    \"\"\"\n    Fit the last layer BERT output to bi-lstm.\n    Concat the forward and backward final hidden state, then a Linear classifier\n    \n    Args:\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = BiLSTMModel(pretrain_model, tokenizer).to(device)\n      out  = modelA(list(tweet))\n      print(out[0].shape) #torch.Size([32, 3])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_model, tokenizer, \n                 lstm_hidden = 1024,\n                 lstm_num_layer = 2,\n                 classifier_hidden_dim = 512, \n                 classifier_dropout = 0,\n                 num_labels = 3, \n                 freeze_pretrained=False,\n                 last_hidden_layer=-1,\n                 **kwargs):\n        super(BiLSTMModel, self).__init__()\n        self.pretrain_model = pretrain_model\n        self.tokenizer = tokenizer\n        self.lstm_hidden = lstm_hidden\n        self.lstm_num_layer = lstm_num_layer\n        self.num_layers = len(pretrain_model.encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        \n        if freeze_pretrained:\n            print(\"You are freezing the BERT pertrain\")\n            for name, p in self.pretrain_model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        \n        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n        self.lstm = nn.LSTM(embed_size, lstm_hidden, lstm_num_layer, bidirectional=True, batch_first=True)\n        \n        self.final_classifier = nn.Sequential(\n            nn.Linear(2*lstm_hidden, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n        else:\n            pre_train_output = self.pretrain_model(inputs)\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        batch_size, seq_len, embed_size = last_hidden.shape\n        output, (h_n, c_n) = self.lstm(last_hidden)\n        output = output.view(batch_size, seq_len, 2, self.lstm_hidden) #batch, seq_len, num_directions, hidden_size\n        h_n = h_n.view(self.lstm_num_layer, 2, batch_size, self.lstm_hidden) # num_layers, num_directions, batch, hidden_size\n        c_n = c_n.view(self.lstm_num_layer, 2, batch_size, self.lstm_hidden) # num_layers, num_directions, batch, hidden_size\n        forward_last = h_n[-1, 0, :, :]  #(N, H)\n        backward_last = h_n[-1, 1, :, :] #(N, H)\n        output = torch.hstack([forward_last, backward_last]) #(N,2H)\n        output = self.final_classifier(output)\n        out += [output]\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out = modelA(list(tweet))\n# print(out[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:30.874233Z","iopub.execute_input":"2022-04-21T11:57:30.874583Z","iopub.status.idle":"2022-04-21T11:57:30.891418Z","shell.execute_reply.started":"2022-04-21T11:57:30.874545Z","shell.execute_reply":"2022-04-21T11:57:30.890618Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class EnsembleBertLastAttnModel(nn.Module):\n    \"\"\"\n    Use the [CLS] as query and all other output as key and values.\n    Pass it to a Multi-Head Attention, then a Linear classifier\n    \n    Args:\n      auxiliary_head(list(int)): Only used when training\n        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n        - See BertConfig['num_hidden_layers'] for total number of layers\n        - EG: `auxiliary_head=[10,11,12]`.\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n      all_CLS_attn(bool): use last CLS as query, all previous CLS as key and values -> Multi-Head Attention\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n      out = modelA(list(tweet))\n      print(out[0].shape, attn_weight[1].shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_models, tokenizer, \n             last_attn_num_head = 8,\n             classifier_hidden_dim = 512, \n             classifier_dropout = 0,\n             num_labels = 3, \n             freeze_pretrained=False,\n             auxiliary_head=None,\n             last_hidden_layer=-1,\n             all_CLS_attn=False,\n             **kwargs):\n        super(EnsembleBertLastAttnModel, self).__init__()\n        self.pretrain_models = pretrain_models\n        self.tokenizer = tokenizer\n        self.auxiliary_head = auxiliary_head\n        self.num_layers = len(pretrain_models[0].encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        self.all_CLS_attn = all_CLS_attn\n        \n        if freeze_pretrained:\n            if self.auxiliary_head is not None:\n              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n            print(\"You are freezing the BERT pertrain\")\n            for pretrain_model in self.pretrain_models:\n                for name, p in pretrain_model.named_parameters():\n                    if 'classifier' not in name:\n                        p.requires_grad = False\n        \n        embed_size = pretrain_models[0].embeddings.word_embeddings.embedding_dim\n        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n        self.final_classifier = nn.Sequential(\n            nn.Linear(embed_size, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        # deal with aux \n        if self.auxiliary_head is not None:\n          self.aux_classifiers = nn.ModuleList()\n          for i in self.auxiliary_head:\n            self.aux_classifiers.append(nn.Sequential(\n              nn.Linear(embed_size, classifier_hidden_dim),\n              nn.ReLU(),\n              nn.Dropout(classifier_dropout),\n              nn.Linear(classifier_hidden_dim, num_labels)\n            ))\n            \n        # use all CLS attention\n        if self.all_CLS_attn:\n            self.all_CLS = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n\n            pre_train_output = self.pretrain_models[0](inputs, attention_mask=attention_mask)\n            last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n            hidden_states = list(pre_train_output['hidden_states'])\n            for i in range(1, len(self.pretrain_models)):\n                pre_train_output = self.pretrain_models[i](inputs, attention_mask=attention_mask)\n                last_hidden += pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n                for j in range(len(hidden_states)):\n                    hidden_states[j] += pre_train_output[\"hidden_states\"][j]\n\n        else:\n            pre_train_output = self.pretrain_models[0](inputs)\n            last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n            hidden_states = list(pre_train_output['hidden_states'])\n            for i in range(1, len(self.pretrain_models)):\n                pre_train_output = self.pretrain_models[i](inputs)\n                last_hidden += pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n                for j in range(len(hidden_states)):\n                    hidden_states[j] += pre_train_output[\"hidden_states\"][j]\n        \n        last_hidden = last_hidden/len(self.pretrain_models)\n        hidden_states = tuple(map(lambda hs: hs/len(self.pretrain_models), hidden_states))\n\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        # last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n        if self.all_CLS_attn:\n            o = [hidden[:,0,:] for hidden in pre_train_output[\"hidden_states\"][:self.last_hidden_layer-1]]\n            rest = torch.stack(o).permute(1,0,2)  # (N,self.last_hidden_layer-1,E)\n            last_hidden_state_cls,_=self.all_CLS(last_hidden_state_cls, rest, rest)\n        last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n        atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n        last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n                                                    key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n        last_attn_out = last_attn_out.squeeze(1) #(N,E)\n        output = self.final_classifier(last_attn_out)\n        out += [output, last_attn_w]\n        ## auxiliary_head\n        if self.auxiliary_head is not None:\n          if \"hidden_states\" not in pre_train_output:\n            raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n          for idx in range(len(self.aux_classifiers)):\n            hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n            out.append(self.aux_classifiers[idx](hidden_cls))\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out, attn_weight = modelA(list(tweet))\n# print(out.shape, attn_weight.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:24:01.322647Z","iopub.execute_input":"2022-04-21T12:24:01.322899Z","iopub.status.idle":"2022-04-21T12:24:01.348074Z","shell.execute_reply.started":"2022-04-21T12:24:01.322871Z","shell.execute_reply":"2022-04-21T12:24:01.347037Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Initialization","metadata":{}},{"cell_type":"code","source":"def get_bert(model_name):\n    config = AutoConfig.from_pretrained(\n        model_name, \n        output_hidden_states = True,\n        output_attention = False,\n        hidden_dropout_prob = 0.2,\n    ) \n#     print(config)\n    pretrain_model = AutoModel.from_pretrained(\n        model_name,\n        config = config\n    ).to(device)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return pretrain_model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:24:07.936551Z","iopub.execute_input":"2022-04-21T12:24:07.937088Z","iopub.status.idle":"2022-04-21T12:24:07.942405Z","shell.execute_reply.started":"2022-04-21T12:24:07.937049Z","shell.execute_reply":"2022-04-21T12:24:07.941581Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset(train_data)\ndataloader_train = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\ndataset_valid = Dataset(valid_data)\ndataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=32, shuffle=False)\ndataset_test = Dataset(test_data)\ndataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=False)\n\nconfig = {'bert_list': ['GroNLP/hateBERT', 'diptanu/fBERT']}\n\nlossfn = nn.CrossEntropyLoss().to(device)\nhateBert, tokenizer = get_bert('GroNLP/hateBERT')\nfBert, _ = get_bert('diptanu/fBERT')\npretrain_model = [hateBert, fBert]\n\nDconfig = {'Dmodel_name':'EnsembleBertLastAttn',\n           'freeze_pretrained':True,\n           'classifier_dropout':0.1,\n           'auxiliary_head':None,\n           'last_hidden_layer':5,\n           'all_CLS_attn':True,\n           'epochs': 20}\nmodel = EnsembleBertLastAttnModel(pretrain_model, tokenizer, **Dconfig).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n\nepochs = Dconfig['epochs']\n\nsave_path=\"./ensemblelastATNN.pt\"","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:41:18.544704Z","iopub.execute_input":"2022-04-21T12:41:18.545412Z","iopub.status.idle":"2022-04-21T12:41:29.431896Z","shell.execute_reply.started":"2022-04-21T12:41:18.545351Z","shell.execute_reply":"2022-04-21T12:41:29.431155Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for datas in dataloader_train:\n    count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-18T17:24:48.2942Z","iopub.execute_input":"2022-04-18T17:24:48.294609Z","iopub.status.idle":"2022-04-18T17:24:48.32986Z","shell.execute_reply.started":"2022-04-18T17:24:48.294571Z","shell.execute_reply":"2022-04-18T17:24:48.329226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(list(tweet))[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T17:24:48.330994Z","iopub.execute_input":"2022-04-18T17:24:48.331383Z","iopub.status.idle":"2022-04-18T17:24:49.06025Z","shell.execute_reply.started":"2022-04-18T17:24:48.331347Z","shell.execute_reply":"2022-04-18T17:24:49.059409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DEBUG","metadata":{}},{"cell_type":"code","source":"# model_name = 'GroNLP/hateBERT'\n# config = AutoConfig.from_pretrained(\n#     model_name, \n#     output_hidden_states = True,\n#     output_attention = False\n# ) \n# print(config)\n# model = AutoModel.from_pretrained(\n#     model_name,\n#     config = config\n# ).to(device)\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.009612Z","iopub.execute_input":"2022-04-15T03:56:25.010067Z","iopub.status.idle":"2022-04-15T03:56:25.01398Z","shell.execute_reply.started":"2022-04-15T03:56:25.010025Z","shell.execute_reply":"2022-04-15T03:56:25.013329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweet = iter(dataloader_train).next()[-1]\n# tokens = tokenizer(list(tweet), padding=True, truncation=True, max_length=50)\n# print(tokens.keys())\n# inputs = torch.LongTensor(tokens['input_ids']).to(device)\n# atten_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n# print(inputs.shape)\n# hiddens = model(inputs)\n# print(hiddens.keys())\n# print(hiddens['last_hidden_state'].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.015437Z","iopub.execute_input":"2022-04-15T03:56:25.015948Z","iopub.status.idle":"2022-04-15T03:56:25.024254Z","shell.execute_reply.started":"2022-04-15T03:56:25.015867Z","shell.execute_reply":"2022-04-15T03:56:25.023533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lstm = nn.LSTM(model.embeddings.word_embeddings.embedding_dim, 1024, 2, bidirectional=True, batch_first=True).cuda()\n# batch_size, seq_len, embed_size = hiddens['last_hidden_state'].shape\n# output, (h_n, c_n) = lstm(hiddens['last_hidden_state'])\n# print(output.shape, h_n.shape, c_n.shape)\n# output = output.view(batch_size, seq_len, 2, 1024) #batch, seq_len, num_directions, hidden_size\n# h_n = h_n.view(2, 2, batch_size, 1024) # num_layers, num_directions, batch, hidden_size\n# c_n = c_n.view(2, 2, batch_size, 1024) # num_layers, num_directions, batch, hidden_size\n# print(output.shape, h_n.shape, c_n.shape)\n# forward_last = h_n[-1, 0, :, :]\n# backward_last = h_n[-1, 1, :, :]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.025621Z","iopub.execute_input":"2022-04-15T03:56:25.026175Z","iopub.status.idle":"2022-04-15T03:56:25.032757Z","shell.execute_reply.started":"2022-04-15T03:56:25.026137Z","shell.execute_reply":"2022-04-15T03:56:25.032071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hiddens['pooler_output'].shape\n# hiddens['last_hidden_state'][:,0,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.034142Z","iopub.execute_input":"2022-04-15T03:56:25.034718Z","iopub.status.idle":"2022-04-15T03:56:25.042709Z","shell.execute_reply.started":"2022-04-15T03:56:25.034636Z","shell.execute_reply":"2022-04-15T03:56:25.042106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.pooler","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.044192Z","iopub.execute_input":"2022-04-15T03:56:25.044515Z","iopub.status.idle":"2022-04-15T03:56:25.051112Z","shell.execute_reply.started":"2022-04-15T03:56:25.044479Z","shell.execute_reply":"2022-04-15T03:56:25.050473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.pooler(hiddens['last_hidden_state']) == hiddens['pooler_output']","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.052474Z","iopub.execute_input":"2022-04-15T03:56:25.052856Z","iopub.status.idle":"2022-04-15T03:56:25.059954Z","shell.execute_reply.started":"2022-04-15T03:56:25.052809Z","shell.execute_reply":"2022-04-15T03:56:25.059139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# atten_mask_pad = (inputs == 0)\n# print(atten_mask_pad.shape)\n# print(atten_mask_pad[:,1:].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.061222Z","iopub.execute_input":"2022-04-15T03:56:25.061494Z","iopub.status.idle":"2022-04-15T03:56:25.072048Z","shell.execute_reply.started":"2022-04-15T03:56:25.06146Z","shell.execute_reply":"2022-04-15T03:56:25.071303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble([out[0], torch.randn((32,3)).cuda(), out[0]-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-15T03:56:25.07349Z","iopub.execute_input":"2022-04-15T03:56:25.073749Z","iopub.status.idle":"2022-04-15T03:56:25.081041Z","shell.execute_reply.started":"2022-04-15T03:56:25.073715Z","shell.execute_reply":"2022-04-15T03:56:25.080168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train(dataloader_train, dataloader_valid = None, model = None, \n          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n    \n    trainloss = []\n    validloss = []\n    trainscore = []\n    validscore = []\n    bestt_score = 0\n    for i in range(epochs):\n        model.train()\n        averageloss = 0\n        averagef1 = 0\n        averagePrecision = 0\n        averageRecall = 0\n        for datas in tqdm(dataloader_train):\n            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n            target = target.to('cuda')\n            optimizer.zero_grad()\n            pred = model(list(tweet), has_mask)\n            loss = lossfn(pred[0], target.squeeze(1))\n            # aux head\n            for j in range(2, len(pred)):\n                loss+=0.3*lossfn(pred[j], target.squeeze(1))\n            loss.backward()\n            optimizer.step()\n            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n            precision=sklearn.metrics.precision_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n            recall=sklearn.metrics.recall_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n            averageloss += loss.item()/len(dataloader_train)\n            averagef1 += f1score/len(dataloader_train)\n            averagePrecision += precision/len(dataloader_train)\n            averageRecall += recall/len(dataloader_train)\n        trainloss.append(averageloss)\n        trainscore.append((averagef1, averagePrecision, averageRecall))\n        if dataloader_valid is not None:\n            model.eval()\n            averageloss = 0\n            averagef1 = 0\n            averagePrecision = 0\n            averageRecall = 0\n            for datas in tqdm(dataloader_valid):\n                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n                target = target.to('cuda')\n                pred = model(list(tweet), has_mask)\n                loss = lossfn(pred[0], target.squeeze(1))\n                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n                precision=sklearn.metrics.precision_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n                recall=sklearn.metrics.recall_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n                averageloss += loss.item()/len(dataloader_valid)\n                averagef1 += f1score/len(dataloader_valid)\n                averagePrecision += precision/len(dataloader_valid)\n                averageRecall += recall/len(dataloader_valid)\n            validloss.append(averageloss)\n            validscore.append((averagef1, averagePrecision, averageRecall))\n            print(f\"epoch: {i}, train loss: {trainloss[-1]}, validation loss: {validloss[-1]}\\n train f1score: {trainscore[-1]}\\nvalidation f1score: {validscore[-1]}\")\n            if averagef1 > bestt_score:\n                print(\"Found Best Model\")\n                to_save = {'model': model.state_dict(),\n                           'config': config,\n                           'Dconfig': Dconfig,\n                           'optimizer': optimizer,\n                           'lr_s':None}\n                torch.save(to_save, save_path.replace('.pt', '_best.pt'))\n                bestt_score = averagef1\n        else:\n            print(f\"epoch: {i}, train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n\n\n    return trainloss, validloss, trainscore, validscore\n\n\ndef test(dataloader, model = None, lossfn = None, epochs = 1, has_mask = True):\n    \"\"\"\n    Args:\n        model: if `pytorch Model` -> normal test. if `list(pytorch Model)` -> ensemble\n    \"\"\"\n#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n    testloss = []\n    testscore = []\n    if isinstance(model, list):\n        for mmm in model:\n            mmm.eval()\n    else:\n        model.eval()\n    averageloss = 0\n    averagef1 = 0\n    averagePrecision = 0\n    averageRecall = 0\n    overallPred = []\n    overallTar = []\n    for datas in tqdm(dataloader):\n        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n        target = target.to('cuda')\n        if isinstance(model, list):\n            ensem = []\n            for mmm in model:\n                p = mmm(list(tweet), has_mask)\n                ensem.append(p[0])\n            pred = [ensemble(ensem)]\n        else:\n            pred = model(list(tweet), has_mask)\n        loss = lossfn(pred[0], target.squeeze(1))\n        averageloss += loss.item()/len(dataloader)\n        overallPred.append(pred[0].argmax(-1).cpu().numpy())\n        overallTar.append(target.squeeze(1).cpu().numpy())\n#     print(overallPred[:2])\n#     print(overallTar[:2])\n    predicts = np.concatenate(overallPred)\n    targets = np.concatenate(overallTar)\n#     print(predicts.shape)\n#     print(targets.shape)\n\n    f1score = sklearn.metrics.f1_score(targets, predicts, average = 'weighted')\n    precision=sklearn.metrics.precision_score(targets, predicts, average='weighted', zero_division=0)\n    recall=sklearn.metrics.recall_score(targets, predicts, average='weighted', zero_division=0)\n\n    print(f\"test loss: {averageloss}\")\n    print(f\"test score: {(f1score, precision, recall)}\")\n    return averageloss, (f1score, precision, recall)\n            \ndef get_csv(dataloader, model = None, has_mask = True):\n    \"\"\"get the prediction csv\n    \n    Examples:\n        o = get_csv(dataset_test, model1)\n        o.to_csv(save_path.replace('.pt', '.csv'))\n    \"\"\"\n#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n    OverallPred = []\n    for datas in tqdm(dataloader):\n        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n        target = target.to('cuda')\n        if isinstance(model, list):\n            ensem = []\n            for mmm in model:\n                mmm.eval()\n                p = mmm(list(tweet), has_mask)\n                ensem.append(p[0])\n            pred = [ensemble(ensem)]\n        else:\n            model.eval()\n            pred = model(list(tweet), has_mask)\n        OverallPred.append(pred[0].cpu().detach().numpy())\n    output = np.concatenate(OverallPred)\n    return pd.DataFrame(output)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-04-21T11:58:30.741881Z","iopub.execute_input":"2022-04-21T11:58:30.742133Z","iopub.status.idle":"2022-04-21T11:58:30.778509Z","shell.execute_reply.started":"2022-04-21T11:58:30.742104Z","shell.execute_reply":"2022-04-21T11:58:30.777763Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainloss, validloss, trainscore, validscore = train(dataloader_train, dataloader_valid, model = model, optimizer = optimizer, \n      lossfn = lossfn, epochs = epochs, has_mask = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:41:29.535865Z","iopub.execute_input":"2022-04-21T12:41:29.536160Z","iopub.status.idle":"2022-04-21T13:14:13.523305Z","shell.execute_reply.started":"2022-04-21T12:41:29.536129Z","shell.execute_reply":"2022-04-21T13:14:13.522447Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Use best val set model\nstate = torch.load(save_path.replace('.pt', '_best.pt'))\nmodel.load_state_dict(state['model'])\n# Do test\ntest(dataloader_test, model = model, lossfn = lossfn, has_mask = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T13:14:13.525129Z","iopub.execute_input":"2022-04-21T13:14:13.525471Z","iopub.status.idle":"2022-04-21T13:14:24.105951Z","shell.execute_reply.started":"2022-04-21T13:14:13.525433Z","shell.execute_reply":"2022-04-21T13:14:24.105111Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(trainloss)\nplt.plot(validloss)\nplt.legend(['train loss', 'valid loss'])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:02:06.421018Z","iopub.execute_input":"2022-04-18T18:02:06.421281Z","iopub.status.idle":"2022-04-18T18:02:06.652292Z","shell.execute_reply.started":"2022-04-18T18:02:06.421253Z","shell.execute_reply":"2022-04-18T18:02:06.651615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.tensor(trainscore)[:,0]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:02:10.157385Z","iopub.execute_input":"2022-04-18T18:02:10.157636Z","iopub.status.idle":"2022-04-18T18:02:10.180139Z","shell.execute_reply.started":"2022-04-18T18:02:10.157608Z","shell.execute_reply":"2022-04-18T18:02:10.179331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(torch.tensor(trainscore)[:,0])\nplt.plot(torch.tensor(validscore)[:,0])\nplt.legend(['train f1', 'valid f1'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T10:35:57.122514Z","iopub.execute_input":"2022-04-17T10:35:57.122785Z","iopub.status.idle":"2022-04-17T10:35:57.350964Z","shell.execute_reply.started":"2022-04-17T10:35:57.122756Z","shell.execute_reply":"2022-04-17T10:35:57.350121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Below is not used anymore, the best_valid model is auto saved with train()\n# to_save = {'model': model.state_dict(),\n#            'config': config,\n#            'Dconfig': Dconfig,\n#            'optimizer': optimizer,\n#            'lr_s':None}\n# torch.save(to_save, save_path)\n\no = get_csv(dataset_test, model)\no.to_csv(save_path.replace('.pt', '.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T09:10:12.776177Z","iopub.execute_input":"2022-04-17T09:10:12.776758Z","iopub.status.idle":"2022-04-17T09:10:16.874428Z","shell.execute_reply.started":"2022-04-17T09:10:12.776722Z","shell.execute_reply":"2022-04-17T09:10:16.871738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"# Dconfig = {'Dmodel_name':'BiLSTMModel',\n#            'freeze_pretrained':True,\n#            'lstm_num_layer':1, \n#            'classifier_dropout':0.1,\n#            'last_hidden_layer':5,\n#            'epochs': 20}\n# model1 = BiLSTMModel(pretrain_model, tokenizer, **Dconfig).to(device)\n# state1 = torch.load('../input/brian/biLSTM_layer1.pt')\n# assert Dconfig == state1['Dconfig']\n# model1.load_state_dict(state1['model'])\n# print(test(dataloader_test, model = model1, lossfn = lossfn, has_mask = True))\n\n# Dconfig = {'Dmodel_name':'LastAttnModel',\n#            'freeze_pretrained':True,\n#            'classifier_dropout':0.1,\n#            'auxiliary_head':None,\n#            'last_hidden_layer':5,\n#            'all_CLS_attn':True,\n#            'epochs': 20}\n# model2 = LastAttnModel(pretrain_model, tokenizer, **Dconfig).to(device)\n# state2 = torch.load('./lastATNN.pt')\n# assert Dconfig == state2['Dconfig']\n# model2.load_state_dict(state2['model'])\n# print(test(dataloader_test, model = model2, lossfn = lossfn, has_mask = True))\n\n# print(test(dataloader_test, model = [model1, model2], lossfn = lossfn, has_mask = True))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T04:17:57.434272Z","iopub.execute_input":"2022-04-15T04:17:57.434548Z","iopub.status.idle":"2022-04-15T04:17:57.440524Z","shell.execute_reply.started":"2022-04-15T04:17:57.434512Z","shell.execute_reply":"2022-04-15T04:17:57.439764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In case You want know what is `pred`  \n# Check the attension weight to swear word, only useful for LastAttnModel()","metadata":{}},{"cell_type":"code","source":"# for datas in dataloader_valid:\n#     model.eval()\n#     count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n#     pred = model(list(tweet), True)\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-04-15T04:17:57.44187Z","iopub.execute_input":"2022-04-15T04:17:57.442281Z","iopub.status.idle":"2022-04-15T04:17:57.456508Z","shell.execute_reply.started":"2022-04-15T04:17:57.442242Z","shell.execute_reply":"2022-04-15T04:17:57.455762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred, w = pred[0], pred[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-15T04:17:57.457821Z","iopub.execute_input":"2022-04-15T04:17:57.458318Z","iopub.status.idle":"2022-04-15T04:17:57.465773Z","shell.execute_reply.started":"2022-04-15T04:17:57.458275Z","shell.execute_reply":"2022-04-15T04:17:57.465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idx = 6\n# print(f\"Prediction: {pred[idx].argmax().item()}. Ground-Truth: {target[idx].item()}\")\n# print(tweet[idx])\n# w_pure = w[idx][w[idx] !=0 ][1:].cpu().detach().numpy()\n# tokens = tokenizer.tokenize(tweet[idx])\n# full = list(zip(tokens, w_pure))\n# display(full)\n# # here `w_pure.sum() != 1` because the <cls> token score is not included, so will be small then one.","metadata":{"execution":{"iopub.status.busy":"2022-04-15T04:17:57.467503Z","iopub.execute_input":"2022-04-15T04:17:57.467785Z","iopub.status.idle":"2022-04-15T04:17:57.474927Z","shell.execute_reply.started":"2022-04-15T04:17:57.467746Z","shell.execute_reply":"2022-04-15T04:17:57.474214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # combine tokens to word by handling '##'\n# out = []\n# out.append(list(full[0]))\n# for i in range(1, len(full)):\n#     if full[i][0][:2] == '##':\n#         out[-1][0] += full[i][0][2:]\n#         out[-1][1] += full[i][1]\n#     else:\n#         out.append(list(full[i]))\n# out","metadata":{"execution":{"iopub.status.busy":"2022-04-15T04:17:57.476223Z","iopub.execute_input":"2022-04-15T04:17:57.476626Z","iopub.status.idle":"2022-04-15T04:17:57.48445Z","shell.execute_reply.started":"2022-04-15T04:17:57.476577Z","shell.execute_reply":"2022-04-15T04:17:57.483571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # score by attention score\n# sorted(out, key=lambda out: out[1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T04:17:57.486051Z","iopub.execute_input":"2022-04-15T04:17:57.486747Z","iopub.status.idle":"2022-04-15T04:17:57.497688Z","shell.execute_reply.started":"2022-04-15T04:17:57.486693Z","shell.execute_reply":"2022-04-15T04:17:57.496788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
