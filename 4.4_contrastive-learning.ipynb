{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T10:10:52.272251Z","iopub.status.busy":"2022-04-18T10:10:52.271662Z","iopub.status.idle":"2022-04-18T10:11:12.351985Z","shell.execute_reply":"2022-04-18T10:11:12.351088Z","shell.execute_reply.started":"2022-04-18T10:10:52.272173Z"},"trusted":true},"outputs":[],"source":["# !pip install transformers\n","# !wget https://github.com/t-davidson/hate-speech-and-offensive-language/raw/master/data/labeled_data.csv\n","# Put me under data/\n","!pip install ekphrasis\n","!pip3 install nltk emoji"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-18T14:17:39.441213Z","iopub.status.busy":"2022-04-18T14:17:39.440928Z","iopub.status.idle":"2022-04-18T14:17:39.450102Z","shell.execute_reply":"2022-04-18T14:17:39.449415Z","shell.execute_reply.started":"2022-04-18T14:17:39.441181Z"},"tags":[],"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import numpy as np\n","# from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","from sklearn.utils import shuffle\n","import sklearn\n","import random\n","from ekphrasis.classes.spellcorrect import SpellCorrector\n","import re\n","import emoji as emoji\n","import string\n","from sklearn.metrics import classification_report, accuracy_score\n","seed = 888\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:16:58.146169Z","iopub.status.busy":"2022-04-18T14:16:58.145359Z","iopub.status.idle":"2022-04-18T14:16:58.158835Z","shell.execute_reply":"2022-04-18T14:16:58.158082Z","shell.execute_reply.started":"2022-04-18T14:16:58.1461Z"},"trusted":true},"outputs":[],"source":["# from https://github.com/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media\n","def pre_process_dataset(values):\n","    new_values = list()\n","\n","    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\n","                 ';-)',\n","                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\n","                 '=)',\n","                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\n","                 ':>',\n","                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\n","                 ':-|',\n","                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","\n","    for value in values:\n","        text = value.replace(\".\", \" \").lower()\n","        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","        users = re.findall(\"[@]\\w+\", text)\n","        for user in users:\n","            text = text.replace(user, \"<user>\")\n","        urls = re.findall(r'(https?://[^\\s]+)', text)\n","        if len(urls) != 0:\n","            for url in urls:\n","                text = text.replace(url, \"<url >\")\n","        for emo in text:\n","            if emo in emoji.UNICODE_EMOJI:\n","                text = text.replace(emo, \"<emoticon >\")\n","        for emo in emoticons:\n","            text = text.replace(emo, \"<emoticon >\")\n","        numbers = re.findall('[0-9]+', text)\n","        for number in numbers:\n","            text = text.replace(number, \"<number >\")\n","        text = text.replace('#', \"<hashtag >\")\n","        text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","        text = \"\".join(l for l in text if l not in string.punctuation)\n","        text = re.sub(r'[\" \"]+', \" \", text)\n","        new_values.append(text)\n","    return new_values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:16:59.175183Z","iopub.status.busy":"2022-04-18T14:16:59.174435Z","iopub.status.idle":"2022-04-18T14:16:59.590067Z","shell.execute_reply":"2022-04-18T14:16:59.589354Z","shell.execute_reply.started":"2022-04-18T14:16:59.175104Z"},"trusted":true},"outputs":[],"source":["sp = SpellCorrector(corpus=\"english\") \n","print(sp.correct(\"korrect\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:17:00.238832Z","iopub.status.busy":"2022-04-18T14:17:00.238261Z","iopub.status.idle":"2022-04-18T14:17:06.340464Z","shell.execute_reply":"2022-04-18T14:17:06.339701Z","shell.execute_reply.started":"2022-04-18T14:17:00.238795Z"},"trusted":true},"outputs":[],"source":["\n","from ekphrasis.classes.preprocessor import TextPreProcessor\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","\n","text_processor = TextPreProcessor(\n","    # terms that will be normalized\n","   \n","    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n","        'time', 'url', 'date', 'number'],\n","    # terms that will be annotated\n","    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n","        'emphasis', 'censored'},\n","    fix_html=True,  # fix HTML tokens\n","    \n","    # corpus from which the word statistics are going to be used \n","    # for word segmentation \n","    segmenter=\"twitter\", \n","    \n","    # corpus from which the word statistics are going to be used \n","    # for spell correction\n","    corrector=\"twitter\", \n","    \n","    unpack_hashtags=True,  # perform word segmentation on hashtags\n","    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n","    spell_correct_elong=True,  # spell correction for elongated words\n","    spell_correction=True,\n","    remove_tags=True,\n","    # select a tokenizer. You can use SocialTokenizer, or pass your own\n","    # the tokenizer, should take as input a string and return a list of tokens\n","    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n","    \n","    # list of dictionaries, for replacing tokens extracted from the text,\n","    # with other expressions. You can pass more than one dictionaries.\n","    dicts=[emoticons]\n","    \n",")\n","\n","sentences = [\n","    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n","    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n","    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!   YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n","]\n","\n","for s in sentences:\n","    print(\" \".join(text_processor.pre_process_doc(s)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:17:06.342639Z","iopub.status.busy":"2022-04-18T14:17:06.342213Z","iopub.status.idle":"2022-04-18T14:17:06.34666Z","shell.execute_reply":"2022-04-18T14:17:06.345832Z","shell.execute_reply.started":"2022-04-18T14:17:06.3426Z"},"tags":[],"trusted":true},"outputs":[],"source":["# path of data and the name of pretrained weights\n","path1 = '../input/nlpproj/labeled_data.csv'\n","path = '../input/nlpproj/labelled_data_spell.csv'\n","model_name = \"vinai/bertweet-base\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:17:55.207183Z","iopub.status.busy":"2022-04-18T14:17:55.206902Z","iopub.status.idle":"2022-04-18T14:17:55.319873Z","shell.execute_reply":"2022-04-18T14:17:55.319183Z","shell.execute_reply.started":"2022-04-18T14:17:55.207152Z"},"tags":[],"trusted":true},"outputs":[],"source":["# df = pd.read_csv(path, index_col = 0)\n","df1 = pd.read_csv(path)\n","df = df1.dropna().reset_index()\n","\n","df2 = pd.read_csv(path1)\n","df = df2[df1['tweet'].isna()==False].dropna().reset_index()\n","\n","df = shuffle(df)\n","df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:17:57.454493Z","iopub.status.busy":"2022-04-18T14:17:57.453961Z","iopub.status.idle":"2022-04-18T14:17:57.464569Z","shell.execute_reply":"2022-04-18T14:17:57.463904Z","shell.execute_reply.started":"2022-04-18T14:17:57.454457Z"},"tags":[],"trusted":true},"outputs":[],"source":["# 364 & 11176, Just to make sure the experiment is reproducible\n","df.iloc[0:2] "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:14.786148Z","iopub.status.busy":"2022-04-18T14:18:14.785869Z","iopub.status.idle":"2022-04-18T14:18:14.795729Z","shell.execute_reply":"2022-04-18T14:18:14.795054Z","shell.execute_reply.started":"2022-04-18T14:18:14.7861Z"},"trusted":true},"outputs":[],"source":["df['class'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:16.494048Z","iopub.status.busy":"2022-04-18T14:18:16.493526Z","iopub.status.idle":"2022-04-18T14:18:16.535368Z","shell.execute_reply":"2022-04-18T14:18:16.534545Z","shell.execute_reply.started":"2022-04-18T14:18:16.494011Z"},"tags":[],"trusted":true},"outputs":[],"source":["train_data = df.sample(frac = 0.8)\n","test_data = df.drop(train_data.index)\n","valid_data = test_data.sample(frac = 0.5)\n","test_data = test_data.drop(valid_data.index)\n","\n","display(train_data.head())\n","print(\"===================================\")\n","display(valid_data.head())\n","print(\"===================================\")\n","display(test_data.head())\n","\n","print(train_data.shape)\n","print(valid_data.shape)\n","print(test_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Build Dataset\n","\n","The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data df contains 5 columns:\n","\n","count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n","\n","hate_speech = number of CF users who judged the tweet to be hate speech.\n","\n","offensive_language = number of CF users who judged the tweet to be offensive.\n","\n","neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n","\n","class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:19.123941Z","iopub.status.busy":"2022-04-18T14:18:19.123658Z","iopub.status.idle":"2022-04-18T14:18:19.134321Z","shell.execute_reply":"2022-04-18T14:18:19.133546Z","shell.execute_reply.started":"2022-04-18T14:18:19.123911Z"},"tags":[],"trusted":true},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, df, model_name,  train = True, device='cuda'):\n","        super(Dataset, self).__init__()\n","        self.df = df\n","        self.device = device\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        device = self.device\n","        count = torch.LongTensor([self.df.iloc[idx]['count']])\n","        hate_speech = torch.LongTensor([self.df.iloc[idx]['hate_speech']])\n","        offensive_language = torch.LongTensor([self.df.iloc[idx]['offensive_language']])\n","        neither = torch.LongTensor([self.df.iloc[idx]['neither']])\n","        target = torch.LongTensor([self.df.iloc[idx]['class']])\n","        tweet = self.df.iloc[idx]['tweet']\n","        return (count.to(device), hate_speech.to(device), offensive_language.to(device)\n","                , neither.to(device), target.to(device), tweet)\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:21.889879Z","iopub.status.busy":"2022-04-18T14:18:21.889085Z","iopub.status.idle":"2022-04-18T14:18:21.900331Z","shell.execute_reply":"2022-04-18T14:18:21.899444Z","shell.execute_reply.started":"2022-04-18T14:18:21.889839Z"},"trusted":true},"outputs":[],"source":["class DatasetSample(torch.utils.data.Dataset):\n","    def __init__(self, df, model_name,  train = True, device='cuda'):\n","        super(DatasetSample, self).__init__()\n","        self.df = df\n","        self.device = device\n","        self.cls0 = self.df.loc[df['class'] ==0]\n","        self.cls1 = self.df.loc[df['class'] ==1]\n","        self.cls2 = self.df.loc[df['class'] ==2]\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        zero = random.randint(0, len(self.df.loc[df['class'] ==0])-1)\n","        one = random.randint(0, len(self.df.loc[df['class'] ==1])-1)\n","        two = random.randint(0, len(self.df.loc[df['class'] ==2])-1)\n","        tweetHate = self.cls0.iloc[zero]['tweet']\n","        tweetOffen = self.cls1.iloc[one]['tweet']\n","        tweetNeither = self.cls2.iloc[two]['tweet']\n","        return (tweetHate, tweetOffen, tweetNeither)\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:23.586768Z","iopub.status.busy":"2022-04-18T14:18:23.586224Z","iopub.status.idle":"2022-04-18T14:18:23.599203Z","shell.execute_reply":"2022-04-18T14:18:23.598432Z","shell.execute_reply.started":"2022-04-18T14:18:23.58673Z"},"trusted":true},"outputs":[],"source":["tweetHate =  len(df.loc[df['class'] ==0])\n","tweetOffen = len(df.loc[df['class'] ==1])\n","tweetNeither = len(df.loc[df['class'] ==2])\n","print(tweetHate)\n","print(tweetOffen)\n","print(tweetNeither)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:25.94769Z","iopub.status.busy":"2022-04-18T14:18:25.946756Z","iopub.status.idle":"2022-04-18T14:18:25.975903Z","shell.execute_reply":"2022-04-18T14:18:25.97514Z","shell.execute_reply.started":"2022-04-18T14:18:25.947647Z"},"tags":[],"trusted":true},"outputs":[],"source":["dataset = Dataset(df, model_name, device=device)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size= 6, shuffle=True)\n","datasetSample = DatasetSample(df, model_name, device=device)\n","dataloadersample = torch.utils.data.DataLoader(datasetSample, batch_size= 6, shuffle=True)\n","output = next(iter(dataloader))\n","print(list(output[-1]))\n","sentences = list(output[-1])\n","print(\"==================================\")\n","slist = []\n","for s in sentences:\n","    slist.append(\" \".join(text_processor.pre_process_doc(s)))\n","print(slist)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:40.325098Z","iopub.status.busy":"2022-04-18T14:18:40.324798Z","iopub.status.idle":"2022-04-18T14:18:40.360283Z","shell.execute_reply":"2022-04-18T14:18:40.359599Z","shell.execute_reply.started":"2022-04-18T14:18:40.325064Z"},"trusted":true},"outputs":[],"source":["output = next(iter(dataloadersample))\n","print(output)"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:41.826843Z","iopub.status.busy":"2022-04-18T14:18:41.82456Z","iopub.status.idle":"2022-04-18T14:18:41.833355Z","shell.execute_reply":"2022-04-18T14:18:41.832577Z","shell.execute_reply.started":"2022-04-18T14:18:41.826806Z"},"trusted":true},"outputs":[],"source":["def number_params(model, exclude_freeze=False):\n","    \"\"\"calculate the number of parameters in a model\n","\n","    Args:\n","        model (nn.Module): PyTorch model\n","        exclude_freeze (bool, optional): Whether to count the frozen layer. Defaults to False.\n","    \"\"\"\n","    pp = 0\n","    for p in list(model.parameters()):\n","        if exclude_freeze and p.requires_grad is False:\n","            continue\n","        nn = 1\n","        for s in list(p.size()):\n","            nn = nn*s\n","        pp += nn\n","    return pp"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:43.412454Z","iopub.status.busy":"2022-04-18T14:18:43.412197Z","iopub.status.idle":"2022-04-18T14:18:43.431651Z","shell.execute_reply":"2022-04-18T14:18:43.430632Z","shell.execute_reply.started":"2022-04-18T14:18:43.412425Z"},"trusted":true},"outputs":[],"source":["class LastAttnModelForOrdinalReg(nn.Module):\n","    \"\"\"\n","    Use the [CLS] as query and all other output as key and values.\n","    Pass it to a Multi-Head Attention, then a Linear classifier\n","    \n","    Args:\n","      auxiliary_head(list(int)): Only used when training\n","        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n","        - See BertConfig['num_hidden_layers'] for total number of layers\n","        - EG: `auxiliary_head=[10,11,12]`.\n","      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n","\n","    Examples:\n","      tweet = iter(dataloader_train).next()[-1]\n","      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n","      out, attn_weight = modelA(list(tweet))\n","      print(out.shape, attn_weight.shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n","    \"\"\"\n","    \n","    def __init__(self, pretrain_model, tokenizer, \n","                 last_attn_num_head = 8,\n","                 classifier_hidden_dim = 512, \n","                 classifier_dropout = 0,\n","                 num_labels = 3, \n","                 freeze_pretrained=False,\n","                 auxiliary_head=None,\n","                 last_hidden_layer=-1):\n","        super(LastAttnModelForOrdinalReg, self).__init__()\n","        self.pretrain_model = pretrain_model\n","        self.tokenizer = tokenizer\n","        self.auxiliary_head = auxiliary_head\n","        self.num_layers = len(pretrain_model.encoder.layer)\n","        self.last_hidden_layer = last_hidden_layer\n","        \n","        if freeze_pretrained:\n","            if self.auxiliary_head is not None:\n","              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n","            print(\"You are freezing the BERT pertrain\")\n","            for name, p in self.pretrain_model.named_parameters():\n","                if 'classifier' not in name:\n","                    p.requires_grad = False\n","        \n","        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n","        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n","        self.final_classifier = nn.Sequential(\n","            nn.Linear(embed_size, classifier_hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(classifier_dropout),\n","            nn.Linear(classifier_hidden_dim, 1)\n","        )\n","        self.linear_1_bias = nn.Parameter(torch.zeros(2).float())\n","        # deal with aux \n","        if self.auxiliary_head is not None:\n","          self.aux_classifiers = nn.ModuleList()\n","          for i in self.auxiliary_head:\n","            self.aux_classifiers.append(nn.Sequential(\n","              nn.Linear(embed_size, classifier_hidden_dim),\n","              nn.ReLU(),\n","              nn.Dropout(classifier_dropout),\n","              nn.Linear(classifier_hidden_dim, classifier_hidden_dim)\n","            ))\n","\n","\n","        print(f\"Total number of params: {number_params(self)}\")\n","        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n","    def forward(self, src, has_mask=False):\n","        # print(src)\n","        out = []\n","        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n","        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n","        if has_mask == True:\n","            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n","            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n","            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n","        else:\n","            pre_train_output = self.pretrain_model(inputs)\n","        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n","        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n","        last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n","        last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n","        atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n","        last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n","                                                    key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n","        last_attn_out = last_attn_out.squeeze(1) #(N,E)\n","        output = self.final_classifier(last_attn_out)\n","        out += [output, last_attn_w]\n","        ## auxiliary_head\n","        if self.auxiliary_head is not None:\n","          if \"hidden_states\" not in pre_train_output:\n","            raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n","          for idx in range(len(self.aux_classifiers)):\n","            hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n","            out.append(self.aux_classifiers[idx](hidden_cls))\n","        out = out[0]\n","        out = out + self.linear_1_bias\n","        \n","        return out, torch.sigmoid(out)\n","    \n","# ## Usage\n","# tweet = iter(dataloader_train).next()[-1]\n","# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n","# out, attn_weight = modelA(list(tweet))\n","# print(out.shape, attn_weight.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:45.349066Z","iopub.status.busy":"2022-04-18T14:18:45.34854Z","iopub.status.idle":"2022-04-18T14:18:45.364065Z","shell.execute_reply":"2022-04-18T14:18:45.363042Z","shell.execute_reply.started":"2022-04-18T14:18:45.34903Z"},"trusted":true},"outputs":[],"source":["class LanguageModelCNNForContrastive(nn.Module):\n","    \n","    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False,):\n","        super(LanguageModelCNNForContrastive, self).__init__()\n","        self.model = AutoModel.from_pretrained(model_name, num_labels=num_labels)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n","        \n","        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(624, 624) # before : 442 with max_length 36 # 806 with max_length 64\n","        self.flat = nn.Flatten()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_pretrained:\n","            print(\"You are freezing the BERT\")\n","            for name, p in self.model.named_parameters():\n","                if 'classifier' not in name:\n","                    p.requires_grad = False\n","                if '11' in name:\n","                     p.requires_grad = True\n","                if '10' in name:\n","                     p.requires_grad = True\n","                if '9' in name:\n","                     p.requires_grad = True\n","        print(f\"Total number of params: {number_params(self.model)}\")\n","        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n","\n","    def forward(self, src, has_mask=False):\n","        # print(src)\n","        slist = []\n","        for s in src:\n","            slist.append(\" \".join(text_processor.pre_process_doc(s)))\n","        output = self.tokenizer(slist, padding='max_length', truncation=True, max_length=50)\n","        output = torch.LongTensor(output['input_ids']).to(device)\n","        if has_mask == True:\n","            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n","            output = self.model(output, attention_mask=attention_mask, output_hidden_states=True)\n","        else:\n","            output = self.model(output, output_hidden_states=True)\n","        \n","        out = output['hidden_states']\n","        \n","        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in out]), 0), 0, 1)\n","        # print(x.size())# 32 13 50 768\n","        x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n","        x = self.dropout(self.flat(self.dropout(x)))\n","        #print(x.size())\n","        x = self.fc(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:46.940148Z","iopub.status.busy":"2022-04-18T14:18:46.939883Z","iopub.status.idle":"2022-04-18T14:18:46.967463Z","shell.execute_reply":"2022-04-18T14:18:46.966734Z","shell.execute_reply.started":"2022-04-18T14:18:46.940101Z"},"trusted":true},"outputs":[],"source":["class LastAttnModelForContrastive(nn.Module):\n","    \"\"\"\n","    Use the [CLS] as query and all other output as key and values.\n","    Pass it to a Multi-Head Attention, then a Linear classifier\n","    \n","    Args:\n","      auxiliary_head(list(int)): Only used when training\n","        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n","        - See BertConfig['num_hidden_layers'] for total number of layers\n","        - EG: `auxiliary_head=[10,11,12]`.\n","      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n","\n","    Examples:\n","      tweet = iter(dataloader_train).next()[-1]\n","      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n","      out, attn_weight = modelA(list(tweet))\n","      print(out.shape, attn_weight.shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n","    \"\"\"\n","    \n","    def __init__(self, pretrain_model, tokenizer, \n","                 last_attn_num_head = 8,\n","                 classifier_hidden_dim = 512, \n","                 classifier_dropout = 0,\n","                 num_labels = 3, \n","                 freeze_pretrained=False,\n","                 auxiliary_head=None,\n","                 last_hidden_layer=-1):\n","        super(LastAttnModelForContrastive, self).__init__()\n","        self.pretrain_model = pretrain_model\n","        self.tokenizer = tokenizer\n","        self.auxiliary_head = auxiliary_head\n","        self.num_layers = len(pretrain_model.encoder.layer)\n","        self.last_hidden_layer = last_hidden_layer\n","        # self.stage1 = stage1\n","\n","        if freeze_pretrained:\n","            if self.auxiliary_head is not None:\n","              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n","            print(\"You are freezing the BERT pertrain\")\n","            for name, p in self.pretrain_model.named_parameters():\n","                if 'classifier' not in name:\n","                    p.requires_grad = False\n","#                 if '11' in name:\n","#                      p.requires_grad = True\n","#                 if '10' in name:\n","#                      p.requires_grad = True\n","#                 if '9' in name:\n","#                      p.requires_grad = True\n","        \n","        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n","        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n","        self.final_classifier = nn.Sequential(\n","            nn.Linear(embed_size, classifier_hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(classifier_dropout),\n","            nn.Linear(classifier_hidden_dim, 20)\n","        )\n","        self.stageone = nn.Sequential(nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, num_labels))\n","        # deal with aux \n","        if self.auxiliary_head is not None:\n","          self.aux_classifiers = nn.ModuleList()\n","          for i in self.auxiliary_head:\n","            self.aux_classifiers.append(nn.Sequential(\n","              nn.Linear(embed_size, classifier_hidden_dim),\n","              nn.ReLU(),\n","              nn.Dropout(classifier_dropout),\n","              nn.Linear(classifier_hidden_dim, classifier_hidden_dim)\n","            ))\n","\n","\n","        print(f\"Total number of params: {number_params(self)}\")\n","        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n","    def forward(self, src, has_mask=False, stage1 = False, nograd = False):\n","        # print(src)\n","        if nograd == True:\n","            with torch.no_grad():\n","                out = []\n","                tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n","                inputs = torch.LongTensor(tokens['input_ids']).to(device)\n","                if has_mask == True:\n","                    # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n","                    attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n","                    pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n","                else:\n","                    pre_train_output = self.pretrain_model(inputs)\n","                # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n","                last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n","                last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n","                last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n","                atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n","                last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n","                                                            key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n","                last_attn_out = last_attn_out.squeeze(1) #(N,E)\n","                output = self.final_classifier(last_attn_out)\n","                out += [output, last_attn_w]\n","                ## auxiliary_head\n","                if self.auxiliary_head is not None:\n","                  if \"hidden_states\" not in pre_train_output:\n","                    raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n","                  for idx in range(len(self.aux_classifiers)):\n","                    hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n","                    out.append(self.aux_classifiers[idx](hidden_cls))\n","        else:\n","            out = []\n","            tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n","            inputs = torch.LongTensor(tokens['input_ids']).to(device)\n","            if has_mask == True:\n","                # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n","                attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n","                pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n","            else:\n","                pre_train_output = self.pretrain_model(inputs)\n","            # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n","            last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n","            last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n","            last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n","            atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n","            last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n","                                                        key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n","            last_attn_out = last_attn_out.squeeze(1) #(N,E)\n","            output = self.final_classifier(last_attn_out)\n","            out += [output, last_attn_w]\n","            ## auxiliary_head\n","            if self.auxiliary_head is not None:\n","              if \"hidden_states\" not in pre_train_output:\n","                raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n","              for idx in range(len(self.aux_classifiers)):\n","                hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n","                out.append(self.aux_classifiers[idx](hidden_cls))\n","        if stage1 == True:\n","            outlabel = self.stageone(out[0])\n","            return [outlabel]\n","        else:\n","            outlabel = self.stageone(out[0])\n","            return out, [outlabel]\n","    \n","# ## Usage\n","# tweet = iter(dataloader_train).next()[-1]\n","# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n","# out, attn_weight = modelA(list(tweet))\n","# print(out.shape, attn_weight.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:47.931227Z","iopub.status.busy":"2022-04-18T14:18:47.930943Z","iopub.status.idle":"2022-04-18T14:18:47.94659Z","shell.execute_reply":"2022-04-18T14:18:47.945146Z","shell.execute_reply.started":"2022-04-18T14:18:47.931195Z"},"trusted":true},"outputs":[],"source":["class LanguageModelCNN(nn.Module):\n","    \n","    def __init__(self, model_name, num_labels = 3, freeze_pretrained=True):\n","        super(LanguageModelCNN, self).__init__()\n","        self.model = AutoModel.from_pretrained(model_name, num_labels=num_labels)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n","        \n","        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(624, 3) # before : 442 with max_length 36 # 806 with max_length 64\n","        self.flat = nn.Flatten()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        if freeze_pretrained:\n","            print(\"You are freezing the BERT\")\n","            for name, p in self.model.named_parameters():\n","                if 'classifier' not in name:\n","                    p.requires_grad = False\n","#                 if '11' in name:\n","#                     p.requires_grad = True\n","#                 if '10' in name:\n","#                      p.requires_grad = True\n","#                 if '9' in name:\n","#                      p.requires_grad = True\n","        print(f\"Total number of params: {number_params(self.model)}\")\n","        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n","\n","    def forward(self, src, has_mask=False):\n","        # print(src)\n","        slist = []\n","        for s in src:\n","            slist.append(\" \".join(text_processor.pre_process_doc(s)))\n","        output = self.tokenizer(slist, padding='max_length', truncation=True, max_length=50)\n","        output = torch.LongTensor(output['input_ids']).to(device)\n","        if has_mask == True:\n","            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n","            output = self.model(output, attention_mask=attention_mask, output_hidden_states=True)\n","        else:\n","            output = self.model(output, output_hidden_states=True)\n","        \n","        out = output['hidden_states']\n","        \n","        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in out]), 0), 0, 1)\n","        # print(x.size())# 32 13 50 768\n","        x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n","        x = self.dropout(self.flat(self.dropout(x)))\n","        #print(x.size())\n","        x = self.fc(x)\n","        \n","        return [self.softmax(x)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:48.846873Z","iopub.status.busy":"2022-04-18T14:18:48.846318Z","iopub.status.idle":"2022-04-18T14:18:48.911987Z","shell.execute_reply":"2022-04-18T14:18:48.911067Z","shell.execute_reply.started":"2022-04-18T14:18:48.846837Z"},"trusted":true},"outputs":[],"source":["class LanguageModelAllAttn(nn.Module):\n","    \n","    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False, last_attn_num_head = 8,):\n","        super(LanguageModelAllAttn, self).__init__()\n","        \n","        self.model = AutoModel.from_pretrained(model_name, num_labels=num_labels)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n","        embed_size =self.model.embeddings.word_embeddings.embedding_dim\n","        # self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(624, 3) # before : 442 with max_length 36 # 806 with max_length 64\n","        self.flat = nn.Flatten()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        # 13 transformers for bert \n","        self.attn = nn.ModuleList([nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True) for i in range(13)])\n","        self.conv = nn.Conv1d(13, 1, 1, stride=1, padding=0)\n","        self.linear = nn.Linear(768,3)\n","        if freeze_pretrained:\n","            print(\"You are freezing the BERT\")\n","            for name, p in self.model.named_parameters():\n","                if 'classifier' not in name:\n","                    p.requires_grad = False\n","                if '11' in name:\n","                    p.requires_grad = True\n","                if '10' in name:\n","                     p.requires_grad = True\n","#                 if '9' in name:\n","#                      p.requires_grad = True\n","        print(f\"Total number of params: {number_params(self.model)}\")\n","        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n","\n","    def forward(self, src, has_mask=False):\n","        # print(src)\n","        slist = []\n","        for s in src:\n","            slist.append(\" \".join(text_processor.pre_process_doc(s)))\n","        output = self.tokenizer(slist, padding='max_length', truncation=True, max_length=50)\n","        tok = torch.LongTensor(output['input_ids']).to(device)\n","        \n","        if has_mask == True:\n","            attention_mask=(tok != 0).float() # here `0` is the <pad> token, i guess\n","            output = self.model(tok, attention_mask=attention_mask, output_hidden_states=True)\n","        else:\n","            output = self.model(tok, output_hidden_states=True)\n","        \n","        out = output['hidden_states']\n","        \n","        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in out]), 0), 0, 1)\n","        # print(x.size())# 32 13 50 768\n","        layer_attns = []\n","        for i in range(x.size(1)):\n","        \n","            hidden_state_cls = x[:, i, 0, :].unsqueeze(1)   # (N,i,1,E)\n","            hidden_state_rest = x[:, i, 1:, :] # (N,i,T-1,E)\n","            atten_mask_pad = (tok == 0)[:,1:] #(N,T-1)\n","            last_attn_out, last_attn_w = self.attn[i](hidden_state_cls, hidden_state_rest, hidden_state_rest,\n","                                                        key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n","            last_attn_out = last_attn_out.squeeze(1) #(N,E)\n","            layer_attns.append(last_attn_out)\n","        layer_outputs = torch.stack(layer_attns)\n","        layer_outputs = torch.transpose(layer_outputs, 0, 1)\n","        \n","        layer_outputs = self.relu(self.conv(layer_outputs).squeeze(1))\n","        #print(layer_outputs.size())\n","        layer_outputs = self.linear(layer_outputs)\n","        #print(layer_outputs.size())\n","       \n","        \n","        \n","        \n","        #x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n","        #x = self.dropout(self.flat(self.dropout(x)))\n","        #print(x.size())\n","        #x = self.fc(x)\n","        \n","        return [self.softmax(layer_outputs)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:18:51.23072Z","iopub.status.busy":"2022-04-18T14:18:51.230463Z","iopub.status.idle":"2022-04-18T14:18:51.244282Z","shell.execute_reply":"2022-04-18T14:18:51.24363Z","shell.execute_reply.started":"2022-04-18T14:18:51.230692Z"},"tags":[],"trusted":true},"outputs":[],"source":["class LanguageModel(nn.Module):\n","    \n","    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False):\n","        super(LanguageModel, self).__init__()\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n","        if freeze_pretrained:\n","            print(\"You are freezing the BERT\")\n","            for name, p in self.model.named_parameters():\n","                if 'classifier' not in name:\n","                    p.requires_grad = False\n","                if '11' in name:\n","                     p.requires_grad = True\n","                if '10' in name:\n","                     p.requires_grad = True\n","                if '9' in name:\n","                     p.requires_grad = True\n","        print(f\"Total number of params: {number_params(self.model)}\")\n","        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n","\n","    def forward(self, src, has_mask=False):\n","        # print(src)\n","        slist = []\n","        for s in src:\n","            slist.append(\" \".join(text_processor.pre_process_doc(s)))\n","        output = self.tokenizer(slist, padding=True, truncation=True, max_length=50)\n","        output = torch.LongTensor(output['input_ids']).to(device)\n","        if has_mask == True:\n","            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n","            output = self.model(output, attention_mask=attention_mask)\n","        else:\n","            output = self.model(output)\n","        print(output[0].size())\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T12:41:43.174991Z","iopub.status.busy":"2022-04-18T12:41:43.174733Z","iopub.status.idle":"2022-04-18T12:41:55.889716Z","shell.execute_reply":"2022-04-18T12:41:55.888971Z","shell.execute_reply.started":"2022-04-18T12:41:43.174962Z"},"tags":[],"trusted":true},"outputs":[],"source":["from transformers import AutoModel\n","from transformers import BertModel, BertTokenizer\n","dataset = Dataset(train_data, model_name)\n","dataloader_train = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","dataset_valid = Dataset(valid_data, model_name)\n","dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=64, shuffle=False)\n","dataset_test = Dataset(test_data, model_name)\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=False)\n","datasetSample = DatasetSample(train_data, model_name, device=device)\n","dataloadersample = torch.utils.data.DataLoader(datasetSample, batch_size= 1, shuffle=False)\n","\n","model_name = 'bert-base-uncased'\n","lossfn = nn.CrossEntropyLoss()\n","# model = LanguageModel(model_name, freeze_pretrained=False).to(device)\n","# model = LanguageModelAllAttn(model_name, freeze_pretrained=True,).to(device)\n","model = LanguageModelCNN(model_name).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","epochs = 30"]},{"cell_type":"markdown","metadata":{},"source":["# Train directly"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T12:41:55.89173Z","iopub.status.busy":"2022-04-18T12:41:55.891466Z","iopub.status.idle":"2022-04-18T12:41:55.907734Z","shell.execute_reply":"2022-04-18T12:41:55.906853Z","shell.execute_reply.started":"2022-04-18T12:41:55.891692Z"},"tags":[],"trusted":true},"outputs":[],"source":["def train(dataloader_train, dataloader_valid = None, model = None, \n","          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n","    \n","    trainloss = []\n","    validloss = []\n","    trainscore = []\n","    validscore = []\n","\n","    for i in range(epochs):\n","        model.train()\n","        averageloss = 0\n","        averagescore = 0\n","        for datas in tqdm(dataloader_train):\n","            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","            optimizer.zero_grad()\n","            pred = model(list(tweet), has_mask)\n","            loss = lossfn(pred[0], target.squeeze(1))\n","            loss.backward()\n","            optimizer.step()\n","            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n","            averageloss += loss.item()/len(dataloader_train)\n","            averagescore += f1score/len(dataloader_train)\n","        #print(classification_report(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy()))\n","        trainloss.append(averageloss)\n","        trainscore.append(averagescore)\n","        if dataloader_valid is not None:\n","            model.eval()\n","            averageloss = 0\n","            averagescore = 0\n","            for datas in tqdm(dataloader_valid):\n","                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","                pred = model(list(tweet), has_mask)\n","                loss = lossfn(pred[0], target.squeeze(1))\n","                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n","                averageloss += loss.item()/len(dataloader_valid)\n","                averagescore += f1score/len(dataloader_valid)\n","            #print(classification_report(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy()))\n","            validloss.append(averageloss)\n","            validscore.append(averagescore)\n","            print(f\"epoch: {i}, train loss: {trainloss[-1]}, validation loss: {validloss[-1]}\\n train f1score: {trainscore[-1]}, validation f1score: {validscore[-1]}\")\n","        else:\n","            print(f\"epoch: {i}, train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n","\n","        if (i+1) % 1 == 0:\n","            torch.save({\n","                    'model': model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'trainloss': trainloss,\n","                    'trainscore': trainscore,\n","                    'validloss': validloss,\n","                    'validscore': validscore\n","                    }, './outputmodel_{}'.format(i))\n","    return trainloss, validloss, trainscore, validscore\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# untoggle to train this session\n","# trainloss, validloss, trainscore, validscore = train(dataloader_train, dataloader_valid, model = model, optimizer = optimizer, \n","#       lossfn = lossfn, epochs = epochs, has_mask = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T01:49:07.085281Z","iopub.status.busy":"2022-04-18T01:49:07.084979Z","iopub.status.idle":"2022-04-18T01:49:07.098594Z","shell.execute_reply":"2022-04-18T01:49:07.097866Z","shell.execute_reply.started":"2022-04-18T01:49:07.085245Z"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def test(dataloader_valid, model):\n","    averageloss = 0\n","    averagescore = 0\n","    averagePrecision = 0\n","    averageRecall = 0\n","    tocsv = []\n","    with torch.no_grad():\n","        for datas in tqdm(dataloader_valid):\n","            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","\n","            pred = model(list(tweet), True)\n","            tocsv.append(pred[0].cpu().numpy()[0])\n","            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n","            precision=sklearn.metrics.precision_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n","            recall=sklearn.metrics.recall_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(),  average='weighted', zero_division=0)\n","\n","    #         averageloss += loss.item()/len(dataloader_valid)\n","            averagescore += f1score/len(dataloader_valid)\n","            averagePrecision += precision/len(dataloader_valid)\n","            averageRecall += recall/len(dataloader_valid)\n","\n","    #     print(\"averageloss: {}\".format(averageloss))\n","        print(\"averagescore: {}\".format(averagescore))\n","        print(\"averagePrecision: {}\".format(averagePrecision))\n","        print(\"averageRecall: {}\".format(averageRecall))\n","\n","        print(\"=============\")\n","        return tocsv\n","\n","# tocsv = test(dataloader_test, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-17T17:16:30.616489Z","iopub.status.busy":"2022-04-17T17:16:30.615638Z","iopub.status.idle":"2022-04-17T17:16:30.620913Z","shell.execute_reply":"2022-04-17T17:16:30.619268Z","shell.execute_reply.started":"2022-04-17T17:16:30.616434Z"},"trusted":true},"outputs":[],"source":["# df = pd.DataFrame(tocsv)\n","# df.to_csv('./outputmodel_CNN.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Train Contrastive Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:19:03.141146Z","iopub.status.busy":"2022-04-18T14:19:03.140825Z","iopub.status.idle":"2022-04-18T14:19:03.151344Z","shell.execute_reply":"2022-04-18T14:19:03.150659Z","shell.execute_reply.started":"2022-04-18T14:19:03.141099Z"},"trusted":true},"outputs":[],"source":["class ContrastiveLoss(nn.Module):\n","\n","    def __init__(self,):\n","        super(ContrastiveLoss, self).__init__()\n","        self.crit =  nn.MSELoss()\n","        self.crit2 = nn.BCEWithLogitsLoss()\n","        self.ce =  nn.CrossEntropyLoss()\n","        self.softmax = nn.Softmax(dim = -1)\n","    def forward(self, pred, embeddings, target, topk=10, MSELoss = True):\n","#         print(pred.size()) # 32, 20\n","#         print(embeddings.size())# 3, 20\n","#         print(target.size()) # 32\n","        loss = 0\n","        simlist = []\n","        for j in range(embeddings.size(0)):\n","            # cosineSim = nn.functional.cosine_similarity(pred, embeddings[j].unsqueeze(0), dim=1, eps=1e-08) \n","            # size = cosineSim.size()\n","            simlist.append(pred*embeddings[j].unsqueeze(0))\n","        cosineSim = torch.stack(simlist) # 3,32, 20\n","        cosineSim = torch.transpose(cosineSim,0,1) # 32,3,20\n","        cosineSim = torch.sum(cosineSim, dim= -1)# 32,3\n","        targetOnehot = nn.functional.one_hot(target, num_classes=3)\n","        # loss = self.ce(cosineSim, target)\n","        logits = self.softmax(cosineSim)\n","        pos = logits*targetOnehot\n","        pos = torch.sum(pos, dim=-1)\n","        logitssum = torch.sum(logits, dim=-1)\n","        pos = -torch.log(pos/logitssum)\n","#         if topk < pred.size(0):\n","#             pos = torch.topk(pos, topk)[0]\n","#             loss = torch.sum(pos)/topk\n","#         else:\n","#             loss = torch.sum(pos)/pred.size(0)\n","        loss = torch.sum(pos)/pred.size(0)\n","        return loss, logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:19:08.097059Z","iopub.status.busy":"2022-04-18T14:19:08.096497Z","iopub.status.idle":"2022-04-18T14:19:08.110548Z","shell.execute_reply":"2022-04-18T14:19:08.109552Z","shell.execute_reply.started":"2022-04-18T14:19:08.097022Z"},"trusted":true},"outputs":[],"source":["dataset = Dataset(train_data, model_name)\n","dataloader_train = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","dataset_valid = Dataset(valid_data, model_name)\n","dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n","dataset_test = Dataset(test_data, model_name)\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)\n","datasetSample = DatasetSample(train_data, model_name, device=device)\n","dataloadersample = torch.utils.data.DataLoader(datasetSample, batch_size= 1, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:19:12.150473Z","iopub.status.busy":"2022-04-18T14:19:12.149755Z","iopub.status.idle":"2022-04-18T14:19:24.829594Z","shell.execute_reply":"2022-04-18T14:19:24.827729Z","shell.execute_reply.started":"2022-04-18T14:19:12.150435Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModel\n","from transformers import BertModel, BertTokenizer\n","from transformers import AutoTokenizer, AutoConfig\n","# model = LanguageModel(model_name, freeze_pretrained=False).to(device)\n","# model = LanguageModelAllAttn(model_name, freeze_pretrained=True,).to(device)\n","# model = LanguageModelCNNForContrastive(model_name).to(device)\n","lossfn = ContrastiveLoss()\n","model_name = 'bert-base-uncased'\n","config = AutoConfig.from_pretrained(\n","    model_name, \n","    output_hidden_states = True,\n","    output_attention = False,\n","#     hidden_dropout_prob = 0.2,\n",") \n","pretrain_model = AutoModel.from_pretrained(\n","    model_name,\n","    config = config\n",").to(device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = LastAttnModelForContrastive(pretrain_model, tokenizer, freeze_pretrained=True,\n","                       classifier_dropout=0,\n","                       auxiliary_head=None,\n","                       last_hidden_layer=-1).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","epochs = 100"]},{"cell_type":"markdown","metadata":{},"source":["# Stage1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:19:24.831658Z","iopub.status.busy":"2022-04-18T14:19:24.831305Z","iopub.status.idle":"2022-04-18T14:19:24.843816Z","shell.execute_reply":"2022-04-18T14:19:24.843053Z","shell.execute_reply.started":"2022-04-18T14:19:24.831617Z"},"trusted":true},"outputs":[],"source":["class ContrastiveLoss(nn.Module):\n","\n","    def __init__(self,):\n","        super(ContrastiveLoss, self).__init__()\n","        self.crit =  nn.MSELoss()\n","        self.crit2 = nn.BCEWithLogitsLoss()\n","        self.ce =  nn.CrossEntropyLoss()\n","        self.softmax = nn.Softmax(dim = -1)\n","    def forward(self, pred, embeddings, target, topk=10, MSELoss = True):\n","#         print(pred.size()) # 32, 20\n","#         print(embeddings.size())# 3, 20\n","#         print(target.size()) # 32\n","        loss = 0\n","        simlist = []\n","        for j in range(embeddings.size(0)):\n","            # cosineSim = nn.functional.cosine_similarity(pred, embeddings[j].unsqueeze(0), dim=1, eps=1e-08) \n","            # size = cosineSim.size()\n","            simlist.append(pred*embeddings[j].unsqueeze(0))\n","        cosineSim = torch.stack(simlist) # 3,32, 20\n","        cosineSim = torch.transpose(cosineSim,0,1) # 32,3,20\n","        cosineSim = torch.sum(cosineSim, dim= -1)# 32,3\n","        targetOnehot = nn.functional.one_hot(target, num_classes=3)\n","        # loss = self.ce(cosineSim, target)\n","        logits = self.softmax(cosineSim)\n","        pos = logits*targetOnehot\n","        pos = torch.sum(pos, dim=-1)\n","        logitssum = torch.sum(logits, dim=-1)\n","        pos = -torch.log(pos/logitssum)\n","#         if topk < pred.size(0):\n","#             pos = torch.topk(pos, topk)[0]\n","#             loss = torch.sum(pos)/topk\n","#         else:\n","#             loss = torch.sum(pos)/pred.size(0)\n","        loss = torch.sum(pos)/pred.size(0)\n","        return loss, logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T14:19:24.845906Z","iopub.status.busy":"2022-04-18T14:19:24.845126Z","iopub.status.idle":"2022-04-18T14:19:24.856807Z","shell.execute_reply":"2022-04-18T14:19:24.856088Z","shell.execute_reply.started":"2022-04-18T14:19:24.845866Z"},"trusted":true},"outputs":[],"source":["lossfn = ContrastiveLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","epochs = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# run this code to train stage one.\n","count = 0\n","compareIter = 10\n","trainloss = []\n","validloss = []\n","trainscore = []\n","validscore = []\n","trainscoreCE = []\n","validscoreCE = []\n","for i in range(epochs):\n","    averagescore = 0\n","    averagescoreCE = 0\n","    averageloss = 0\n","    for datas in tqdm(dataloader_train):\n","        model.train()\n","        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","        optimizer.zero_grad()\n","#         with torch.no_grad():\n","        pred, spec = model(list(tweet), True)\n","        ce =  nn.CrossEntropyLoss()\n","        # loss1 = ce(spec[0],  target.squeeze(1))\n","        spec = torch.argmax(spec[0], dim=1)\n","        spec = spec.cpu().detach().numpy()\n","        pred = pred[0]\n","            # print(pred.size())\n","        argmaxs = []\n","        loss = 0\n","        logitslist = []\n","        for k in range(compareIter):\n","            sampledatas = next(iter(dataloadersample))\n","            inputsample = []\n","            for j in range(len(sampledatas)): #3\n","                inputsample.append(sampledatas[j][0])\n","            #with torch.no_grad():\n","            pred1, spec1 = model(list(inputsample), True)\n","            pred1 = pred1[0]\n","            loss, logits = lossfn(pred, pred1, target.squeeze(1))\n","            logitslist.append(logits)\n","            loss += loss/compareIter\n","        loss = loss\n","        logitslist = torch.stack(logitslist)\n","        logitslist = torch.sum(logitslist, dim=0) # 32,3\n","        logitslist = torch.argmax(logitslist, dim=1)\n","        argmaxs = logitslist.cpu().detach().numpy()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        f1score = sklearn.metrics.f1_score( target.squeeze(1).cpu().numpy(), argmaxs, average = 'weighted')\n","        f1score_ce = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), spec, average = 'weighted')\n","#         print(f1score)\n","        averageloss += loss.item()/len(dataloader_train)\n","        averagescore += f1score/len(dataloader_train)\n","        averagescoreCE += f1score_ce/len(dataloader_train)\n","#     print(averagescore)\n","#     print(averageloss)\n","    trainloss.append(averageloss)\n","    trainscore.append(averagescore)\n","    trainscoreCE.append(averagescoreCE)\n","    averagescore = 0\n","    averageloss = 0\n","    averagescoreCE = 0\n","    for datas in tqdm(dataloader_valid):\n","        \n","        model.eval()\n","        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","        with torch.no_grad():\n","            pred, spec = model(list(tweet), True)\n","            ce =  nn.CrossEntropyLoss()\n","            loss1 = ce(spec[0],  target.squeeze(1))\n","            spec = torch.argmax(spec[0], dim=1)\n","            spec = spec.cpu().detach().numpy()\n","            pred = pred[0]\n","            argmaxs = []\n","            loss = 0\n","            logitslist = []\n","            for k in range(compareIter):\n","                sampledatas = next(iter(dataloadersample))\n","                inputsample = []\n","                for j in range(len(sampledatas)): #3\n","                    inputsample.append(sampledatas[j][0])\n","\n","                loss, logits = lossfn(pred, model(list(inputsample), True)[0][0], target.squeeze(1))\n","                logitslist.append(logits)\n","                loss += loss/compareIter\n","            loss = loss\n","            logitslist = torch.stack(logitslist)\n","            logitslist = torch.sum(logitslist, dim=0) # 32,3\n","            logitslist = torch.argmax(logitslist, dim=1)\n","            argmaxs = logitslist.cpu().detach().numpy()\n","            \n","            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), argmaxs, average = 'weighted')\n","            f1score_ce = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), spec, average = 'weighted')\n","    #         print(f1score)\n","            averageloss += loss.item()/len(dataloader_valid)\n","            averagescore += f1score/len(dataloader_valid)\n","            averagescoreCE += f1score_ce/len(dataloader_valid)\n","    validloss.append(averageloss)\n","    validscore.append(averagescore)\n","    validscoreCE.append(averagescoreCE)\n","    if (i+1) % 1 == 0:\n","        torch.save({\n","                'model': model.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'trainloss': trainloss,\n","                'trainscore': trainscore,\n","                'validloss': validloss,\n","                'validscore': validscore\n","                }, './outputmodel_{}'.format(i))\n","    print(f\"epoch: {i}, train loss: {trainloss[-1]}, validation loss: {validloss[-1]}\\n train f1score: {trainscore[-1]}, validation f1score: {validscore[-1]}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T01:49:18.857457Z","iopub.status.busy":"2022-04-18T01:49:18.857134Z","iopub.status.idle":"2022-04-18T01:49:18.863687Z","shell.execute_reply":"2022-04-18T01:49:18.863026Z","shell.execute_reply.started":"2022-04-18T01:49:18.857423Z"},"trusted":true},"outputs":[],"source":["# checkpoint = torch.load('../input/outputmodel10k/outputmodel_0')\n","# model.load_state_dict(checkpoint['model'], strict=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T01:49:18.865876Z","iopub.status.busy":"2022-04-18T01:49:18.865106Z","iopub.status.idle":"2022-04-18T01:49:18.896786Z","shell.execute_reply":"2022-04-18T01:49:18.895781Z","shell.execute_reply.started":"2022-04-18T01:49:18.865837Z"},"trusted":true},"outputs":[],"source":["# for k=10, outputmodel10 is the best\n","compareIter = 10\n","def test_stage1(): \n","    averagescore = 0\n","    for datas in tqdm(dataloader_valid):\n","        model.eval()\n","        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","        with torch.no_grad():\n","            pred, spec = model(list(tweet), True)\n","            ce =  nn.CrossEntropyLoss()\n","            loss1 = ce(spec[0],  target.squeeze(1))\n","            spec = torch.argmax(spec[0], dim=1)\n","            spec = spec.cpu().detach().numpy()\n","            pred = pred[0]\n","            argmaxs = []\n","            loss = 0\n","            logitslist = []\n","            for k in range(compareIter):\n","                sampledatas = next(iter(dataloadersample))\n","                inputsample = []\n","                for j in range(len(sampledatas)): #3\n","                    inputsample.append(sampledatas[j][0])\n","\n","                loss, logits = lossfn(pred, model(list(inputsample), True)[0][0], target.squeeze(1))\n","                logitslist.append(logits)\n","                loss += loss/compareIter\n","            loss = loss\n","            logitslist = torch.stack(logitslist)\n","            logitslist = torch.sum(logitslist, dim=0) # 32,3\n","            logitslist = torch.argmax(logitslist, dim=1)\n","            argmaxs = logitslist.cpu().detach().numpy()\n","            \n","            f1score = sklearn.metrics.f1_score( target.squeeze(1).cpu().numpy(), argmaxs, average = 'weighted')\n","            averagescore += f1score/len(dataloader_valid)\n","    print(averagescore)\n","    \n","# for i in range(20):\n","#     checkpoint = torch.load('../input/outputmodel10k/outputmodel_{}'.format(i))\n","#     model.load_state_dict(checkpoint['model'], strict=False)\n","#     test_stage1()"]},{"cell_type":"markdown","metadata":{},"source":["# Stage2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T15:26:45.129038Z","iopub.status.busy":"2022-04-18T15:26:45.128769Z","iopub.status.idle":"2022-04-18T15:26:45.134374Z","shell.execute_reply":"2022-04-18T15:26:45.133696Z","shell.execute_reply.started":"2022-04-18T15:26:45.129009Z"},"trusted":true},"outputs":[],"source":["class SoftCrossEntropyLoss(nn.Module):\n","    def __init__(self, weights):\n","        super().__init__()\n","        self.weights = weights\n","\n","    def forward(self, y_hat, y):\n","        p = F.log_softmax(y_hat, 1)\n","        w_labels = self.weights*y\n","        loss = -(w_labels*p).sum() / (w_labels).sum()\n","        return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T15:26:46.081092Z","iopub.status.busy":"2022-04-18T15:26:46.080615Z","iopub.status.idle":"2022-04-18T15:26:46.086957Z","shell.execute_reply":"2022-04-18T15:26:46.08607Z","shell.execute_reply.started":"2022-04-18T15:26:46.081056Z"},"trusted":true},"outputs":[],"source":["\n","lossfn = SoftCrossEntropyLoss(1)\n","lossfn = nn.CrossEntropyLoss()\n","epochs = 40\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T15:27:03.025158Z","iopub.status.busy":"2022-04-18T15:27:03.024874Z","iopub.status.idle":"2022-04-18T15:27:03.370747Z","shell.execute_reply":"2022-04-18T15:27:03.369987Z","shell.execute_reply.started":"2022-04-18T15:27:03.025121Z"},"trusted":true},"outputs":[],"source":["# can load pretrain model from stage1\n","# checkpoint = torch.load('./outputmodel_11')\n","# model.load_state_dict(checkpoint['model'], strict=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T15:27:08.617441Z","iopub.status.busy":"2022-04-18T15:27:08.616943Z","iopub.status.idle":"2022-04-18T15:27:08.636833Z","shell.execute_reply":"2022-04-18T15:27:08.636107Z","shell.execute_reply.started":"2022-04-18T15:27:08.617403Z"},"trusted":true},"outputs":[],"source":["def train(dataloader_train, dataloader_valid = None, model = None, \n","          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n","    \n","    trainloss = []\n","    validloss = []\n","    trainscore = []\n","    validscore = []\n","\n","    for i in range(epochs):\n","        model.train()\n","        averageloss = 0\n","        averagescore = 0\n","        for datas in tqdm(dataloader_train):\n","            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","            smoothTarget = torch.stack([hate_speech.squeeze(-1), offensive_language.squeeze(-1), neither.squeeze(-1)])\n","            smoothTarget = torch.transpose(smoothTarget,  0, 1)\n","            sumSmooth = torch.sum(smoothTarget,dim=-1).unsqueeze(1)\n","            smoothTarget = smoothTarget/sumSmooth\n","            \n","            optimizer.zero_grad()\n","            pred = model(list(tweet), has_mask, stage1 = True, nograd = True)\n","            loss = lossfn(pred[0], target.squeeze(1))\n","            loss.backward()\n","            optimizer.step()\n","            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n","            averageloss += loss.item()/len(dataloader_train)\n","            averagescore += f1score/len(dataloader_train)\n","        #print(classification_report(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy()))\n","        trainloss.append(averageloss)\n","        trainscore.append(averagescore)\n","        if dataloader_valid is not None:\n","            model.eval()\n","            averageloss = 0\n","            averagescore = 0\n","            for datas in tqdm(dataloader_valid):\n","                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","                smoothTarget = torch.stack([hate_speech.squeeze(-1), offensive_language.squeeze(-1), neither.squeeze(-1)])\n","                smoothTarget = torch.transpose(smoothTarget,  0, 1)\n","                sumSmooth = torch.sum(smoothTarget,dim=-1).unsqueeze(1)\n","                smoothTarget = smoothTarget/sumSmooth\n","\n","                pred = model(list(tweet), has_mask, stage1 = True)\n","                loss = lossfn(pred[0], target.squeeze(1))\n","                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n","                averageloss += loss.item()/len(dataloader_valid)\n","                averagescore += f1score/len(dataloader_valid)\n","            #print(classification_report(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy()))\n","            validloss.append(averageloss)\n","            validscore.append(averagescore)\n","            print(f\"epoch: {i}, train loss: {trainloss[-1]}, validation loss: {validloss[-1]}\\n train f1score: {trainscore[-1]}, validation f1score: {validscore[-1]}\")\n","        else:\n","            print(f\"epoch: {i}, train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n","        if (i+1) % 1 == 0 and i >5:\n","            torch.save({\n","                    'model': model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'trainloss': trainloss,\n","                    'trainscore': trainscore,\n","                    'validloss': validloss,\n","                    'validscore': validscore\n","                    }, './outputmodel_{}'.format(i))\n","\n","    return trainloss, validloss, trainscore, validscore\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T15:27:15.796718Z","iopub.status.busy":"2022-04-18T15:27:15.795993Z","iopub.status.idle":"2022-04-18T16:03:05.21846Z","shell.execute_reply":"2022-04-18T16:03:05.217098Z","shell.execute_reply.started":"2022-04-18T15:27:15.796684Z"},"trusted":true},"outputs":[],"source":["trainloss1, validloss1, trainscore1, validscore1 = train(dataloader_train, dataloader_valid, model = model, optimizer = optimizer, \n","      lossfn = lossfn, epochs = epochs, has_mask = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Test contrastive"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T16:13:30.542051Z","iopub.status.busy":"2022-04-18T16:13:30.541776Z","iopub.status.idle":"2022-04-18T16:13:31.628812Z","shell.execute_reply":"2022-04-18T16:13:31.627965Z","shell.execute_reply.started":"2022-04-18T16:13:30.54202Z"},"trusted":true},"outputs":[],"source":["!ls ../input/nlpmodel/\n","dataset = Dataset(train_data, model_name)\n","dataloader_train = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","dataset_valid = Dataset(valid_data, model_name)\n","dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=32, shuffle=False)\n","dataset_test = Dataset(test_data, model_name)\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)\n","datasetSample = DatasetSample(train_data, model_name, device=device)\n","dataloadersample = torch.utils.data.DataLoader(datasetSample, batch_size= 1, shuffle=False)\n","\n","# can load model after training\n","# checkpoint = torch.load('../input/bugfixnlpmodel/outputmodel_3610knewtune')\n","# checkpoint = torch.load('./outputmodel_19')\n","# model.load_state_dict(checkpoint['model'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T16:13:31.912129Z","iopub.status.busy":"2022-04-18T16:13:31.911872Z","iopub.status.idle":"2022-04-18T16:14:05.29905Z","shell.execute_reply":"2022-04-18T16:14:05.298179Z","shell.execute_reply.started":"2022-04-18T16:13:31.912082Z"},"trusted":true},"outputs":[],"source":["def test(dataloader_valid, model):\n","    averageloss = 0\n","    averagescore = 0\n","    averagePrecision = 0\n","    averageRecall = 0\n","    tocsv = []\n","    targettocsv = []\n","    with torch.no_grad():\n","        for datas in tqdm(dataloader_valid):\n","            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n","            smoothTarget = torch.stack([hate_speech.squeeze(-1), offensive_language.squeeze(-1), neither.squeeze(-1)])\n","            smoothTarget = torch.transpose(smoothTarget,  0, 1)\n","            sumSmooth = torch.sum(smoothTarget,dim=-1).unsqueeze(1)\n","            smoothTarget = smoothTarget/sumSmooth\n","\n","            pred = model(list(tweet), True, stage1=True)\n","#             print(pred[0])\n","            tocsv.append(pred[0].cpu().numpy()[0])\n","            targettocsv.append(target.squeeze(1).cpu().numpy())\n","            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n","            precision=sklearn.metrics.precision_score( target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n","            recall=sklearn.metrics.recall_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average='weighted', zero_division=0)\n","\n","    #         averageloss += loss.item()/len(dataloader_valid)\n","            averagescore += f1score/len(dataloader_valid)\n","            averagePrecision += precision/len(dataloader_valid)\n","            averageRecall += recall/len(dataloader_valid)\n","\n","    #     print(\"averageloss: {}\".format(averageloss))\n","        print(\"averagescore: {}\".format(averagescore))\n","        print(\"averagePrecision: {}\".format(averagePrecision))\n","        print(\"averageRecall: {}\".format(averageRecall))\n","\n","        print(\"=============\")\n","    return tocsv, targettocsv\n","# for i in range(33):\n","#     print(i+6)\n","#     checkpoint = torch.load('./outputmodel_{}'.format(i+6))\n","#     model.load_state_dict(checkpoint['model'], strict=True)\n","tocsv, targettocsv = test(dataloader_test, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T16:14:05.303171Z","iopub.status.busy":"2022-04-18T16:14:05.302896Z","iopub.status.idle":"2022-04-18T16:14:05.339892Z","shell.execute_reply":"2022-04-18T16:14:05.339251Z","shell.execute_reply.started":"2022-04-18T16:14:05.303135Z"},"trusted":true},"outputs":[],"source":["# df = pd.DataFrame(tocsv)\n","# df.to_csv('./outputmodel_CLnew.csv')\n","# df = pd.DataFrame(targettocsv)\n","# df.to_csv('./target.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T16:14:05.341775Z","iopub.status.busy":"2022-04-18T16:14:05.341097Z","iopub.status.idle":"2022-04-18T16:14:05.357268Z","shell.execute_reply":"2022-04-18T16:14:05.356647Z","shell.execute_reply.started":"2022-04-18T16:14:05.341736Z"},"trusted":true},"outputs":[],"source":["files = ['../input/nlpcsvs/bi_lstm.csv',\n","          '../input/nlpcsvs/lastATNN.csv','../input/nlpcsvs/lastATNN_allCLS.csv', './outputmodel_CL.csv']\n","# 0.900, 0.893, 0.901, 0.900\n","files = ['./outputmodel_CLnew.csv'] \n","def combine(files = []):\n","    softmax = nn.Softmax(dim=-1)\n","    for i in range(len(files)):\n","        df = pd.read_csv(files[i] ,index_col = 0)\n","        df = torch.tensor(df.values)\n","        df = softmax(df)\n","        if i > 0:\n","            df_combine += df\n","        if i == 0:\n","            df_combine = df\n","    df_combine = df_combine.argmax(-1).numpy()\n","    return df_combine\n","df_combine = combine(files)\n","target = pd.read_csv('./target.csv', index_col = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T16:14:05.359419Z","iopub.status.busy":"2022-04-18T16:14:05.35916Z","iopub.status.idle":"2022-04-18T16:14:05.37326Z","shell.execute_reply":"2022-04-18T16:14:05.372103Z","shell.execute_reply.started":"2022-04-18T16:14:05.359384Z"},"trusted":true},"outputs":[],"source":["f1score = sklearn.metrics.f1_score(target, df_combine, average = 'weighted')\n","precision = sklearn.metrics.precision_score(target, df_combine, average='weighted', zero_division=0)\n","recall = sklearn.metrics.recall_score(target, df_combine, average='weighted', zero_division=0)\n","print(f1score)\n","print(precision)\n","print(recall)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
