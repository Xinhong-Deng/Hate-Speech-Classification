{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\n!pip install pyenchant\n!wget http://archive.ubuntu.com/ubuntu/pool/main/libr/libreoffice-dictionaries/hunspell-id_6.4.3-1_all.deb\n!dpkg -i hunspell-id_6.4.3-1_all.deb\n!apt update && apt install -y enchant libenchant1c2a hunspell hunspell-en-us libhunspell-1.6-0\nnltk.download('wordnet')\n!sudo apt-get install libenchant1c2a -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThere is functionality for plotting confusion matrices in scikit-learn, but they are not very pretty\nAs such, this code is used, available at a public github repository from:\n\nTrimarchi, D., 2019. DTrimarchi10/confusion_matrix. GitHub.\nAvailable at:\nhttps://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py\n[Accessed May 7, 2021]'''\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm\n        using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be\n                   shown in each square.\n    categories:    List of strings containing the categories to be displayed on\n                    the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix.\n                    Default is True.\n    normalize:     If True, show the proportions for each category.\n                    Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off\n                    the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the\n                    figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure.\n                    Default is True.\n    figsize:       Tuple representing the figure size. Default will be the\n                    matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm.\n                    Default is 'Blues'\n        See\n        http://matplotlib.org/examples/color/colormaps_reference.html\n\n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) / float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] / sum(cf[:,1])\n            recall    = cf[1,1] / sum(cf[1,:])\n            f1_score  = 2*precision*recall / (precision + recall)\n            stats_text = \"\"\"\n            \\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\n            \"\"\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,\n                annot=box_labels,\n                fmt=\"\",\n                cmap=cmap,\n                cbar=cbar,\n                xticklabels=categories,\n                yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n\n    if title:\n        plt.title(title)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(string):\n    temp = string.lower()\n    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n    # path = '../input/ai6127/labeled_data.csv'\n    temp = re.sub(\"[!:]+\",\"\", temp)\n    temp = re.sub(r\"&amp;\", \"\", temp)\n#     temp = re.sub(\"[A-Za-z0-9_]+\",\"\", temp)\n    temp = re.sub(\"#\",\"\", temp)\n#     temp = re.sub(r\"http\\S+\", \"<url>\", temp)\n#     temp = re.sub(r\"www.\\S+\", \"<url>\", temp)\n    temp = re.sub(r\"http\\S+\", \"\", temp)\n    temp = re.sub(r\"www.\\S+\", \"\", temp)\n    temp = re.sub('[()!?]', ' ', temp)\n    temp = re.sub('\\[.*?\\]',' ', temp)\n    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n    temp = re.sub(r\"rt\", \"\", temp)\n    temp = temp.strip()\n    return temp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_contractions(string):\n    expanded_words = []   \n    for word in string.split():\n      # using contractions.fix to expand the shortened words\n      expanded_words.append(contractions.fix(word))  \n\n    expanded_text = ' '.join(expanded_words)\n    return expanded_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def misspellings(string):\n    d = enchant.request_dict(\"en_US\")\n    chkr = SpellChecker(\"en_US\", string)\n    for err in chkr:\n        suggest = d.suggest(err.word)\n        if len(suggest) != 0:\n            err.replace(suggest[0])\n    \n    return chkr.get_text()#print(chkr.get_text())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification,AutoModel\nfrom transformers import AutoTokenizer, AutoConfig\nfrom sklearn.utils import shuffle\nimport sklearn\nimport random\nimport warnings\nimport re\nimport contractions\n# from math import comb\n\nfrom enchant.checker import SpellChecker\nimport enchant\n\nseed = 888\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using {device}\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def dup_data(df, on_class, repeat=1000):\n    \"\"\" data augmentation\n    \n    This will random pick two row and mix them together\n    So two sentence will be concat tgt, generate a longer sentence\n    For safety, it only do augmentation on the same class\n      It will NOT generate new data nest-ed-ly. \n      Means augmented data will not be used to generate new data\n    \n    Args:\n        df: DataFrame\n        on_class (int): Which class to augment, either [0,1,2]\n        repeat (int): how many new data to generate\n    \n    Return:\n        pd.DataFrame: The augmented data and org df will be concat tgt\n        \n    Examples:\n        >>> df = dup_data(df, 0)\n    \"\"\"\n    cl = {'hate_speech': 0, 'offensive_language': 1, 'neither': 2}\n    out = []\n    tmp = df[df['class'] == on_class]\n#     print(f\"class={on_class} have {tmp.shape[0]} data, each time pick 2. nCr = {comb(tmp.shape[0], 2)}\")\n    for k in range(repeat):\n        i,j = random.randint(0, tmp.shape[0]-1), random.randint(0, tmp.shape[0]-1)\n        out.append(tmp.iloc[i]+tmp.iloc[j])\n    out = pd.concat(out, axis=1).T\n    out[['count', 'hate_speech', 'offensive_language', 'neither', 'class']] = out[['count', 'hate_speech', 'offensive_language', 'neither', 'class']].astype('int')\n    # handle edge case\n    out['class'] = out[['hate_speech', 'offensive_language', 'neither']].idxmax(1).map({'hate_speech': 0, 'offensive_language': 1, 'neither': 2})\n    return pd.concat([df,out]).reset_index(drop=True)\n\ndef number_params(model, exclude_freeze=False):\n    \"\"\"calculate the number of parameters in a model\n\n    Args:\n        model (nn.Module): PyTorch model\n        exclude_freeze (bool, optional): Whether to count the frozen layer. Defaults to False.\n    \"\"\"\n    pp = 0\n    for p in list(model.parameters()):\n        if exclude_freeze and p.requires_grad is False:\n            continue\n        nn = 1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef ensemble(models, do_argmax=False):\n    \"\"\" ensemble models prediction\n    \n    Args:\n        models (list(tensor)): list of prediction, each prediction should have same shape(N,C).\n    \n    Examples:\n        tweet = iter(dataloader_train).next()[-1]\n        out = model(list(tweet))\n        ensemble([out[0], out[0]])\n    \"\"\"\n    out = torch.softmax(models[0], 1)\n    for i in range(1, len(models)):\n        out += torch.softmax(models[i], 1)\n    if do_argmax:\n        return out.argmax(1)\n    else:\n        return out\n\ndef finetune(\n        model: nn.Module,\n        base_lr: float,\n        groups,\n        ignore_the_rest: bool = False,\n        raw_query: bool = False,\n        regex=False):\n    \"\"\" This is something call per-parameter options\n\n    Separate out the finetune parameters with a learning rate for each layers of parameters\n    This function only support setting a different learning rate for each layer's arameter.\n    Depending on the optimizer, you can set extra parameter for that layer for the optmizer -> See Notes \n    If you freeze layer using this function and want to unfreeze it later:\n    See https://discuss.pytorch.org/t/correct-way-to-freeze-layers/26714/2\n\n    Args:\n        model (nn.Module): Pytorch Model\n        base_lr (float): learning rate of all layers\n        groups (Dict[str, float]): key is `name` of layers, value is the `extra_lr` (or False).\n          all layers that contains that `name` will have `lr` of base_lr*extra_lr.\n          it uses fnmatch|regex to check whether a layer contains that `name`.\n          fnmatch is matching structure like `layer1*`, `layer?.conv?.`, `*conv2*`, etc...\n          regex is the comman regex matching.\n          Hence, `name` here is either fnmatch or regex expression if using raw_query.\n          If `float` is False: those layers with `name` will be freeze. \n          In particular, they will not be included in the return output and require_grad will be set to False\n        ignore_the_rest (bool, optional): Include the remaining layer that are not stated in `grouprs` or not. Defaults to False.\n        raw_query (bool, optional): Modify the keys of `groups` as f'*{key}*' if False. Only useful when `regex=False`\n          Do not do any modification to the keys of `groups` if True. Defaults to False.\n        regex (bool, optional): Use regex instead of fnmatch on keys of groups. Defaults to False.\n          This will overrride raw_query to True. \n          Notice: `regex=False` is depracted\n\n    Returns:\n        List[Dict[str, Union[float, Iterable]]]: list of dict that has two or more key-value pair.\n          The first one is feature generation layers. [those layers must start with `features` name] <usually is backbone>\n            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'query':query, 'lr':base_lr*groups[groups.keys()]]\n          The remaining are all others layer. [all others params for last one, if ignore_the_rest = False]\n            is a dict['params':list(model.parameters()), 'names':list(`layer's name`), 'lr':base_lr]\n\n    Examples:\n        >>> model = models.resnet50()\n        >>> # all layers that has name start with `layer1 and layer2` will have learning rate `0.001*0.01`\n        >>> # all layers that has name start with `layer3` will be froozen`\n        >>> # all layers that has name start with `layer4` will have learning rate `0.001*0.001`\n        >>> # for all other layers will have the base_lr `0.001`\n        >>> model_params = finetune(model, base_lr=0.001, groups={'^layer[1-2].*': 0.01, '^layer3.*': False, '^layer4.*': 0.001}, regex=True)\n        >>> # setting extra parameter (other than learning rate) for that optimizer\n        >>> # the second param_group `layer4` will have weight_decay 1e-2\n        >>> model_params[1]['weight_decay'] = 1e-2\n        >>> # init optimizer with the above setting\n        >>> # the argument under `torch.optim.SGD` will be overrided by finetune() if they exist.\n        >>> # For example, all model_params will have weight_decay=5e-3 except model_params[1]\n        >>> optimizer = torch.optim.SGD(model_params, momentum=0.9, lr=0.1, weight_decay=5e-3)\n    \"\"\"\n    if regex:\n        raw_query = True\n    else:\n        warnings.warn(\"regex=False is deprecated; use regex=True\", DeprecationWarning)\n    # Deal with Freeze Later\n    freeze_group = dict()\n    freeze = False\n    for k,v in groups.items():\n        if v is False:\n            freeze_group[k] = 1\n            freeze=True\n    for k in freeze_group.keys():\n        del groups[k]\n    freeze_group = \"(\" + \")|(\".join(freeze_group) + \")\"\n\n    parameters = [\n        dict(params=[],\n             names=[],\n             query=query if raw_query else '*' + query + '*',\n             lr = lr * base_lr,\n             initial_lr = lr * base_lr) for query, lr in groups.items()\n    ]\n    rest_parameters = dict(params=[], names=[], lr=base_lr, initial_lr=base_lr)\n    for k, v in model.named_parameters():\n        rest = 0\n        if freeze and regex and re.match(freeze_group, k):\n            v.requires_grad = False\n            continue\n        for group in parameters:\n            if not regex and fnmatch(k, group['query']):\n                group['params'].append(v)\n                group['names'].append(k)\n                rest = 1\n                break\n            elif regex and re.compile(group['query']).search(k):\n                group['params'].append(v)\n                group['names'].append(k)\n                rest = 1\n                break\n        if rest == 0:\n            rest_parameters['params'].append(v)\n            rest_parameters['names'].append(k)\n\n    if not ignore_the_rest:\n        parameters.append(rest_parameters)\n    for group in parameters:\n        group['params'] = iter(group['params'])\n    return parameters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# path of data and the name of pretrained weights\n# path = '../input/ai6127/labeled_data_spell.csv'\npath = '../input/ai6127/labeled_data.csv'","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path, index_col = 0).dropna()\ndf[\"tweet\"] = df[\"tweet\"].apply(fix_contractions)\ndf[\"tweet\"] = df[\"tweet\"].apply(preprocess)\ndf[\"tweet\"] = df[\"tweet\"].apply(misspellings)\ndf = shuffle(df)\ndf.describe()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 364 & 11176, Just to make sure the experiment is reproducible\ndf.iloc[0:2] ","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['class'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data into train and test\ntrain_data = df.sample(frac = 0.8)\ntest_data = df.drop(train_data.index)\nvalid_data = test_data.sample(frac = 0.5)\ntest_data = test_data.drop(valid_data.index)\n\ndisplay(train_data.head())\nprint(\"===================================\")\ndisplay(valid_data.head())\nprint(\"===================================\")\ndisplay(test_data.head())\n\nprint(train_data.shape)\nprint(valid_data.shape)\nprint(test_data.shape)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['class'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Dataset\n\nThe data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data df contains 5 columns:\n\ncount = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n\nhate_speech = number of CF users who judged the tweet to be hate speech.\n\noffensive_language = number of CF users who judged the tweet to be offensive.\n\nneither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n\nclass = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_name=None,  train = True, device='cuda'):\n        super(Dataset, self).__init__()\n        self.df = df\n        self.device = device\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        device = self.device\n        count = torch.LongTensor([self.df.iloc[idx]['count']])\n        hate_speech = torch.LongTensor([self.df.iloc[idx]['hate_speech']])\n        offensive_language = torch.LongTensor([self.df.iloc[idx]['offensive_language']])\n        neither = torch.LongTensor([self.df.iloc[idx]['neither']])\n        target = torch.LongTensor([self.df.iloc[idx]['class']])\n        tweet = self.df.iloc[idx]['tweet']\n        return (count.to(device), hate_speech.to(device), offensive_language.to(device)\n                , neither.to(device), target.to(device), tweet)\n        \n        ","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This one is just for DEBUG, not the real dataset to be used\ndataset = Dataset(df, device=device)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size= 6, shuffle=True)\noutput = next(iter(dataloader))\nprint(output)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    \n    def __init__(self, model_name, num_labels = 3, freeze_pretrained=False):\n        super(LanguageModel, self).__init__()\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if freeze_pretrained:\n            print(\"You are freezing the BERT\")\n            for name, p in self.model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        print(f\"Total number of params: {number_params(self.model)}\")\n        print(f\"Total number of trainable params: {number_params(self.model, exclude_freeze=True)}\")\n\n    def forward(self, src, has_mask=False):\n        # print(src)\n        output = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        output = torch.LongTensor(output['input_ids']).to(device)\n        if has_mask == True:\n            attention_mask=(output != 0).float() # here `0` is the <pad> token, i guess\n            output = self.model(output, attention_mask=attention_mask)\n        else:\n            output = self.model(output)\n        return output","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LastAttnModel(nn.Module):\n    \"\"\"\n    Use the [CLS] as query and all other output as key and values.\n    Pass it to a Multi-Head Attention, then a Linear classifier\n    \n    Args:\n      auxiliary_head(list(int)): Only used when training\n        - list of idx of hidden_layers that will be used as auxiliary_head. Here `idx` start from 1\n        - See BertConfig['num_hidden_layers'] for total number of layers\n        - EG: `auxiliary_head=[10,11,12]`.\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n      all_CLS_attn(bool): use last CLS as query, all previous CLS as key and values -> Multi-Head Attention\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n      out = modelA(list(tweet))\n      print(out[0].shape, attn_weight[1].shape) #torch.Size([32, 3]) torch.Size([32, 1, 49])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_model, tokenizer, \n                 last_attn_num_head = 8,\n                 classifier_hidden_dim = 512, \n                 classifier_dropout = 0,\n                 num_labels = 3, \n                 freeze_pretrained=False,\n                 auxiliary_head=None,\n                 last_hidden_layer=-1,\n                 all_CLS_attn=False,\n                 **kwargs):\n        super(LastAttnModel, self).__init__()\n        self.pretrain_model = pretrain_model\n        self.tokenizer = tokenizer\n        self.auxiliary_head = auxiliary_head\n        self.num_layers = len(pretrain_model.encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        self.all_CLS_attn = all_CLS_attn\n        \n        if freeze_pretrained:\n            if self.auxiliary_head is not None:\n              warnings.warn(\"freeze_pretrained and auxiliary_head set to True together is useless for training. Consider use `finetune()`\")\n            print(\"You are freezing the BERT pertrain\")\n            for name, p in self.pretrain_model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        \n        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n        self.last_attn = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n        self.final_classifier = nn.Sequential(\n            nn.Linear(embed_size, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        # deal with aux \n        if self.auxiliary_head is not None:\n          self.aux_classifiers = nn.ModuleList()\n          for i in self.auxiliary_head:\n            self.aux_classifiers.append(nn.Sequential(\n              nn.Linear(embed_size, classifier_hidden_dim),\n              nn.ReLU(),\n              nn.Dropout(classifier_dropout),\n              nn.Linear(classifier_hidden_dim, num_labels)\n            ))\n            \n        # use all CLS attention\n        if self.all_CLS_attn:\n            self.all_CLS = nn.MultiheadAttention(embed_size, last_attn_num_head, batch_first=True)\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n        else:\n            pre_train_output = self.pretrain_model(inputs)\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        last_hidden_state_cls = last_hidden[:, 0, :].unsqueeze(1)   # (N,1,E)\n        if self.all_CLS_attn:\n            o = [hidden[:,0,:] for hidden in pre_train_output[\"hidden_states\"][:self.last_hidden_layer-1]]\n            rest = torch.stack(o).permute(1,0,2)  # (N,self.last_hidden_layer-1,E)\n            last_hidden_state_cls,_=self.all_CLS(last_hidden_state_cls, rest, rest)\n        last_hidden_state_rest = last_hidden[:, 1:, :] # (N,T-1,E)\n        atten_mask_pad = (inputs == 0)[:,1:] #(N,T-1)\n        last_attn_out, last_attn_w = self.last_attn(last_hidden_state_cls, last_hidden_state_rest, last_hidden_state_rest,\n                                                    key_padding_mask=atten_mask_pad) #(N,1,E), (N,1,T-1)\n        last_attn_out = last_attn_out.squeeze(1) #(N,E)\n        output = self.final_classifier(last_attn_out)\n        out += [output, last_attn_w]\n        ## auxiliary_head\n        if self.auxiliary_head is not None:\n          if \"hidden_states\" not in pre_train_output:\n            raise Exception(\"Put `pre_train_output=True` in AutoConfig\")\n          for idx in range(len(self.aux_classifiers)):\n            hidden_cls = pre_train_output[\"hidden_states\"][self.auxiliary_head[idx-1]][:, 0, :] # (N,E)\n            out.append(self.aux_classifiers[idx](hidden_cls))\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out, attn_weight = modelA(list(tweet))\n# print(out.shape, attn_weight.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiLSTMModel(nn.Module):\n    \"\"\"\n    Fit the last layer BERT output to bi-lstm.\n    Concat the forward and backward final hidden state, then a Linear classifier\n    \n    Args:\n      last_hidden_layer(int): Treat the output of this layer as last_hidden_layer\n\n    Examples:\n      tweet = iter(dataloader_train).next()[-1]\n      modelA = BiLSTMModel(pretrain_model, tokenizer).to(device)\n      out  = modelA(list(tweet))\n      print(out[0].shape) #torch.Size([32, 3])\n      \n    Returns:\n      list(tensor): the first tensor is the prediction, the second is the attention weight\n    \"\"\"\n    \n    def __init__(self, pretrain_model, tokenizer, \n                 lstm_hidden = 1024,\n                 lstm_num_layer = 2,\n                 classifier_hidden_dim = 512, \n                 classifier_dropout = 0,\n                 num_labels = 3, \n                 freeze_pretrained=False,\n                 last_hidden_layer=-1,\n                 **kwargs):\n        super(BiLSTMModel, self).__init__()\n        self.pretrain_model = pretrain_model\n        self.tokenizer = tokenizer\n        self.lstm_hidden = lstm_hidden\n        self.lstm_num_layer = lstm_num_layer\n        self.num_layers = len(pretrain_model.encoder.layer)\n        self.last_hidden_layer = last_hidden_layer\n        \n        if freeze_pretrained:\n            print(\"You are freezing the BERT pertrain\")\n            for name, p in self.pretrain_model.named_parameters():\n                if 'classifier' not in name:\n                    p.requires_grad = False\n        \n        embed_size = pretrain_model.embeddings.word_embeddings.embedding_dim\n        self.lstm = nn.LSTM(embed_size, lstm_hidden, lstm_num_layer, bidirectional=True, batch_first=True)\n        \n        self.final_classifier = nn.Sequential(\n            nn.Linear(2*lstm_hidden, classifier_hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(classifier_hidden_dim, num_labels)\n        )\n\n        print(f\"Total number of params: {number_params(self)}\")\n        print(f\"Total number of trainable params: {number_params(self, exclude_freeze=True)}\")\n    def forward(self, src, has_mask=False, count=None):\n        # print(src)\n        out = []\n        tokens = self.tokenizer(src, padding=True, truncation=True, max_length=50)\n        inputs = torch.LongTensor(tokens['input_ids']).to(device)\n        if has_mask == True:\n            # attention_mask=(inputs != 0).float() # here `0` is the <pad> token, i guess\n            attention_mask = torch.LongTensor(tokens['attention_mask']).to(device)\n            pre_train_output = self.pretrain_model(inputs, attention_mask=attention_mask)\n        else:\n            pre_train_output = self.pretrain_model(inputs)\n        # last_hidden = pre_train_output[\"last_hidden_state\"]      # (N,T,E)\n        last_hidden = pre_train_output[\"hidden_states\"][self.last_hidden_layer]\n        batch_size, seq_len, embed_size = last_hidden.shape\n        output, (h_n, c_n) = self.lstm(last_hidden)\n        output = output.view(batch_size, seq_len, 2, self.lstm_hidden) #batch, seq_len, num_directions, hidden_size\n        h_n = h_n.view(self.lstm_num_layer, 2, batch_size, self.lstm_hidden) # num_layers, num_directions, batch, hidden_size\n        c_n = c_n.view(self.lstm_num_layer, 2, batch_size, self.lstm_hidden) # num_layers, num_directions, batch, hidden_size\n        forward_last = h_n[-1, 0, :, :]  #(N, H)\n        backward_last = h_n[-1, 1, :, :] #(N, H)\n        output = torch.hstack([forward_last, backward_last]) #(N,2H)\n        output = self.final_classifier(output)\n        out += [output]\n        return out\n    \n# ## Usage\n# tweet = iter(dataloader_train).next()[-1]\n# modelA = LastAttnModel(pretrain_model, tokenizer).to(device)\n# out = modelA(list(tweet))\n# print(out[0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization","metadata":{}},{"cell_type":"code","source":"dataset = Dataset(train_data)\ndataloader_train = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\ndataset_valid = Dataset(valid_data)\ndataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=32, shuffle=False)\ndataset_test = Dataset(test_data)\ndataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=False)\n\nlossfn = nn.CrossEntropyLoss().to(device)\n# lossfn = nn.CrossEntropyLoss(weight=torch.tensor([4.6761, 1.8533, 4.0554])).to(device)\n\n\n# model_name = 'bert-base-uncased'\n# model_name = 'cardiffnlp/twitter-roberta-base-emotion'\n# model_name = \"pysentimiento/bertweet-hate-speech\"\n# model_name = 'GroNLP/hateBERT'\n# model = LanguageModel(model_name, freeze_pretrained=True).to(device)\n\nconfig = AutoConfig.from_pretrained(\n    model_name, \n    output_hidden_states = True,\n    output_attention = False,\n    hidden_dropout_prob = 0.2,\n    from_tf=True\n) \nprint(config)\npretrain_model = AutoModel.from_pretrained(\n    model_name,\n    config = config\n).to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nDconfig = {'Dmodel_name':'LastAttnModel',\n           'freeze_pretrained':True,\n           'classifier_dropout':0.1,\n           'auxiliary_head':None,\n           'last_hidden_layer':5,\n           'all_CLS_attn':True,\n           'epochs': 20}\nmodel = LastAttnModel(pretrain_model, tokenizer, **Dconfig).to(device)\n# Dconfig = {'Dmodel_name':'BiLSTMModel',\n#            'freeze_pretrained':True,\n#            'lstm_num_layer':1, \n#            'classifier_dropout':0.1,\n#            'last_hidden_layer':5,\n#            'epochs': 20}\n# model = BiLSTMModel(pretrain_model, tokenizer, **Dconfig).to(device)\nprint(Dconfig)\n\n# model_params = finetune(model, base_lr=1e-4, groups={'^pretrain_model.*':0.01}, regex=True)\n# model_params = finetune(model, base_lr=1e-4, \n#                         groups={'^pretrain_model.encoder.layer.([0-2])\\..*': False, \n#                                 '^pretrain_model.encoder.layer.([3-4])\\..*': 0.01, \n#                                 '^pretrain_model.encoder.layer.([5]|1[012]).*': False, \n#                                 '^pretrain_model.pooler.*': False,\n#                                 '^pretrain_model.embeddings.*':False},\n#                         regex=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\nepochs = Dconfig['epochs']\n\nsave_path=\"./lastATNN.pt\"","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for datas in dataloader_train:\n    count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(list(tweet))[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train(dataloader_train, dataloader_valid = None, model = None, \n          optimizer = None, lossfn = None,  epochs = 10, has_mask = True):\n    \n    trainloss = []\n    validloss = []\n    trainscore = []\n    validscore = []\n    bestt_score = 0\n    for i in range(epochs):\n        model.train()\n        averageloss = 0\n        averagef1 = 0\n        averagePrecision = 0\n        averageRecall = 0\n        for datas in tqdm(dataloader_train):\n            count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n            optimizer.zero_grad()\n            pred = model(list(tweet), has_mask)\n            loss = lossfn(pred[0], target.squeeze(1))\n            # aux head\n            for j in range(2, len(pred)):\n                loss+=0.3*lossfn(pred[j], target.squeeze(1))\n            loss.backward()\n            optimizer.step()\n            f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n            precision=sklearn.metrics.precision_score(pred[0].argmax(-1).cpu().numpy(), target.squeeze(1).cpu().numpy(), average='weighted', zero_division=0)\n            recall=sklearn.metrics.recall_score(pred[0].argmax(-1).cpu().numpy(), target.squeeze(1).cpu().numpy(), average='weighted', zero_division=0)\n            averageloss += loss.item()/len(dataloader_train)\n            averagef1 += f1score/len(dataloader_train)\n            averagePrecision += precision/len(dataloader_train)\n            averageRecall += recall/len(dataloader_train)\n        trainloss.append(averageloss)\n        trainscore.append((averagef1, averagePrecision, averageRecall))\n        if dataloader_valid is not None:\n            model.eval()\n            averageloss = 0\n            averagef1 = 0\n            averagePrecision = 0\n            averageRecall = 0\n            for datas in tqdm(dataloader_valid):\n                count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n                pred = model(list(tweet), has_mask)\n                loss = lossfn(pred[0], target.squeeze(1))\n                f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n                precision=sklearn.metrics.precision_score(pred[0].argmax(-1).cpu().numpy(), target.squeeze(1).cpu().numpy(), average='weighted', zero_division=0)\n                recall=sklearn.metrics.recall_score(pred[0].argmax(-1).cpu().numpy(), target.squeeze(1).cpu().numpy(), average='weighted', zero_division=0)\n                averageloss += loss.item()/len(dataloader_valid)\n                averagef1 += f1score/len(dataloader_valid)\n                averagePrecision += precision/len(dataloader_valid)\n                averageRecall += recall/len(dataloader_valid)\n            validloss.append(averageloss)\n            validscore.append((averagef1, averagePrecision, averageRecall))\n            print(f\"epoch: {i}, train loss: {trainloss[-1]}, validation loss: {validloss[-1]}\\n train f1score: {trainscore[-1]}\\nvalidation f1score: {validscore[-1]}\")\n            if averagef1 > bestt_score:\n                print(\"Found Best Model\")\n                to_save = {'model': model.state_dict(),\n                           'config': config,\n                           'Dconfig': Dconfig,\n                           'optimizer': optimizer,\n                           'lr_s':None}\n                torch.save(to_save, save_path.replace('.pt', '_best.pt'))\n                bestt_score = averagef1\n        else:\n            print(f\"epoch: {i}, train loss: {trainloss[-1]}, train f1score: {trainscore[-1]}\")\n\n\n    return trainloss, validloss, trainscore, validscore\n\n\ndef test(dataloader_test, model = None, lossfn = None, epochs = 1, has_mask = True):\n    \"\"\"\n    Args:\n        model: if `pytorch Model` -> normal test. if `list(pytorch Model)` -> ensemble\n    \"\"\"\n    total_preds = []\n    testloss = []\n    testscore = []\n    if isinstance(model, list):\n        for mmm in model:\n            mmm.eval()\n    else:\n        model.eval()\n    averageloss = 0\n    averagef1 = 0\n    averagePrecision = 0\n    averageRecall = 0\n    for datas in tqdm(dataloader_test):\n        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n        if isinstance(model, list):\n            ensem = []\n            for mmm in model:\n                p = mmm(list(tweet), has_mask)\n                ensem.append(p[0])\n            pred = [ensemble(ensem)]\n        else:\n            pred = model(list(tweet), has_mask)\n            total_preds.append(pred)\n        loss = lossfn(pred[0], target.squeeze(1))\n        f1score = sklearn.metrics.f1_score(target.squeeze(1).cpu().numpy(), pred[0].argmax(-1).cpu().numpy(), average = 'weighted')\n        precision=sklearn.metrics.precision_score(pred[0].argmax(-1).cpu().numpy(), target.squeeze(1).cpu().numpy(), average='weighted', zero_division=0)\n        recall=sklearn.metrics.recall_score(pred[0].argmax(-1).cpu().numpy(), target.squeeze(1).cpu().numpy(), average='weighted', zero_division=0)\n        averageloss += loss.item()/len(dataloader_test)\n        averagef1 += f1score/len(dataloader_test)\n        averagePrecision += precision/len(dataloader_test)\n        averageRecall += recall/len(dataloader_test)\n    print(f\"test loss: {averageloss}\")\n    print(f\"test score: {(averagef1, averagePrecision, averageRecall)}\")\n    \n\n    return averageloss, (averagef1, averagePrecision, averageRecall), total_preds\n            \ndef get_csv(dataset, model = None, has_mask = True):\n    \"\"\"get the prediction csv\n    \n    Examples:\n        o = get_csv(dataset_test, model1)\n        o.to_csv(save_path.replace('.pt', '.csv'))\n    \"\"\"\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n    for datas in tqdm(dataloader):\n        count, hate_speech, offensive_language, neither, target, tweet = datas[0], datas[1], datas[2], datas[3], datas[4], datas[5]\n        if isinstance(model, list):\n            ensem = []\n            for mmm in model:\n                p = mmm(list(tweet), has_mask)\n                ensem.append(p[0])\n            pred = [ensemble(ensem)]\n        else:\n            pred = model(list(tweet), has_mask)\n    return pd.DataFrame(pred[0].cpu().detach().numpy())","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloss, validloss, trainscore, validscore = train(dataloader_train, dataloader_valid, model = model, optimizer = optimizer, \n      lossfn = lossfn, epochs = epochs, has_mask = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use best val set model\nstate = torch.load(save_path.replace('.pt', '_best.pt'))\nmodel.load_state_dict(state['model'])\n# Do test\n_, _, preds = test(dataloader_test, model = model, lossfn = lossfn, has_mask = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor pred in preds:\n    predictions += list(pred[0].argmax(-1).cpu().numpy())\n\n# preds[1][0].argmax(-1).cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mlxtend.evaluate import confusion_matrix\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom pandas.api.types import CategoricalDtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_vals(val):\n    vals = [0, 1, 2]\n    labels = [\"Hate\", \"Offensive\", \"Neutral\"]\n    d = dict(zip(vals, labels))\n    return d[val]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order = [\"Hate\", \"Offensive\", \"Neutral\"]\nlabels = [\"Hate\", \"Offensive\", \"Neutral\"]\nval = pd.DataFrame({\"predictions\" : test_data[\"class\"].values, \"test\" : predictions})\nval[\"predictions\"] = val[\"predictions\"].apply(add_vals)\nval[\"test\"] = val[\"test\"].apply(add_vals)\n\ncat_type = CategoricalDtype(categories=order, ordered=True)\nval[\"predictions\"] = val[\"predictions\"].astype(cat_type)\nval[\"test\"] = val[\"test\"].astype(cat_type)\nval[\"predictions\"].value_counts(normalize = True).sort_index()\n\n\ncm = confusion_matrix(y_target=val[\"test\"].cat.codes, \n                      y_predicted=val[\"predictions\"].cat.codes)\n    \nmake_confusion_matrix(cm, categories = labels, figsize = (8, 7))\nplt.savefig(\"BERT_base_uncased_full_preprocessing_confusion_matrix.png\", dpi=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
